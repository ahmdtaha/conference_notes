* On Regularization of Convolutional Kernels in Neural Networks **#arxiv2019**
* Robust Learning with Jacobian Regularization **#arxiv2019**
* Self-Supervised Learning of Pretext-Invariant Representations **#arxiv2019**
* ClusterFit: Improving Generalization of Visual Representations **#arxiv2019**
* Selfie: Self-supervised pretraining for image embedding **#arxiv2019**
* Axial attention in multidimensional transformers **#arxiv2019**
* A novel task for fine-grained image understanding **#arxiv2019**
* A Full-Image Full-Resolution End-to-End-Trainable CNN Framework for Image Forgery Detection **#arxiv2019**
* Multigrain: a unified image embedding for classes and instances `Retrieval` `FAIR`
	> PCA whitening + Generalized mean pooling (GeM)
* Are All Layers Created Equal? `arXiv`
	> This is why I like Samy Bengio
* Fine-tuning CNN image retrieval with no human annotation `PAMI` `Microsoft`
	> SFM (postive and negative pairs) => Pretrained CNN => Generalized Pooling => Post-processing Whitening => \alpha-Query Expansion (??)
* Luck Matters: Understanding Training Dynamics of Deep ReLU Networks `Meta`	> Nice paper, good idea, poorly written
* A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark `Google`
	> Visual Task Adaptation benchmark for evaluating different visual representation (fully supervised vs. self-supervised vs. generative, etc.)