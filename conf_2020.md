* Reinforcement Learning with Videos: Combining Offline Observations with Interaction -- `PI_Reading_Grp` **#corl2020**
* Learning Hyperbolic Representations For Un- Supervised 3D Segmentation `arXiv`
* Improved Baselines with Momentum Contrastive Learning `arXiv` `Meta`
	> Boost MOCO by using MLP (+ different temperature), stronger augmentation, big queue. 

* ResNeSt: Split-Attention Networks `arXiv`
* Sparse R-CNN: End-to-End Object Detection with Learnable Proposals `DET` `arXiv` `Sparse`
* Eta-Dataset: A Dataset Of Datasets For Learning To Learn From Few Examples `ICLR`
* Large Batch Optimization for Deep Learning: Training BERT in 76 minutes `ICLR` `LAMB` `Google`
* Fast is better than free: Revisiting adversarial training `ICLR`

* On the Variance of the Adaptive Learning Rate and Beyond `ICLR`
	> R-Adam reduce/control variance in the 2nd momentum used by vanilla Adam. Multiple the adaptive learning rate with a rectifier (<=1) to reduce varaince.
* Electra: Pretraining text encoders as discriminators rather than generators `Google` `Stanford` `ICLR`
	> In NLP, 15% of tokens are masked and loss is computed for this subset only. This slows convergence. So, this paper proposes a discriminator loss applied to all tokens instead of the reconstruction (generation) loss applied to masked token only. The discriminator loss trains a network to tell the difference between original and replaced tokens. 

* ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring `ICLR` `Google`
	> Align with labeled data distribution + Weak augmentation for pseudo label generation.

* Self-labelling via simultaneous clustering and representation learning. `ICLR` `SSL`
* On the Relationship between Self-Attention and Convolutional Layers `ICLR`

* Decoupling Representation and Classifier for Long-Tailed Recognition `ICLR` `Nice` `SG` `FB`
	> It seems like long-tailed classification doesn't need sophisticated balancing techniques. We just need to decouple the representation and classifier learning stage. After learning a strong representation, the paper re-initializes the classifier weights and achieve SOTA using class-balanced sampling. The paper proposes other approaches to re-normalize class-features. These approaches achieves competitive performs without bells and whistles (instance-based sampling during joint training)!
	
* Comparing Rewinding and Fine-tuning in Neural Network Pruning `ICLR` (I read this on 26 Dec 2020)
* How much position information do convolutional neural networks encode? `ICLR`
* Linear mode connectivity and the lottery ticket hypothesis `ICML`
* RIFLE: Backpropagation in Depth for Deep Transfer Learning through ReInitializing the Fully-connected LayEr `ICML`
	> Perturb the last FC layer
* Stable Prediction with Model Misspecification and Agnostic Distribution Shift `AAAI`
	> Watch this video first https://www.youtube.com/watch?v=wCJ8I-MtJdQ
* Random Erasing Data Augmentation `AAAI`
* The Lottery Ticket Hypothesis for Pre-trained BERT Networks `NIPS`
* Denoising Diffusion Probabilistic Models `NIPS`
* Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection `NIPS`
* Unsupervised learning of visual features by contrasting cluster assignments `SSL` `NIPS` `Facebook` `SwAV`
* Retrospective Loss: Looking Back to Improve Training of Deep Neural Networks  **#kdd2020**
* tree-cnn: a hierarchical deep convolutional neural network for incremental learning **#NeuralNetworks2020**
* FCOS: A Simple and Strong Anchor-free Object Detector **#pami2020**
* Curriculum by Smoothing -- `PI_Reading_Grp` **#nips2020**
* CompRess: Self-Supervised Learning by Compressing Representations **#nips2020**
* Convbert: Improving bert with span-based dynamic convolution **#nips2020**
* Contrastive Learning with Adversarial Examples **#nips2020**
* RandAugment: Practical Automated Data Augmentation with a Reduced Search Space **#nips2020** `Google`
	> Used by timm create_transform 
* Stochastic Optimization with Laggard Data Pipelines **#nips2020** -- `Nice` 
* FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence `NIPS` `Google`
	> Use weak augmentation to generate pseudo label for strongly augmented image.

* The origins and prevalence of texture bias in convolutional neural networks **#nips2020**
* FixMatch: Simplifying Semi Supervised learning with consistency and confidence -- `PI_Reading_Grp` **#nips2020**
* Big Self Supervised Models are Strong Semi Supervised learners -- `PI_Reading_Grp` **#nips2020**
* What makes for good views for contrastive learning `NIPS` `Nice`
* HiPPO: Recurrent Memory with Optimal Polynomial Projections `NIPS`
	> Just 1D functions! 
* Structured Convolutions for Efficient Neural Network Design **#nips2020**
	> Tensor decomposition. Not sure why flops are not reported!
* LoCo: Local Contrastive Representation Learning **#nips2020**
* Auxiliary Task Reweighting for Minimum-data Learning **#nips2020**
	> Well written paper
* Bootstrap Your Own Latent A New Approach to Self-Supervised Learning **#nips2020**
* Understanding the Role of Individual Units in a Deep Neural Network **National Academy of Sciences2020**
* Learning to combine Top-down to Buttom-up signals **#icml2020**
* Revisiting Training Strategies and Generalization Performance in Deep Metric Learning **#icml2020**
* Using Deep Learning to Accelerate Knee MRI at 3 T: Results of an Interchangeability Study -- `NYU` **American Journal of Roentgenology**
* International evaluation of an AI system for breast cancer screening -- `DeepMind` **Nature**
* A Hypersensitive Breast Cancer Detector -- `WR-AI` **SPIE Medical Imaging 2020**
* Adaptation of a deep learning malignancy model from full-field digital mammography to digital breast tomosynthesis -- `WR-AI` **SPIE Medical Imaging 2020**
* Effect of artificial intelligence-based triaging of breast cancer screening mammograms on cancer detection and radiologist workload: a retrospective simulation study -- `WR-AI` **The Lancet 2020**
* A Multi-site Study of a Breast Density Deep Learning Model for Full-field Digital Mammography Images and Synthetic Mammography Images -- `WR-AI` **Radiology: Artificial IntelligenceVol 2020**
* MommiNet: Mammographic Multi-View Mass Identification Networks -- `WR-AI` **MICCAI2020**
* Performance deterioration of deep neural networks for lesion classification in mammography due to distribution shift: an analysis based on artificially created distribution shift **SPIE2020** `Nice paper`

* Quantifying Attention Flow in Transformers `ACL` `Nice paper`
	> Vanilla attention weights are not informative for quantifing input-tokens' contribution. This paper propose two methods for a more informative signal: attention rollout and attention flow.
* Don't Stop Pretraining: Adapt Language Models to Domains and Tasks `ACL`
	> Keep pre-training on similar domain/task data before fine-tuning.
* LS-SSDD-v1.0: A Deep Learning Dataset Dedicated to Small Ship Detection from Large-Scale Sentinel-1 SAR Images **Remote Sensing**
* Pre-training without Natural Image `ACCV`
* CorDEL: A Contrastive Deep Learning Approach for Entity Linkage `ICDM-Data Mining` `AMZN`
* Generative Pretraining From Pixels `OpenAI` `GPT` `Images` `PMLR`
* PyHessian: Neural Networks Through the Lens of the Hessian `BigData`
	> Open source library to study the hessian of deep networks

* What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation `arXiv` `Apple`
	> There are samples that when memorized boosts generalization (test accuracy).
* Shortcut Learning in Deep Neural Networks `Nature Machine Intel`
	> The tank legend and the Clever Hans.

* Effect of artificial intelligence-based triaging of breast cancer screening mammograms on cancer detection and radiologist workload: a retrospective simulation study `Lancet Digital Health`
	> triage cases into two streams: no rad work stream and enhanced assessment work stream. The no work stream reduce rad workload while missing a small number of cancers. The enhanced work stream catch challenging cancers (interval and next-screen round cancer) by using MRI. The threshold were tuned on the study population! 

* Conformer: Convolution-augmented transformer for speech recognition `US` `Google` `INTERSPEECH` 
	> Add conv layer to Transformer layers so that attention captures long-term dependency while conv captures short-term dependency.
	
* wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations `NIPS` `FB` `US`
	> Quantize audio features for a contrastive self-supervised learning. The quantization process learns a discrete speech units.
	
* Improved Noisy Student Training for Automatic Speech Recognition `US` `Google` `INTERSPEECH` 
	> Semi-supervised learning approach, where a fine-tuned model generate pseudo labels for unlabeled dataset. The pseudo labels are filtered to ensure high quality, then added to a pool of training sample used to fine-tune a model again.  
 
