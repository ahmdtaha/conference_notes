* Reinforcement Learning with Videos: Combining Offline Observations with Interaction -- `PI_Reading_Grp` **#corl2020**
* Learning Hyperbolic Representations For Un- Supervised 3D Segmentation **#arXiv2020**
* ResNeSt: Split-Attention Networks **#arXiv2020**
* Sparse R-CNN: End-to-End Object Detection with Learnable Proposals `DET` `arXiv` `Sparse`
* Eta-Dataset: A Dataset Of Datasets For Learning To Learn From Few Examples **#iclr2020**
* Large Batch Optimization for Deep Learning: Training BERT in 76 minutes **#iclr2020** `LAMB` `Google`
* Fast is better than free: Revisiting adversarial training **#iclr2020**
* ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring `ICLR` `Google`
	> Align with labeled data distribution + Weak augmentation for pseudo label generation.
* Self-labelling via simultaneous clustering and representation learning. `ICLR` `SSL`
* On the Relationship between Self-Attention and Convolutional Layers `ICLR`
* Comparing Rewinding and Fine-tuning in Neural Network Pruning `ICLR` (I read this on 26 Dec 2020)
* How much position information do convolutional neural networks encode? `ICLR`
* Linear mode connectivity and the lottery ticket hypothesis `ICML`
* RIFLE: Backpropagation in Depth for Deep Transfer Learning through ReInitializing the Fully-connected LayEr `ICML`
	> Perturb the last FC layer
* Stable Prediction with Model Misspecification and Agnostic Distribution Shift `AAAI`
	> Watch this video first https://www.youtube.com/watch?v=wCJ8I-MtJdQ
* Random Erasing Data Augmentation `AAAI`
* The Lottery Ticket Hypothesis for Pre-trained BERT Networks `NIPS`
* Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection `NIPS`
* Unsupervised learning of visual features by contrasting cluster assignments `SSL` `NIPS` `Facebook` `SwAV`
* Retrospective Loss: Looking Back to Improve Training of Deep Neural Networks  **#kdd2020**
* tree-cnn: a hierarchical deep convolutional neural network for incremental learning **#NeuralNetworks2020**
* FCOS: A Simple and Strong Anchor-free Object Detector **#pami2020**
* Curriculum by Smoothing -- `PI_Reading_Grp` **#nips2020**
* CompRess: Self-Supervised Learning by Compressing Representations **#nips2020**
* Convbert: Improving bert with span-based dynamic convolution **#nips2020**
* Contrastive Learning with Adversarial Examples **#nips2020**
* RandAugment: Practical Automated Data Augmentation with a Reduced Search Space **#nips2020** `Google`
	> Used by timm create_transform 
* Stochastic Optimization with Laggard Data Pipelines **#nips2020** -- `Nice` 
* FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence `NIPS` `Google`
	> Use weak augmentation to generate pseudo label for strongly augmented image.
* The origins and prevalence of texture bias in convolutional neural networks **#nips2020**
* FixMatch: Simplifying Semi Supervised learning with consistency and confidence -- `PI_Reading_Grp` **#nips2020**
* Big Self Supervised Models are Strong Semi Supervised learners -- `PI_Reading_Grp` **#nips2020**
* What makes for good views for contrastive learning `NIPS` `Nice`
* HiPPO: Recurrent Memory with Optimal Polynomial Projections `NIPS`
	> Just 1D functions! 
* Structured Convolutions for Efficient Neural Network Design **#nips2020**
	> Tensor decomposition. Not sure why flops are not reported!
* LoCo: Local Contrastive Representation Learning **#nips2020**
* Auxiliary Task Reweighting for Minimum-data Learning **#nips2020**
	> Well written paper
* Bootstrap Your Own Latent A New Approach to Self-Supervised Learning **#nips2020**
* Understanding the Role of Individual Units in a Deep Neural Network **National Academy of Sciences2020**
* Learning to combine Top-down to Buttom-up signals **#icml2020**
* Revisiting Training Strategies and Generalization Performance in Deep Metric Learning **#icml2020**
* Using Deep Learning to Accelerate Knee MRI at 3 T: Results of an Interchangeability Study -- `NYU` **American Journal of Roentgenology**
* International evaluation of an AI system for breast cancer screening -- `DeepMind` **Nature**
* A Hypersensitive Breast Cancer Detector -- `WR-AI` **SPIE Medical Imaging 2020**
* Adaptation of a deep learning malignancy model from full-field digital mammography to digital breast tomosynthesis -- `WR-AI` **SPIE Medical Imaging 2020**
* Effect of artificial intelligence-based triaging of breast cancer screening mammograms on cancer detection and radiologist workload: a retrospective simulation study -- `WR-AI` **The Lancet 2020**
* A Multi-site Study of a Breast Density Deep Learning Model for Full-field Digital Mammography Images and Synthetic Mammography Images -- `WR-AI` **Radiology: Artificial IntelligenceVol 2020**
* MommiNet: Mammographic Multi-View Mass Identification Networks -- `WR-AI` **MICCAI2020**
* Performance deterioration of deep neural networks for lesion classification in mammography due to distribution shift: an analysis based on artificially created distribution shift **SPIE2020** `Nice paper`
* Quantifying Attention Flow in Transformers `ACL` `Nice paper`
	> Vanilla attention weights are not informative for quantifing input-tokens' contribution. This paper propose two methods for a more informative signal: attention rollout and attention flow.
* LS-SSDD-v1.0: A Deep Learning Dataset Dedicated to Small Ship Detection from Large-Scale Sentinel-1 SAR Images **Remote Sensing**
* Pre-training without Natural Image `ACCV`
* CorDEL: A Contrastive Deep Learning Approach for Entity Linkage `ICDM-Data Mining` `AMZN`
* Generative Pretraining From Pixels `OpenAI` `GPT` `Images` `PMLR`
* PyHessian: Neural Networks Through the Lens of the Hessian `BigData`
	> Open source library to study the hessian of deep networks
* What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation `arXiv` `Apple`
	> There are samples that when memorized boosts generalization (test accuracy).
* Shortcut Learning in Deep Neural Networks `Nature Machine Intel`
	> The tank legend and the Clever Hans.