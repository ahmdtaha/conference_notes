
## ICLR
* Understanding Dimensional Collapse in Contrastive Self-supervised Learning `FB AI`
	> Bad conclusion, but worth reading
* UniFormer: Unifying Convolution and Self-attention for Visual Recognition
	> Technicalities
* Fortuitous Forgetting in Connectionist Networks `Mila`
* VOS: Learning What You Don't Know by Virtual Outlier Synthesis `Uncertainty`
* Relational Surrogate Loss Learning
* The Uncanny Similarity of Recurrence and Depth `Nice`
* PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
* Unsupervised Semantic Segmentation by Distilling Feature Correspondences
* BEiT: BERT Pre-Training of Image Transformers `Transformers` `Oral`
* iBOT: Image BERT Pre-Training with Online Tokenizer `SSL` `Nice`

## ECCV
* Contrastive Deep Supervision
	> Technicalities
* Frozen CLIP Models are Efficient Video Learners `Video` `Text`
	> Frozen CLIP frame features + light-weight temporal attention for short videos
* EclipSE: Efficient Long-range Video Retrieval using Sight and Sound `Video` `Text` `Audio`
	> Frozen CLIP frame features from long videos (V) + audio (A) features + (A2V) + (V2A) attention models

## CVPR
* Fine-tuning Image Transformers using Learnable Memory `Google` `Transformers` `Continual`
* QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection `Detection` `Technicalities`
* Delving Deep into the Generalization of Vision Transformers under Distribution Shifts `ViTs`
* SimMatch: Semi-supervised Learning with Similarity Matching `Technicalities`
* Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning `Oral` `Medical` `Transformers`
* When Does Contrastive Visual Representation Learning Work? `SSL`
* Vision-language pre-training with triple contrastive learning `ML` `SSL` `AMZN`
* Patch-level Representation Learning for Self-supervised Vision Transformers `SSL`
* What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors
	> Not convincing
* Multi-modal Alignment using Representation Codebook `AMZN` `VLP`
* Align and Prompt: Video-and-Language Pre-training with Entity Prompts `Sales` `VLP` `Video`
* Multiview Transformers for Video Recognition `Video` `Google` `Supervised`
* SimMIM: a Simple Framework for Masked Image Modeling `Image` `Microsoft` `Nice`
* Towards Total Recall in Industrial Anomaly Detection `AMZN`
* Self-Supervised Learning of Object Parts for Semantic Segmentation
	> Poorly written, DINO on batch-level, nice paper
* Masked Autoencoders Are Scalable Vision Learners `Meta` `FAIR`
* On the Importance of Asymmetry for Siamese Representation Learning `Meta` `FAIR` `SSL` `Contrastive` `Siamese`
* Point-Level Region Contrast for Object Detection Pre-Training `Meta` `SSL` `Contrastive` `Detection` `MoCo`
	> Too many loss terms


## arXiv
* Self-supervised Learning from 100 Million Medical Images `Medical` `Siemens`
* A ConvNet for the 2020s
* Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation `Microsoft` `SSL` `Nice`
	> Interesting findings and analysis
* OMNIVORE: A Single Model for Many Visual Modalities
* Vision Transformer for Small-Size Datasets
* Forgetting Data from Pre-trained GANs `KE` `GANs`
* data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language `Meta` `SSL`
	> Use target from multilpe layers instead of the last layer
* Masked Siamese Networks for Label-Efficient Learning `Meta` `SSL`
	> Similar to Data2Vec; use prototypes which I don't like; low-shot motive
* Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision `Meta`
	> Brute force
* One Network Doesn't Rule Them All: Moving Beyond Handcrafted Architectures in Self-Supervised Learning `SSL` `NAS`
* When Does Re-initialization Work? `KE`
* Near Perfect GAN Inversion `AMZN` `GANs`
* The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon `KE` `Apple`
	> flings!
* What Knowledge Gets Distilled in Knowledge Distillation? `KD`
	> Nice Analysis
* Benchmark Assessment for DeepSpeed Optimization Library
	> Needs revision
* High Fidelity Visualization of What Your Self-Supervised Representation Knows About `Meta` `SSL`
* Correct-n-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations 
* A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments `Meta`
* Exploring Plain Vision Transformer Backbone for Object Detection `ViTDet`
* Text and Code Embeddings by Contrastive Pre-Training `OpenAI`
* Convolutional Xformers for Vision 
* ItemSage: Learning Product Embeddings for Shopping Recommendations at Pinterest `ML`
* Robust and Efficient Medical Imaging with Self-Supervision `Google` `Medical`
* BEiT V2: Masked Image Modeling with Vector-Quantized Visual Tokenizers `Microsoft` `MIM`
* BEiT V3: Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks `Microsoft` `MIM`
* VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts `Microsoft` `MIM`
* VL-BEiT: Generative Vision-Language Pretraining `Microsoft` `MIM`
* CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment `Microsoft` `Video` `Text`
* Self-Supervised Learning for Videos: A Survey `SSL`
	> Needs revision
* LAVIS: A Library for Language-Vision Intelligence `Salesforce` `Nice`
* ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training `Baidu` `Image` `Text`
* Mugs: A Multi-Granular Self-Supervised Learning Framework `Image` `SSL`
* Transcending Scaling Laws with 0.1% Extra Compute `NLP` `SSL`
* Scaling Language-Image Pre-training via Masking `Meta` `SSL`
	> MIM + CLIP
* A simple, efficient and scalable contrastive masked autoencoder for learning visual representations `Google`
	> MAE + Contrastive loss
* EVA: Exploring the Limits of Masked Visual Representation Learning at Scale `SSL` `MIM`
* The Impact of Reinitialization on Generalization in Convolutional Neural Networks `Google` 
	> Did it before it is cool!
* Three things everyone should know about Vision Transformers `Meta`
	> Nice analysis 	
* Patches Are All You Need?	
* MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining
	> Nice analysis
* CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet 
	> LLDR, EMA, drop_path, RandAug, Label Smoothing
* Image-and-Language Understanding from Pixels Only `Google`
	> avoid multi-lingual tokenizers
* MaskViT: Masked Visual Pre-Training for Video Prediction `Saleforce` `Video`
	> Multi-stage

## Generic
* Multi-Head Deep Metric Learning Using Global and Local Representations `WACV`
* When and how convolutional neural networks generalize to out-of-distribution categoryâ€“viewpoint combinations `Nature ML`
	> Nice analysis
* PVTv2: Improved Baselines with Pyramid Vision Transformer `CVMJ`
* GLaM: Efficient Scaling of Language Models with Mixture-of-Experts `ICML` `Google`
* Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time `ICML` `Google`
	> Nice analysis
* When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism `AAAI`
* A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications `AAAI`
* Towards Understanding Sharpness-Aware Minimization `ICML` `SAM`
* VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training `NIPS` `VLP` `Video`
* Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks
	> Did it before it is cool!
* Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning `Nature` `Clip` `Stanford` `Harvard` `SSL`
* How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers `TMLR` `Google` 
	> More Data over More Epochs; Augmentations over Regularization; Bigger ViTs over smaller patches
* Learning to Prompt for Vision-Language Models `IJCV`
	> Reminds me of L2-CAF
* Learngene: From Open-World to Your Learning Task `AAAI`
