* When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism `AAAI`
* A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications `AAAI`
* * Self-supervised Learning from 100 Million Medical Images `arXiv` `Medical` `Siemens`
* A ConvNet for the 2020s `arXiv`
* OMNIVORE: A Single Model for Many Visual Modalities `arXiv`
* Vision Transformer for Small-Size Datasets `arXiv`
* Forgetting Data from Pre-trained GANs `arXiv` `KE` `GANs`
* A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments `arXiv` `Meta`
* Exploring Plain Vision Transformer Backbone for Object Detection `arXiv` `ViTDet`
* Text and Code Embeddings by Contrastive Pre-Training `arXiv` `OpenAI`
* Convolutional Xformers for Vision `arXiv`
* ItemSage: Learning Product Embeddings for Shopping Recommendations at Pinterest `arXiv` `ML`
* Towards Understanding Sharpness-Aware Minimization `ICML` `SAM`
* When Does Re-initialization Work? `arXiv` `KE`
* Near Perfect GAN Inversion `arXiv` `AMZN` `GANs`
* The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon `arXiv` `KE` `Apple`
	> flings!
* What Knowledge Gets Distilled in Knowledge Distillation? `arXiv` `KD`
	> Nice Analysis

## ICLR
* Understanding Dimensional Collapse in Contrastive Self-supervised Learning `FB AI`
	> Bad conclusion, but worth reading
* UniFormer: Unifying Convolution and Self-attention for Visual Recognition
	> Technicalities
* Fortuitous Forgetting in Connectionist Networks `Mila`
* VOS: Learning What You Don't Know by Virtual Outlier Synthesis `Uncertainty`
* Relational Surrogate Loss Learning
* The Uncanny Similarity of Recurrence and Depth `Nice`
* PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
* Unsupervised Semantic Segmentation by Distilling Feature Correspondences


## CVPR
* Fine-tuning Image Transformers using Learnable Memory `Google` `Transformers` `Continual`
* QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection `Detection` `Technicalities`
* Delving Deep into the Generalization of Vision Transformers under Distribution Shifts `ViTs`
* SimMatch: Semi-supervised Learning with Similarity Matching `Technicalities`
* Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning `Oral` `Medical` `Transformers`
* When Does Contrastive Visual Representation Learning Work? `SSL`
* Vision-language pre-training with triple contrastive learning `ML` `SSL` `AMZN`
* Patch-level Representation Learning for Self-supervised Vision Transformers `SSL`
* What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors
	> Not convincing

## Generic
* Multi-Head Deep Metric Learning Using Global and Local Representations `WACV`
* When and how convolutional neural networks generalize to out-of-distribution categoryâ€“viewpoint combinations `Nature ML`
	> Nice analysis
* Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision `arXiv` `Meta`
	> Brute force
* One Network Doesn't Rule Them All: Moving Beyond Handcrafted Architectures in Self-Supervised Learning `arXiv` `SSL` `NAS`
* PVTv2: Improved Baselines with Pyramid Vision Transformer `CVMJ`
* Robust and Efficient Medical Imaging with Self-Supervision `arXiv` `Google` `Medical`