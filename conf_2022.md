
## ICLR
* Understanding Dimensional Collapse in Contrastive Self-supervised Learning `FB AI`
	> Bad conclusion, but worth reading
* UniFormer: Unifying Convolution and Self-attention for Visual Recognition
	> Technicalities
* Fortuitous Forgetting in Connectionist Networks `Mila`
* VOS: Learning What You Don't Know by Virtual Outlier Synthesis `Uncertainty`
* Relational Surrogate Loss Learning
* The Uncanny Similarity of Recurrence and Depth `Nice`
* PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
* Unsupervised Semantic Segmentation by Distilling Feature Correspondences
* BEiT: BERT Pre-Training of Image Transformers `arXiv` `Transformers` `Oral`

## ECCV
* Contrastive Deep Supervision
	> Technicalities

## CVPR
* Fine-tuning Image Transformers using Learnable Memory `Google` `Transformers` `Continual`
* QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection `Detection` `Technicalities`
* Delving Deep into the Generalization of Vision Transformers under Distribution Shifts `ViTs`
* SimMatch: Semi-supervised Learning with Similarity Matching `Technicalities`
* Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning `Oral` `Medical` `Transformers`
* When Does Contrastive Visual Representation Learning Work? `SSL`
* Vision-language pre-training with triple contrastive learning `ML` `SSL` `AMZN`
* Patch-level Representation Learning for Self-supervised Vision Transformers `SSL`
* What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors
	> Not convincing
* Multi-modal Alignment using Representation Codebook `AMZN` `VLP`


## arXiv
* Self-supervised Learning from 100 Million Medical Images `Medical` `Siemens`
* A ConvNet for the 2020s
* OMNIVORE: A Single Model for Many Visual Modalities
* Vision Transformer for Small-Size Datasets
* Forgetting Data from Pre-trained GANs `KE` `GANs`
* Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision `Meta`
	> Brute force
* One Network Doesn't Rule Them All: Moving Beyond Handcrafted Architectures in Self-Supervised Learning `SSL` `NAS`
* When Does Re-initialization Work? `KE`
* Near Perfect GAN Inversion `AMZN` `GANs`
* The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon `KE` `Apple`
	> flings!
* What Knowledge Gets Distilled in Knowledge Distillation? `KD`
	> Nice Analysis
* High Fidelity Visualization of What Your Self-Supervised Representation Knows About `Meta` `SSL`
* Correct-n-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations 
* A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments `Meta`
* Exploring Plain Vision Transformer Backbone for Object Detection `ViTDet`
* Text and Code Embeddings by Contrastive Pre-Training `OpenAI`
* Convolutional Xformers for Vision 
* ItemSage: Learning Product Embeddings for Shopping Recommendations at Pinterest `ML`
* Robust and Efficient Medical Imaging with Self-Supervision `Google` `Medical`
* BEiT V2: Masked Image Modeling with Vector-Quantized Visual Tokenizers `MS` `MIM`

## Generic
* Multi-Head Deep Metric Learning Using Global and Local Representations `WACV`
* When and how convolutional neural networks generalize to out-of-distribution categoryâ€“viewpoint combinations `Nature ML`
	> Nice analysis
* PVTv2: Improved Baselines with Pyramid Vision Transformer `CVMJ`
* GLaM: Efficient Scaling of Language Models with Mixture-of-Experts `ICML` `Google`
* Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time `ICML` `Google`
	> Nice analysis
* When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism `AAAI`
* A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications `AAAI`
* Towards Understanding Sharpness-Aware Minimization `ICML` `SAM`
