
## ICLR
* Understanding Dimensional Collapse in Contrastive Self-supervised Learning `FB AI`
	> Bad conclusion, but worth reading
* UniFormer: Unifying Convolution and Self-attention for Visual Recognition
	> Technicalities
* Fortuitous Forgetting in Connectionist Networks `Mila`
	> Nice analysis but arch-specific. Re-visited for technical details; I am surprised that LLF is not only architecture-specific but dataset-specific also! LLF uses different layer cut-off/hyperparameter for the same architecture when applied on different dataset. For a ViT-B arch with 12 blocks, this is a large search space. 
* VOS: Learning What You Don't Know by Virtual Outlier Synthesis `Uncertainty`
* Relational Surrogate Loss Learning
* The Uncanny Similarity of Recurrence and Depth `Nice`
* PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
* Unsupervised Semantic Segmentation by Distilling Feature Correspondences
* BEiT: BERT Pre-Training of Image Transformers `Transformers` `Oral`
* iBOT: Image BERT Pre-Training with Online Tokenizer `SSL` `Nice`
* DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR
	> Learn anchors (x,y,w,h) as queries for DETR
* LoRA: Low-Rank Adaptation of Large Language Models `Microsoft`
	> Decompose Linear layers into two matrixs with lower rank to reduce the cost of fine-tuning.

## ECCV
* Contrastive Deep Supervision
	> Technicalities
* An Impartial Take to the CNN vs Transformer Robustness Contest
	> Nice analysis between transformers and CNNs.
* MaxViT: Multi-Axis Vision Transformer `Google`
	> Nice dilated global attention, but requires input-resolution that matches grid-size
* Frozen CLIP Models are Efficient Video Learners `Video` `Text`
	> Frozen CLIP frame features + light-weight temporal attention for short videos = EVL
* EclipSE: Efficient Long-range Video Retrieval using Sight and Sound `Video` `Text` `Audio`
	> Frozen CLIP frame features from long videos (V) + audio (A) features + (A2V) + (V2A) attention models
* Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning `Efficient` `ViT` `Nice` `MvT` `Multiscale`
	> downsample k/v using Discrete wavelet transform (DWT) instead of the simple pooling/conv-stride. This trick probably works for vision models only, i.e., infeasible for NLP.
* Detecting twenty-thousand classes using image-level supervision `Meta`
	> Use largest bbox and apply classification loss. Detic: Detection with classification data
* Teaching Where to Look: Attention Similarity Knowledge Distillation for Low Resolution Face Recognition
	> Compute channel/spatial-attention for both teacher and student networks. Minimize distance between these attentions.
* PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection
	> Propose gated cross-attention (gated correlation) component for Video object detection (VOD).
* Expanding Language-Image Pretrained Models for General Video Recognition `Oral`
	> Expand VL models (CLIP, Florence) for videos using message tokens (fusion and diffusion).
* MultiMAE: Multi-modal Multi-task Masked Autoencoders `Swiss` `MAE` `EPFL`
	> MAE with multiple inputs (modalities) and multiple output (depth and semnatic segmentation tasks) 
* Training Vision Transformers with Only 2040 Images
	> Training ViT from scratch on a small dataset using both SSL and fine-tuning. Both stages performed on the small dataset.
* DaViT: Dual Attention Vision Transformers `Microsoft`
	> Perform local (window) attention on spatial tokens, but also local attention on channel tokens. Since every individual channel token capture global context, The proposed DaViT captures both local and global context while being computationally efficient.

## CVPR
* Fine-tuning Image Transformers using Learnable Memory `Google` `Transformers` `Continual`
* QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection `Detection` `Technicalities`
* Delving Deep into the Generalization of Vision Transformers under Distribution Shifts `ViTs`
* SimMatch: Semi-supervised Learning with Similarity Matching `Technicalities`
* Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning `Oral` `Medical` `Transformers`
* When Does Contrastive Visual Representation Learning Work? `SSL`
* Vision-language pre-training with triple contrastive learning `ML` `SSL` `AMZN`
* Patch-level Representation Learning for Self-supervised Vision Transformers `SSL`
* What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors
	> Not convincing
* Multi-modal Alignment using Representation Codebook `AMZN` `VLP`
* Align and Prompt: Video-and-Language Pre-training with Entity Prompts `Sales` `VLP` `Video`
* Multiview Transformers for Video Recognition `Video` `Google` `Supervised`
* SimMIM: a Simple Framework for Masked Image Modeling `Image` `Microsoft` `Nice`
* Towards Total Recall in Industrial Anomaly Detection `AMZN`
* Self-Supervised Learning of Object Parts for Semantic Segmentation
	> Poorly written, DINO on batch-level, nice paper
* Masked Autoencoders Are Scalable Vision Learners `Meta` `FAIR`
* On the Importance of Asymmetry for Siamese Representation Learning `Meta` `FAIR` `SSL` `Contrastive` `Siamese`
* Point-Level Region Contrast for Object Detection Pre-Training `Meta` `SSL` `Contrastive` `Detection` `MoCo`
	> Too many loss terms
* Image Segmentation Using Text and Image Prompts `Seg` `CLIP` `Supervised`
* The Unreasonable Effectiveness of CLIP Features for Image Captioning: An Experimental Analysis `Workshop` `CLIP` `Frozen`
* MViTv2: Improved Multiscale Vision Transformers for Classification and Detection `Meta` `Multiscale ViT` `MViT`
	> Technicalities
* Reversible Vision Transformers `Nice` `Meta`
	> ViT + Reversible layers
* CMT: Convolutional Neural Networks Meet Vision Transformers `Noah` `ViT` `Multiscale`
	> Conv Stem + DConv positional embedding + Efficient subsampling attention + Efficient FFN (borrowed from EfficientNet). I wish they released the COCO ckpts.
* Contrastive Learning for Unsupervised Video Highlight Detection `SSL`
	> Nice ablation studies, but no code!!
* Grounded Language-Image Pre-training `VLP` `MSF`
* A Self-Supervised Descriptor for Image Copy Detection `META`
	> Use differential entropy loss to spread embeddings, GEM pooling + ton of augmentations
* MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens 
	> Tokens within long sequence communicate with Hub-Tokens (Messengers/MSG).
* Temporally Efficient Vision Transformer for Video Instance Segmentation
	> Leverage MessengerTokens and same MHSA across both spatial and temporal dimensions.
* The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy `MSR`
	> Regularizer inter-layer tokens, attention matrix and weights (filters).
* Towards Robust Vision Transformer `Nice`
	> Evaluate different components of Conv-Trans-based models. Conv better at patch-embedding; positional-embedding is valuable; more heads (e.g., 8) helps; self-attention better be global;FFN with Conv is better; CLS token is not important; Not sure why PVT suffers in robustness! 
* AdaMixer: A Fast-Converging Query-Based Object Detector 
	> Sparse/Query based detector, sample object features using context vector + positional vector. Replace ROI pooling -- used by Sparse-RCNN -- with a 3D points sampling set. Adaptive mixing of channel and spatial features.
* LiT: Zero-Shot Transfer with Locked-image text Tuning `Google`
	> It is possible to use pre-trained (locked) image/text model for Vision-Text pretraining
* Dataset Distillation by Matching Training Trajectories 
	> Use difference between teacher and student parameters as a signal to compute the gradient for a learnable distilled dataset.
* DN-DETR: Accelerate DETR Training by Introducing Query DeNoising
	> Add Noisy GT boxes as query to speedup DETR convergence
* Masked-attention Mask Transformer for Universal Image Segmentation `Meta`
	> A universal arch for segmentation (semantic, instance, panoptic)
* A Simple Data Mixing Prior for Improving Self-Supervised Learning
	> Improving SSL (Moco and DINO) on ViTs using Data Mix augmentations (Mixup, CutMix, ResizeMix)
* Hard Patches Mining for Masked Image Modeling `China`
	> Use reconstruction loss as a proxy to identify hard patches for MAE. The paper shows faster convergence in terms of number of epochs, but I am not sure it is faster in terms of wall-time.


## arXiv
* Self-supervised Learning from 100 Million Medical Images `Medical` `Siemens`
* A ConvNet for the 2020s
* Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation `Microsoft` `SSL` `Nice`
	> Apply feature distillation as a post-processing setp after pre-training to boost feature generality. The ultimate goal of this paper is to decouple the generality and scalability objectives during pre-training. Interesting findings and analysis. 
* OMNIVORE: A Single Model for Many Visual Modalities
* Vision Transformer for Small-Size Datasets
* Forgetting Data from Pre-trained GANs `KE` `GANs`
* data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language `Meta` `SSL`
	> Use target from multilpe layers instead of the last layer
* Masked Siamese Networks for Label-Efficient Learning `Meta` `SSL`
	> Similar to Data2Vec; use prototypes which I don't like; low-shot motive
* Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision `Meta`
	> Brute force
* One Network Doesn't Rule Them All: Moving Beyond Handcrafted Architectures in Self-Supervised Learning `SSL` `NAS`
* When Does Re-initialization Work? `KE`
* Better plain ViT baselines for ImageNet-1k
	> Ordered according to importance (high to low): Augmentation (Mixup), Smaller bz (more Grad updates), Avg Pool instead of cls-token, Fixed Pos Emb.
* Near Perfect GAN Inversion `AMZN` `GANs`
* The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon `KE` `Apple`
	> flings!
* What Knowledge Gets Distilled in Knowledge Distillation? `KD`
	> Nice Analysis
* Benchmark Assessment for DeepSpeed Optimization Library
	> Needs revision
* High Fidelity Visualization of What Your Self-Supervised Representation Knows About `Meta` `SSL`
* Correct-n-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations 
* A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments `Meta`
* Exploring Plain Vision Transformer Backbone for Object Detection `ViTDet`
* Text and Code Embeddings by Contrastive Pre-Training `OpenAI`
* Convolutional Xformers for Vision 
* ItemSage: Learning Product Embeddings for Shopping Recommendations at Pinterest `ML`
* Robust and Efficient Medical Imaging with Self-Supervision `Google` `Medical`
	> Revise paper: 224 seems enough; Natural-Images=> Medical Images => fine-tuninig
* BEiT V2: Masked Image Modeling with Vector-Quantized Visual Tokenizers `Microsoft` `MIM`
* BEiT V3: Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks `Microsoft` `MIM`
* VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts `Microsoft` `MIM`
* VL-BEiT: Generative Vision-Language Pretraining `Microsoft` `MIM`
* CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment `Microsoft` `Video` `Text`
* Self-Supervised Learning for Videos: A Survey `SSL`
	> Needs revision
* LAVIS: A Library for Language-Vision Intelligence `Salesforce` `Nice`
* ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training `Baidu` `Image` `Text`
* Mugs: A Multi-Granular Self-Supervised Learning Framework `Image` `SSL`
* Transcending Scaling Laws with 0.1% Extra Compute `NLP` `SSL`
* Scaling Language-Image Pre-training via Masking `Meta` `SSL`
	> MIM + CLIP
* A simple, efficient and scalable contrastive masked autoencoder for learning visual representations `Google`
	> MAE + Contrastive loss
* EVA: Exploring the Limits of Masked Visual Representation Learning at Scale `SSL` `MIM`
* The Impact of Reinitialization on Generalization in Convolutional Neural Networks `Google` 
	> Did it before it is cool!
* Three things everyone should know about Vision Transformers `Meta`
	> Nice analysis 	
* Patches Are All You Need?	
* MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining
	> Nice analysis
* CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet 
	> LLDR, EMA, drop_path, RandAug, Label Smoothing
* Image-and-Language Understanding from Pixels Only `Google`
	> avoid multi-lingual tokenizers
* MaskViT: Masked Visual Pre-Training for Video Prediction `Saleforce` `Video`
	> Multi-stage
* FastMIM: Expediting Masked Image Modeling Pre-training for Vision `Ark` `SSL`
* Global Context Vision Transformers `Nvidia` 
	> Local self-attention + efficient global self-attention
* Guillotine Regularization: Improving Deep Networks Generalization by Removing their Head `Meta`
	> The projection heads adsorb bias from SSL task/data/sub-optimal hyperparameters. 
* Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals 
	> EEG signal (1) embedded into latent space => GANs => pixel-level prior, (2) embedded into latent space => caption-level prior, (3) Diffusion model operates on both pixel-level and caption-level priors to reconstruct stimuli
* Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality `Nice`
	> Enable MAE for PVT and Swin archs
* DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection
	> Combine denoising queries + learnable anchor/context queries using query selection.
* Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios `ByteDance` `Nice`
	> The following are responsible for the inefficient vanilla ViT models: quadratic complexity of attention, non-foldable LayerNorm and GELU layers. Hybrid architecture run efficiently on TensorRT, CoreML. Propose a new hybrid architecture with efficient multi-head convolution attention to model short-range dependencies and vanilla attention for long-range dependencies. Ideas#1: Long-range modeling is important for multi-stage architectures used in detection and segmentation tasks. Ideas#2: A single long-range attention layer is enough per stage after performing short-range (convolutional) attention. Idea#3: We can control (reduce) the number of channels in before attention using DWConv.
* CM3: A Causal Masked Multimodal Model of the Internet
	> Train multi-modal model that can perform bi-directional decodiing --  infilling -- beside left-to-right decoding.
* LightViT: Towards Light-Weight Convolution-Free Vision Transformers
	> Small dimension hinder efficient FFN layers. LightViT tackles this problem by proposing a two-branch FFN with both spatial and channel attention.
* Pretraining Without Attention
	> Combine state-space models (SSM) with gated MLP (Gated linear unit GLU) to reach transformer performance with attention complexity. The gating layers seems very important to compensate for omitted attention.
* BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models `Salesforce`
	> Learn language-image representation using frozen image and LLMs.
* MAR: Masked autoencoders for efficient action recognition `Alibaba` `Singapore`
	> Speedup ViT training on videos by dropping a percentage of tokens (patches/tubelets). Given the high-redundancy of spatial-temporal signal, no need to process all tokens. Use a bridging classifier to reduce gap between low-level re-construction features and high-level semantic features. 

## Generic
* Multi-Head Deep Metric Learning Using Global and Local Representations `WACV`
* When and how convolutional neural networks generalize to out-of-distribution category–viewpoint combinations `Nature ML`
	> Nice analysis
* PVT v2: Improved Baselines with Pyramid Vision Transformer `CVMJ`
* GLaM: Efficient Scaling of Language Models with Mixture-of-Experts `ICML` `Google`
* Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time `ICML` `Google`
	> Nice analysis
* General-purpose, long-context autoregressive modeling with Perceiver AR `ICML` `Google`
	> Followup on Perceiver and PerceiverIO
* When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism `AAAI`
* Sparse MLP for Image Recognition: Is Self-Attention Really Necessary? `AAAI`
	> Replace token-mixer component in MLPMixer with a computational friendly sparse component. The proposed new token-mixer component utilize both DWConv and Axial token (row/col) aggregation. "If this module is passed twice, each token can aggregate information across the entire 2D space."
* A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications `AAAI`
* Towards Understanding Sharpness-Aware Minimization `ICML` `SAM`
* VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training `NIPS` `VLP` `Video`
	> Mask tubelets when working with videos. Mask 90% of inputs.
* Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks
	> Did it before it is cool!
* Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning `Nature` `Clip` `Stanford` `Harvard` `SSL`
* How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers `TMLR` `Google` 
	> More Data over More Epochs; Augmentations over Regularization; Bigger ViTs over smaller patches
* Learning to Prompt for Vision-Language Models `IJCV`
	> Reminds me of L2-CAF
* Learngene: From Open-World to Your Learning Task `AAAI`
* ConvMAE: Masked Convolution Meets Masked Autoencoders `NIPS`
* Fast Vision Transformers with HiLo Attention `Australia`
	> split attention heads into local and global heads. Local heads (high frequency) perform local attention on full resolution while global heads (low frequency) perform global attention on a downsampled resolution. The goal is to speedup training and inference. I don't think that can works with MAE pre-training. 
* FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness `NIPS` `Nice`
	> Nice presentation and analysis.
* Contrastive Learning of Medical Visual Representations from Paired Images and Text `MLHC` `Medical`
	> CLIP ancestor
* Croco: Self-supervised pre-training for 3d vision tasks by cross-view completion. `NIPS` `NAVER`
	> MAE for multi-view datasets
* VICRegL: Self-Supervised Learning of Local Visual Features `Meta` `SSL` `NeurIPS`
	> Global + Local loss
* End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking `Nice` `UMD` `NeurIPS`
	> DesneNet inspired arch + use detach to force forget track of time (num iterations).
* Conditioned and composed image retrieval combining and partially fine-tuning CLIP-based features
	> fine-tune clip so that Query_Image + Cond_Text => Traget_Image. The text feature is regarded as a displacement from the Query_Image features to the Traget_Image features.
* Chain of Thought Prompting Elicits Reasoning in Large Language Models `NIPS`
	> Prompt engineering; provide chain of thoughts within prompt.
* Flamingo: a Visual Language Model for Few-Shot Learning `Google`
	> Use frozen vision/language models to learn a multi-modal model. Uses Preciever samplers to generate a fixed number of visual tokens. Add Cross-attention layers so the language's output is conditioned on previous images. 
* Training language models to follow instructions with human feedback `Neurips`
	> InstructGPT paper; after pre-training GPT; do supervised fine-tuning; train reward model (RM), use RM to fine-tune GPT using reinforcement learning.
* Self-Supervised Pretraining Improves Self-Supervised Pretraining `WACV` `WR` `JRC`
* Self-Instruct: Aligning Language Models with Self-Generated Instructions `ACL`
* Transformer Quality in Linear Time `Nice` `Google`
	> Leverage a weaker attention inside GLU/MLP to reduce dependency on Softmax attention within MSHA. We end up using 2* GAU instead of MSHA+MLP. GAU depends on both local and global attention to boost speed for auto-regressive (causal) attention models.
* Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training `MICCAI` `China`
	> Perform self-supervised training on medical images and report using masked autoencoders. Surprisingly, the paper reconstructs the image from different layers in the encoder -- not just the last layer!
