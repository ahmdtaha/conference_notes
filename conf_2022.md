* When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism `AAAI`
* A Random CNN Sees Objects: One Inductive Bias of CNN and Its Applications `AAAI`
* * Self-supervised Learning from 100 Million Medical Images `arXiv` `Medical` `Siemens`
* A ConvNet for the 2020s `arXiv`
* OMNIVORE: A Single Model for Many Visual Modalities `arXiv`
* Vision Transformer for Small-Size Datasets `arXiv`
* A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments `arXiv` `Meta`
* Exploring Plain Vision Transformer Backbone for Object Detection `arXiv` `ViTDet`
* Text and Code Embeddings by Contrastive Pre-Training `arXiv` `OpenAI`
* Convolutional Xformers for Vision `arXiv`
* ItemSage: Learning Product Embeddings for Shopping Recommendations at Pinterest `arXiv` `ML`

## ICLR
* Understanding Dimensional Collapse in Contrastive Self-supervised Learning `FB AI`
	> Bad conclusion, but worth reading
* UniFormer: Unifying Convolution and Self-attention for Visual Recognition
	> Technicalities
* Fortuitous Forgetting in Connectionist Networks `Mila`
* VOS: Learning What You Don't Know by Virtual Outlier Synthesis `Uncertainty`
* Relational Surrogate Loss Learning
* The Uncanny Similarity of Recurrence and Depth `Nice`
* PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
* Unsupervised Semantic Segmentation by Distilling Feature Correspondences


## CVPR
* Fine-tuning Image Transformers using Learnable Memory `Google` `Transformers` `Continual`
* QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection `Detection` `Technicalities`
* Delving Deep into the Generalization of Vision Transformers under Distribution Shifts `ViTs`
* SimMatch: Semi-supervised Learning with Similarity Matching `Technicalities`

## Generic
* Multi-Head Deep Metric Learning Using Global and Local Representations `WACV`
* When and how convolutional neural networks generalize to out-of-distribution categoryâ€“viewpoint combinations `Nature ML`
	> Nice analysis
* Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision `arXiv` `Meta`
	> Brute force
* One Network Doesn't Rule Them All: Moving Beyond Handcrafted Architectures in Self-Supervised Learning `arXiv` `SSL` `NAS`
* PVTv2: Improved Baselines with Pyramid Vision Transformer `CVMJ`
* Robust and Efficient Medical Imaging with Self-Supervision `arXiv` `Google` `Medical`