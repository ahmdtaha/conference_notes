# arXiv

* Scalable Pre-training of Large Autoregressive Image Models `Apple`
	> Pre-Training Image models similar to LLMs works. Use Prefix attention and heavy MLPs
* RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision `MS` `Med`
	> Vision only pre-training is competitive to Vision-language pre-training. On the importance of masking image modeling and multi-crop during pre-training.
* Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model  `WR` `JRC`
	> Use Mamba instead of vanilla attention. Hugo said it is slow during training. The paper is uploaded to arxiv as work in progress.
* GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection `Nice`
	> Project Gradient matrix to lower dimension; use the lower dimension to update optimizer state; project back to the original dimension to update weights. Reminds me of 2017 Goldstein visualization paper.
* Tuning Language Models by Proxy `AI2` `WU`
	> Tuning small models to guide un-tuned large models. Reminds me of GLIDE paper where gradient from a classifier network is used to guide diffusion models.
* When Do We Not Need Larger Vision Models? `Microsoft` `UC Berkeley`
	> Smaller models have the same capacity of large models. Yet, these need multiple image scales (S^2) to achieve the performance of large models.
* The Unreasonable Ineffectiveness of the Deeper Layers `FAIR` `Meta`
	> We can drop last layers for deep networks without much performance degradation. This shows that there are many parameters that are not necessary.
* On the Benefits of Over-parameterization for Out-of-Distribution Generalization
	> In Benign-overfitting regime, we overfit on noisy training data but still perform good on test data. An over-parameterized model can be regarded as a combination of a simple model capturing data pattern truly + complex models (spikes) capturing noise/outliers in the data. These complex models has minimal impact when evaluating on test data -- under the assumption that training noise/outliers points are independent of each other. This is indeed the case in natural distribution datasets (with noisy samples) but not for adversarially created datasets. 
* 94% on CIFAR-10 in 3.29 Seconds on a Single GPU `Nice`
	> How to speed CIFAR training using derandomized image flipping. Lookahead optimizer seems interesting.
* Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction `ByteDance`
	> Instead of Autoregressive (AR) used by GPT, this paper propose Visual Autoregressive (VAR) where entire image map scale is regressed based on (conditioned on) smaller scales. The approach is inspired by human drawing (generation) process where images are created "in a hierarchical manner, first capturing the global structure and then local details.", i.e., coarse-to-fine scales.
* Pre-training Small Base LMs with Fewer Tokens
	> Use Pre-trained large models (with k layers) to initialize a small base model (with n layers). Trains using 0.1% of training data -- just 1B tokens -- within 12 hrs.


# ICLR

* FeatUp: A Model-Agnostic Framework for Features at Any Resolution `MIT` `Google`
	> Learn better high-resolution dense features.
* Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training `Nice` `Stanford`
	> Second-order optimizer that estimates Hessian diagonal and leverage it as a pre-conditioner for gradient/learning-rate during training. Seems particularly valuable for vision transformers and diffusion models 
	
	
# CVPR
* OMG-Seg: Is One Model Good Enough For All Segmentation?
	> Mask2former prerequisite. Combine different segmentation tasks (semantic, instance, panoptic) into a single arch.
* VILA: On Pre-training for Visual Language Models `NVIDIA` 
	> Ablation studies: Tune the LLM, use linear projection for visual tokens. Interleaved data is better than image-text pairs, include text-only data during SFT.
* OpenEQA: Embodied Question Answering in the Era of Foundation Models `FAIR` 
	> EQA task formulation, dataset release, automatic LLM evaluation setup and baseline comparisons. LLMs achieves significantly better than random-guessing. LLMs equipped with vision capabilities achieve superior performance but in simple question-categories (e.g., object recognition). Complex question-categories (e.g., Spatial Understanding, Functional Reasoning) remain challenging.
*  What does CLIP know about peeling a banana? `Workshop`
	> Add FPN to CLIP to have coarse-to-fine visual to text matching then using it for object affordance.
* Probing the 3D Awareness of Visual Foundation Models
	> MAE, CLIP, SigLIP suffers on 3D Awareness, while DINOv2 and StableDiffusion are better (Depth and Surface normal estimation). They all suffer on Multi-view consistency. 