# arXiv

* Scalable Pre-training of Large Autoregressive Image Models `Apple`
	> Pre-Training Image models similar to LLMs works. Use Prefix attention and heavy MLPs
* RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision `MS` `Med`
	> Vision only pre-training is competitive to Vision-language pre-training. On the importance of masking image modeling and multi-crop during pre-training.
* Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model  `WR` `JRC`
	> Use Mamba instead of vanilla attention. Hugo said it is slow during training. The paper is uploaded to arxiv as work in progress.
* GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection `Nice`
	> Project Gradient matrix to lower dimension; use the lower dimension to update optimizer state; project back to the original dimension to update weights. Reminds me of 2017 Goldstein visualization paper.
* Tuning Language Models by Proxy `AI2` `WU`
	> Tuning small models to guide un-tuned large models. Reminds me of GLIDE paper where gradient from a classifier network is used to guide diffusion models.
* When Do We Not Need Larger Vision Models? `Micrsoft` `UC Berkeley`
	> Smaller models have the same capacity of large models. Yet, these need multiple image scales (S^2) to achieve the performance of large models.
* The Unreasonable Ineffectiveness of the Deeper Layers `FAIR` `Meta`
	> We can drop last layers for deep networks without much performance degradation. This shows that there are many parameters that are not necessary.	

# ICLR

* FeatUp: A Model-Agnostic Framework for Features at Any Resolution `MIT` `Google`
	> Learn better high-resolution dense features.