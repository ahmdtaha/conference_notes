# Generic

* OmniVec: Learning robust representations with cross modal sharing
	> Train with multiple modalities (Video, text, image, audio) on multiple tasks (classification, segmentation).
* The limits of fair medical imaging AI in real-world generalization `Medicine` `Nature`
	> Medical image models leverage demographic details (age and race) as shortcuts to achieve better performance. These shortcuts undermine faireness across subgroups. Optimizing for in-distribution fairness doesn't translate to out-of-distribution fairness. For OOD fairness, the paper prefers a model that encodes the least demographic information. Basically, Pick a model that can't predict age/race on ID. This model will probably do better on OOD.
* Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding
	> Use a smaller reference/online models to score examples for the learner model. The reference model is trained before the learner while the online model is trained simultanously with the learner. The reference model identifies easy samples (small loss), while the online model identifies hard samples -- haven't been learned yet. 
* Mammo-CLIP: A Vision Language Foundation Model to Enhance Data Efficiency and Robustness in Mammography `MICCAI`
	> Train a vision-language model for medical images using CLIP. I am not sure why they call the model foundational since they pre-train EfficientNet only!
* SIFT-DBT: Self-supervised Initialization and Fine-Tuning for Imbalanced Digital Breast Tomosynthesis Image Classification `ISBI` `Yale`
	> SSL on TOMO volumes using slab/frame wise contrastive learning. During fine-tuning, use bbox annotation to fine-tune on patches. During inference, aggregate scores from patches within a slab/frame, use the slab with max-score as the key slab.

	
# arXiv

* Scalable Pre-training of Large Autoregressive Image Models `Apple`
	> Pre-Training Image models similar to LLMs works. Use Prefix attention and heavy MLPs
* RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision `MS` `Med`
	> Vision only pre-training is competitive to Vision-language pre-training. On the importance of masking image modeling and multi-crop during pre-training.
* Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model  `WR` `JRC`
	> Use Mamba instead of vanilla attention. Hugo said it is slow during training. The paper is uploaded to arxiv as work in progress.
* GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection `Nice`
	> Project Gradient matrix to lower dimension; use the lower dimension to update optimizer state; project back to the original dimension to update weights. Reminds me of 2017 Goldstein visualization paper.
* Tuning Language Models by Proxy `AI2` `WU`
	> Tuning small models to guide un-tuned large models. Reminds me of GLIDE paper where gradient from a classifier network is used to guide diffusion models.
* When Do We Not Need Larger Vision Models? `Microsoft` `UC Berkeley`
	> Smaller models have the same capacity of large models. Yet, these need multiple image scales (S^2) to achieve the performance of large models.
* The Unreasonable Ineffectiveness of the Deeper Layers `FAIR` `Meta`
	> We can drop last layers for deep networks without much performance degradation. This shows that there are many parameters that are not necessary.
* On the Benefits of Over-parameterization for Out-of-Distribution Generalization
	> In Benign-overfitting regime, we overfit on noisy training data but still perform good on test data. An over-parameterized model can be regarded as a combination of a simple model capturing data pattern truly + complex models (spikes) capturing noise/outliers in the data. These complex models has minimal impact when evaluating on test data -- under the assumption that training noise/outliers points are independent of each other. This is indeed the case in natural distribution datasets (with noisy samples) but not for adversarially created datasets. 
* 94% on CIFAR-10 in 3.29 Seconds on a Single GPU `Nice`
	> How to speed CIFAR training using derandomized image flipping. Lookahead optimizer seems interesting.
* Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction `ByteDance`
	> Instead of Autoregressive (AR) used by GPT, this paper propose Visual Autoregressive (VAR) where entire image map scale is regressed based on (conditioned on) smaller scales. The approach is inspired by human drawing (generation) process where images are created "in a hierarchical manner, first capturing the global structure and then local details.", i.e., coarse-to-fine scales.
* Pre-training Small Base LMs with Fewer Tokens
	> Use Pre-trained large models (with k layers) to initialize a small base model (with n layers). Trains using 0.1% of training data -- just 1B tokens -- within 12 hrs.
* Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models `Nice` `Reka`
	> Use Group Query attention, SwiGLU, RMSNorm, Rotary positional embeddings. Also, use high learning rate, and Ceph filesystem.
* Capabilities of Gemini Models in Medicine `Nice` `Google` 
	> Fine-tune Gemini into Med-Gemini to leverage reason capabilities, multi-modal support, and long-context support. On top of that, the paper uses web-search api to train models on advanced medical reasoning. The paper also performs uncertainty-guided search during inference. The paper promising so much. Not sure what is their focus.
* Chameleon: Mixed-Modal Early-Fusion Foundation Models `Meta`
	> Large multi-modal model (LMM) for both image and text. They seem to struggled with training divergence. They changed the order of normalizatin layer, used dropout, and z-loss regularization.
* The Platonic Representation Hypothesis `Nice`
	> Large/foundational models learn a univeral (similar) representation. The paper delivers a great literature review. Yet, the arguments presented are hand-wavy. These arguments are not even falsifiable. E.g., if a large model learning a different representation, the authors would say that the model is not large enough (low capacity) or the task is not complex enough. Still it is worth reading paper.
* Learning by Reconstruction Produces Uninformative Features For Perception `NYU`
	> reconstruction are not ideal for learning features useful for perception. Reconstruction can learn good features but it needs long training (slow) and high capacity encoder (encoding dimension-> data dimension). Masking helps learn better features compared to additive Gaussian noise. Adding a classification (perception) loss on top of the reconstruction loss is helpful -- assuming labels are available.
* Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection `IDEA`
	> A large (foundational) object detection model developed by IDEA labs in China. Little technical details are provided. Promote early fusion over late fusion. Propose some tricks to deliver an Edge model with less computational cost (faster inference).
* ShortGPT: Layers in Large Language Models are More Redundant Than You Expect `China`
	> Use cosine-similarity metric -- called Block influenece -- to identify redundant layers. Prune layers with low Block influenece.
* Grokfast: Accelerated Grokking by Amplifying Slow Gradients
	> Decompose gradient into slow and fast components. Amplifying the slow component to speed convergence (Grokking). Surprisingly, we still need to keep the fast gradient component. Seems like the slow component is responsible for generalization while the fast gradient is responsible for memorization.
* LoRA Learns Less and Forgets Less `Databricks`
	> Beside the title message, LoRA acts are a regularizer. LoRA falls behind full-finetuning when there is a large gap between target and source domain. It is recommeneded to apply LoRA on all modules (attention+MLP) with r=16.
* Just How Flexible are Neural Networks in Practice? `NYU`
	> Poorly written, but many surprising findings: SGD can fit more samples than Adam; there is a strong correlation between fitting-capacity and generalization; architecture prior is more important than the number of parameters. " neural networks, in their standard form, are wasteful of parameters."; Fitting random labels becomes easier as the number of classes increases;

* The Road Less Scheduled `Meta` `Google` `Samsung`
	> Polyak-Ruppert averages a sequence of model after training; Primal averaging uses gradient from an online averaged model -- instead of the fast iterative model. These averaging methods ought to deliver better models but practically they don't. This paper combines both approaches to deliver a better model. The paper shows that the theortical Polyak-Ruppert generalization can be achieved with the right learning rate scheduler. In another words, learning rate schedulers acts as model averaging. Still, Polyak-Ruppert is not enough as it is susceptible to gradient noise. So the paper combines Polyak-Ruppert with Primal averaging that acts like gradient momentum -- stabilizing gradient. This leads to better performance without assuming the duration of training (number of epochs). Basically, this combination eliminates the need for a learning rate scheduler. 
* Data curation via joint example selection further accelerates multimodal learning `DeepMind`
	> Speedup contrastive learning through an online batch-based filtering method. Identify hard and easy samples that speed up convergence.
* Vision language models are blind
	> VL models are blinds in non-natural image settings. These models suffer on images with primitive shapes (e.g., lines, circle) when evaluated on fine-grained details (# line-interesections, circle touch or not).
* MambaVision: A Hybrid Mamba-Transformer Vision Backbone `NVidia`
	> Create a hybrid stages with state space model (SSM) attention at early blocks and vanilla transformor at late blocks. Not available for commercial use.
* SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models `Apple` `MLLM`
	> A training-free baseline for Video LLMs using Image LLMs. All components of frozen (no training at all), just sample the video carefully to get competitive performance.
* XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training `Australian Artificial Intelligence Institute`
	> Combine contrastive learning (CLIP-style) with masked image/language modeling. Use langauge/image to pick a hard-mask, i.e., no random masking. Feed the entire image into the encoder which limits the value of MAE. All experiments performed on Chest-Xray images.
* Shared Imagination: LLMs Hallucinate Alike `Salesforce`
	> LLMs give the same answers -- significantly above random chance -- to fictional questions. Reminds me of The Platonic Representation Hypothesis.
* Are Bigger Encoders Always Better in Vision Large Models? `Peking` `China` `Uni`
	> Some trival conclusions: more data boost performance; data quality is important. Some "surprising" conclusions: Large models do better on small datasets compared to small models on large datasets; A superior VIT doesn't necessarily translate to better vision-langauge model performance.
* MiniGPT-v2: Large Language Model As a Unified Interface for Vision-Language Multi-task Learning `KAUST` `Saudi Arabia` `Meta`
	> Employ three-stage training procedure; The first stage trains on general (broad) vision-language datasets. The second stage trains on fine-grained datasets. The third stage trains on multi-modal instruction tuning.
* How Well Can Vision Language Models See Image Details? `KAUST` `Saudi Arabia` `Monash` `Australia`
	> Train multi-modal LLMs (MLLM) to predict rgb values of pixel. This task boosts MLLM performance on referring image segmentation and video game playing.
* Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents `CMU` `Salesforce`
	> Use agents ensemble to judge coding agents and pick the right solution.
* 1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data `Pints` `Singapore`
	> Focus on data quality to reduce training time. Focus on reasoning and skills capabilities to train LLMs, then use RAG to keep the LLM up-to-date.

# ICLR

* FeatUp: A Model-Agnostic Framework for Features at Any Resolution `MIT` `Google`
	> Learn better high-resolution dense features.
* Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training `Nice` `Stanford`
	> Second-order optimizer that estimates Hessian diagonal and leverage it as a pre-conditioner for gradient/learning-rate during training. Seems particularly valuable for vision transformers and diffusion models 
* GAIA: A Benchmark for General AI Assistants `Meta`
	> Easy questions for humans but difficult for LLMs. Unambiguous questions with a sinlge correct answers that are hard to be found in training data in plain text.
* Why is SAM Robust to Label Noise?
	> SAM's full potential happens in noisy label setup. SAM scales gradient for clean samples (low-loss sample), so clean samples contribute more with SAM. Yet, this is not the main driving force. SAM's main force comes from perturbing the network jacobian. Such perturbation regularize the final layer weights and intermediate activations.
* Transferring Labels to Solve Annotation Mismatches Across Object Detection Datasets `Nvidia`
	> Solve annotation mismatch between a source and a target dataset. The paper casts this problem as a supervised problem formulated (x,y',y) where y' is the source dataset label, and y is the target dataset label. Hard to read!
* A simple and effective pruning approach for large language models. `Meta`
	> Pruning without fine-tuning. Assign importance based on both weight and activation norms. Pruning using weight norm only is detrimental for LLMs 
* Vision Transformers Need Registers `Meta`
	> Large ViT (e.g., ViT-L) repurpose tokens with little information (e.g., background) as a summary tokens (similar to cls-token). This degrades performance on dense prediction tasks (e.g., localization and segmentation). The paper propose a dedicated register tokens that can be used as summary tokens.
	
# CVPR
* OMG-Seg: Is One Model Good Enough For All Segmentation?
	> Mask2former prerequisite. Combine different segmentation tasks (semantic, instance, panoptic) into a single arch.
* VILA: On Pre-training for Visual Language Models `NVIDIA` 
	> Ablation studies: Tune the LLM, use linear projection for visual tokens. Interleaved data is better than image-text pairs, include text-only data during SFT.
* OpenEQA: Embodied Question Answering in the Era of Foundation Models `FAIR` 
	> EQA task formulation, dataset release, automatic LLM evaluation setup and baseline comparisons. LLMs achieves significantly better than random-guessing. LLMs equipped with vision capabilities achieve superior performance but in simple question-categories (e.g., object recognition). Complex question-categories (e.g., Spatial Understanding, Functional Reasoning) remain challenging.
*  What does CLIP know about peeling a banana? `Workshop`
	> Add FPN to CLIP to have coarse-to-fine visual to text matching then using it for object affordance.
* Probing the 3D Awareness of Visual Foundation Models
	> MAE, CLIP, SigLIP suffers on 3D Awareness, while DINOv2 and StableDiffusion are better (Depth and Surface normal estimation). They all suffer on Multi-view consistency. 
* Tyche: Stochastic In-Context Learning for Medical Image Segmentation `MIT` `WR` `JRC`
	> The paper proposes a foundational model for medical image segmentation that support in-context (prompt) learning. The proposed model can generate multiple stochastic segmentations to mimic different human annotation. I found the paper confusing and hard to read. The architecture's details are not clear. Applying conv operation on different non-registered images makes no sense.
* A Vision Check-up for Language Models `MIT`
	> LLMs are able to generate and -- barely -- discriminate images. The paper encodes images using text/code (e.g., tikz). LLMs are very good with toy shapes (e.g. squares), suffers more with objects and scenes. LLM struggles with spatial organization of objects within a scene.
* Hybrid Proposal Refiner: Revisiting DETR Series from the Faster R-CNN Perspective `Australia`
	> Propose a feature-proposal refiner by combining different approaches in literature (e.g., deformable attention, dynamic kernel, region attention). The paper also shows that Hungarian matching learns a sharp localized features which is not suitable for ROI align.
* VideoMAC: Video Masked Autoencoders Meet ConvNets `China` `Academia`
	> Train CNN backbone on videos using MAE. The key ideas: Use sparse ConvNext to prevent information leakage between patches. Enforce a temporal reconstruction consistency loss between nearby video frames. I am not sure why this is proposed for video because there is no temporal modeling! If I understand this paper correctlt, every frame is processed individually. 
	

# ECCV
	
* X-Former: Unifying Contrastive and Reconstruction Learning for MLLMs `AMZN`
	> A new transformer block that enriches visual representation of frozen CLIP-based and MIM-based models with fine-grained details.