# arXiv

* Scalable Pre-training of Large Autoregressive Image Models `Apple`
	> Pre-Training Image models similar to LLMs works. Use Prefix attention and heavy MLPs
* RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision `MS` `Med`
	> Vision only pre-training is competitive to Vision-language pre-training. On the importance of masking image modeling and multi-crop during pre-training.
* Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model  `WR` `JRC`
	> Use Mamba instead of vanilla attention. Hugo said it is slow during training. The paper is uploaded to arxiv as work in progress.
* GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection `Nice`
	> Project Gradient matrix to lower dimension; use the lower dimension to update optimizer state; project back to the original dimension to update weights. Reminds me of 2017 Goldstein visualization paper.
* Tuning Language Models by Proxy `AI2` `WU`
	> Tuning small models to guide un-tuned large models. Reminds me of GLIDE paper where gradient from a classifier network is used to guide diffusion models.
* When Do We Not Need Larger Vision Models? `Micrsoft` `UC Berkeley`
	> Smaller models have the same capacity of large models. Yet, these need multiple image scales (S^2) to achieve the performance of large models.
* The Unreasonable Ineffectiveness of the Deeper Layers `FAIR` `Meta`
	> We can drop last layers for deep networks without much performance degradation. This shows that there are many parameters that are not necessary.
* On the Benefits of Over-parameterization for Out-of-Distribution Generalization
	> In Benign-overfitting regime, we overfit on noisy training data but still perform good on test data. An over-parameterized model can be regarded as a combination of a simple model capturing data pattern truely + complex models (spikes) capturing noise/outliers in the data. These complex models has minimal impact when evaluating on test data -- under the assumption that training noise/outliers points are independent of each other. This is indeed the case in natural distribution datasets (with noisy samples) but not for adversarially created datasets. 
* 94% on CIFAR-10 in 3.29 Seconds on a Single GPU `Nice`
	> How to speed CIFAR training using derandomized image flipping. Lookahead optimizer seems interesting.



# ICLR

* FeatUp: A Model-Agnostic Framework for Features at Any Resolution `MIT` `Google`
	> Learn better high-resolution dense features.
	
	
# CVPR
* OMG-Seg: Is One Model Good Enough For All Segmentation?
	> Mask2former prerequisite. Combine different segmentation tasks (semantic, instance, panoptic) into a single arch.
* VILA: On Pre-training for Visual Language Models `NVIDIA` 
	> Ablation studies: Tune the LLM, use linear projection for visual tokens. Interleaved data is better than image-text pairs, include text-only data during SFT.