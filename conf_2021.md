### ICCV 2021

* Is it Time to Replace CNNs with Transformers for Medical Images? `Workshop`
* DINO: Emerging properties in self-supervised vision transformers. `Facebook` 
	> Revisited
* End-to-End Semi-Supervised Object Detection with Soft Teacher. `Microsoft` 
* Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet
	> propose overlapping token -- poorly written
* Pyramid vision transformer: A versatile backbone for dense prediction without convolutions -- `Technicalities` `ViT` `Det`
* Big Self-Supervised Models Advance Medical Image Classifications `Medical` `Google` `Nice`
	> Use ImageNet as initialization before self-supervised learning to speed up convergence. Use positive crops for multi-view images. partial (light-weight) augmentationis enough if positive pairs are sampled efficiently (Tab. B.2). SSL help with model robustness (Fig. 4).
* Divide and Contrast: Self-supervised Learning from Uncurated Data `DeepMind` `SSL`
* With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations `SSL` `DeepMind` `Google`
* Dynamic detr: End-to-end object detection with dynamic attention `Microsoft` `Technicalities`
	> Same team and ideas from Dynamic Head (CVPR 2021)
* Visual Transformers: Where Do Transformers Really Belong in Vision Models? `Facebook`	> Tricky	
* Going deeper with Image Transformers `Facebook` `Deep` `Transformers`
* Rethinking imagenet pre-training `FAIR` `Nice` `Detection`
* LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference `Transformers` `Facebook`
* Co-Scale Conv-Attentional Image Transformers `Transformers` `detection` `Oral`
* Detail Me More: Improving GANâ€™s photo-realism of complex scenes `GANs` `AMZN`
* When do GANs replicate? On the choice of dataset size `GANs` `AMZN`
* Multiscale Vision Transformers `FAIR` `Video`
* CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification `MvT` `IBM`
	> Multiscale vision transformer evaluated using classification task only!
* Pretrained ViTs Yield Versatile Representations For Medical Images `Workshop` `Medical` `ViTs`
	> ViTs attention is remarkable
* Instances as Queries `Segmentation` `Detection` `Nice`
	> Use the same query objects (as in DETR) inside DynamicConv to generate object masks. one query -> one object -> mask correspondence. Well Written
* Effectively leveraging attributes for visual similarity.
	> conditional metric learning done using joint image-pair feature embedding.
* Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models
	> Composed image retrieval using pre-trained VLP models.
* An Empirical Study of Training Self-Supervised Vision Transformers `Meta` `Oral`
	> Moco v3 drops key queue, bz=4096 seems enough! Besides backbone+projection, a prediction MLP is added. AdamW seems more stable compared to LAMB. Random patch projection is enough to learn a good representation even without positional embedding. 
* Region Similarity Representation Learning `UC-Ber` `WR` `JRC`
	> Contrastive learning on region-level for localization tasks. This paper relates to `2202.04639`
* ViViT: A Video Vision Transformer `Google`
	> Train Video Video on tubelets. Factorize attention operation to reduce the model's computational cost. E.g., spatial then temporal attention or splits heads so that half perform spatial while the other half perform temporal attention.

* Contrast and Order Representations for Video Self-supervised Learning
	> On the importance of modeling temporal order for videos. 

* Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification `USA` `Academia`
	> AUC loss formulated as a ranking loss: The positive points should rank higher than negative points by a margin m. The high level idea is interesting, but the technical details is a hurdle. The paper formulates the problem as a min-max optimization. Thus, a special optimizer (e.g., PESG) is required. Furthermore, the proposal requires a two-stage training process: (1) using cross-entropy, (2) then using AUC loss!


### WACV 2021
* ResNet or DenseNet? Introducing Dense Shortcuts to ResNet -- `Technicalities`

### MICCAI 2021

* CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation
	> Transformers + Medical Images


### ICLR 2021
* Concept Learners For Few-Shot Learning 
* SEED: Self-supervised Distillation For Visual Representation
* Unbiased Teacher for Semi-Supervised Object Detection
* Rethinking Attention With Performers `Google`
* Sharpness-Aware Minimization For Efficiently Improving Generalization -- `WR-AI` `Google`
* Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning 
	> Did it before it was cool
* What Should Not Be Contrastive In Contrastive Learning
	> Brute Force	
* Grokking: Generalization beyond Overfitting on small algorithmic datasets `Workshop` 
	> Grokking can be observed on toy datasets and transformer models. Weight decay help speedup generalization (Grokking).
* Deformable DETR: Deformable Transformers for End-to-End Object Detection `Oral`
	> Replace vanilla self-attention in DETR with deformable conv to speed up convergence and reduce computational complexity.
* NFNets: High-Performance Large-Scale Image Recognition Without Normalization `DeepMind`
* An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale `PI_Reading_Grp` `Google`
* On the geometry of generalization and memorization in deep neural networks `Nice` `Intel`
	> explain double-descent, early layers generalize while late layers memorize, lack of gradient magnitude differences between early and late layers
* Does enhanced shape bias improve neural network robustness to common corruptions? `Nice`
	> Nice analysis; style augmentation improves robustness corruptions, while enhancing shape bias makes networks more vulnerable


	
### Generic 2021
* Why AI is Harder Than We Think `arXiv`
* ClipCap: CLIP Prefix for Image Captioning `arXiv`
	> Combine CLIP model and a LLM to train a light-weight captioning model.
* Extracting Training Data from Large Language Models `usenix` `LLM`
* How Much Can CLIP Benefit Vision-and-Language Tasks? `arXiv` `CLIP`
* Evaluating Large Language Models Trained on Code `arXiv` `OpenAI`
* Vision Transformers with Patch Diversification `arXiv` `Transformers` `Nice`
	> Apply patch-level regularization.
* MetaFormer is Actually What You Need for Vision `arXiv` `Transformers`
	> Probably wrong conclusion
* RegNet: Self-Regulated Network for Image Classification `arXiv` `Architecture`
* RoFormer: Enhanced Transformer with Rotary Position Embedding `arXiv` `Transformers`
* TransAttUnet: Multi-level attention- guided u-net with transformer for medical image segmentation `arXiv` `Transformers` `Medical` 
* Florence: A New Foundation Model for Computer Vision `arXiv`
* Stochastic Training is Not Necessary for Generalization `arXiv`
* TransClaw U-Net: Claw u-net with transformers for medical image segmentation. `arXiv` `Transformers`  `Medical` 
* TransUNet: Transformers make strong encoders for medical image segmentation `arXiv` `Transformers`  `Medical` 
* Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation `Transformers`  `Medical` 
* TransFuse: Fusing transformers and cnns for medical image segmentation `MICCAI` `Transformers`
* Convolution-free medical image segmentation using transformers `MICCAI` `Transformers`
* Medical Transformer: Gated Axial-Attention for Medical Image Segmentation `MICCAI` `Transformers`
* Multi-view Analysis of Unregistered Medical Images Using Cross-View Transformers `MICCAI` `Transformers`
* Token Pooling In Vision Transformers `arXiv` `Transformers` 
* MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer `arXiv` `Transformers`
* LocalViT: Bringing Locality to Vision Transformers `arXiv` `Transformers`
	> Encode location using DW Conv. Important for early layers
* Not All Knowledge Is Created Equal `arXiv` 
* Patches are all you need `OpenReview` 
	> Smaller patches boost performance
* Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity `arXiv` `DETR` 
* When Vision Transformers Outperform Resnets Without Pre-Training Or Strong Data Augmentations `arXiv` `Transformers` 

* Ranger21: a synergistic deep learning optimizer `arXiv` `Adam` `Optimizer`
	> Combine eight-optimizer extensions to Adam optimizers to deliver a stabler/better version (ranger2021).
* Do Vision Transformers See Like Convolutional Neural Networks? `arXiv` `Transformers` 
* CvT: Introducing convolutions to vision transformers `arXiv``Transformers` 
* Augmenting Convolutional networks with attention-based aggregation `Meta` `arXiv`
	> Seems to have stability issues 
* SLIP: Self-supervision meets Language-Image Pre-training `Meta` `arXiv`
* Object-aware Contrastive Learning for Debiased Scene Representation `NIPS` 
* All Tokens Matter: Token Labeling for Training Better Vision Transformers `NIPS` `ByteDance`
	> Use pre-trained models to distill knowledge and apply classification loss in all tokens and not just the class-token.

* Efficient Training of Visual Transformers with Small-Size Datasets `NIPS`
	> Training on hybrid architectures (Conv+Attentiion) on small dataset and randomly initialized networks. Add a self-supervised loss to speed convergence.

* Unsupervised object-level representation learning from scene images `NIPS` 
	> Use ssl on image-pretraining to retrieval KNN for images. Perform selective search to extract regions from every image. Match regions from one images to regions from a paired (KNN) images. Perform ssl on region-pretraining.

* Intriguing Properties of Vision Transformers `NIPS` `Nice` `Google`
	> ViTs are neither shape nor textured biased. They just less biased towards local textures. There is a shape-bias vs. robustness!

* XCiT: Cross-Covariance Image Transformers `NIPS` `Faceboook`
	> Replace Attention Kernel (Gram) with Co-variance matrix. Mixing channels instead of mixing tokens. "communication between patches is only implicit through the shared statistics".

* CoAtNet: Marrying Convolution and Attention for All Data Sizes `NIPS`
* Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers `NIPS` `RNNs`
* Twins: Revisiting the Design of Spatial Attention in Vision Transformers `NIPS` `ViT` `PvT` `MvT`
	> Use conditional position encoding (CPE) instead of PE + Subsample with Conv. Twins-PCPVT-L seems promising. 
* Overinterpretation reveals image classification model pathologies `Nice` `NIPS` 
* Does Knowledge Distillation Really Work? `NIPS`
* ReAct: Out-of-distribution Detection With Rectified Activations `NIPS` `WR` `JRC`
	> A simple idea to identify OOD samples using penultimate-layers activations in pre-trained models. post-hoc ood method that requires gray-box access for trained models. Seems to assume batchnorm layers.
* Is the Number of Trainable Parameters All That Actually Matters?  `Workshop` `NIPS` `NICE`
	> Only trainable dense-weights count; frozen/structured weights don't

* Focal attention for long-range interactions in vision transformers. `Nice` `MSR`
	> Dedicated arch for object detection. Pyramid-ViT with better performance but more compute (FLops).
* Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks `Nice` `NIPS` 
	> Replace feed-forward stages inside ResNet with a single recurrent stage.
* SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers
	> A segmentation decoder on top of PVT.
* Learning Debiased Representation via Disentangled Feature Augmentation `Nice` `NIPS` 
	> Train two networks: (1) learns bias features, (2) learns intrinsic features.
* Deep Learning on a Data Diet: Finding Important Examples Early in Training `NIPS` 
	> Grad and error ||y-\hat{y}|| norms can identify important examples. All experiments done on small datasets (CIFAR).
* Align before Fuse: Vision and Language Representation Learning with Momentum Distillation `SalesForce` `VLP`
* One Pass ImageNet `NIPS`  `Workshop`  
* Understanding Self-Supervised Learning Dynamics without Contrastive Pairs `ICML`
* Perceiver: General Perception with Iterative Attention `ICML`
* EfficientNetV2: Smaller Models and Faster Training `ICML`
	> progressively increase image-size + augmentation; replace DWConv at early layers with Fused-MBConv; add more layers to later stages (stage 5/6 in EfficientNet)
* Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision `ICML` `Google`
* Relative Positional Encoding for Transformers with Linear Complexity `ICML` `Transformers`  -- Tricky one
* Barlow Twins: Self-Supervised Learning via Redundancy Reduction `ICML`
* Self-training Improves Pre-training for Natural Language Understanding **#naacl2021**
* An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization -- `NYU` **Medical image analysis**
* Robust breast cancer detection in mammography and digital breast tomosynthesis using an annotation-efficient -- `DeepHealth` **Nature Medicine**
* An improved mammography malignancy model with self-supervised learning -- `WR-AI` **SPIE Medical Imaging**
* MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation -- `WR-AI` **MLHC2021**
* CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer -- `KTH` **openreview2021**
* Evaluating Subgroup Disparity Using Epistemic Uncertainty In Mammography **#icml2021** `workshop`
* A Data Set and Deep Learning Algorithm for the Detection of Masses and Architectural Distortions in Digital Breast Tomosynthesis **#nlm2021** 
* Are Convolutional Neural Networks or Transformers more like human vision? **#CogSci2021**
* TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning **@ Computer Methods and Programs in Biomedicine** 
* Evaluation of deep learning-based artificial intelligence techniques for breast cancer detection on mammograms: Results from a retrospective study using a BreastScreen Victoria dataset `Medical`
* Toward robust mammography-based models for breast cancer risk `Medical` `Sci Transl Med` `Nice`
* Stand-Alone Use of Artificial Intelligence for Digital Mammography and Digital Breast Tomosynthesis Screening: A Retrospective Evaluation `RSNA` `Medical`
* PonderNet: Learning to Ponder `ICMLW`

* Is space-time attention all you need for video understanding? `ICML`
	> Train a divided temporal-spatial attention on top of 2d frame representations.
* Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples `IsoMax` `IEEE Neural Networks and Learning Systems`
* Public patient views of artificial intelligence in healthcare: A nominal group technique study `Medical` `Healthcare` `WR`
* Unifying vision-and-language tasks via text generation `VLP` `PMLR`
* Deep Transfer Learning for Multi-Source Entity Linkage via Domain Adaptation `AMZN` `EL` `VLDB`
* Image Composition Assessment with Saliency-augmented Multi-pattern Pooling `BMVC`
	> Evaluate image aesthetic using composition patterns

* [Receptance Weighted Key-Value] https://github.com/BlinkDL/RWKV-LM `Git`
	> followup on `Apple` Attention Free Transformer (AFT) 

* Medical Image Dataset Rebalancing via Conditional Deep Convolutional Generative Adversarial Networks (cDCGANs) `WR` `Stanford` `Intern`
	> Applications of conditional GANs in medical images.

* Trainable summarization to improve breast tomosynthesis classification. `MICCAI` `WR`
	> Temporal attention across slices before feeding into a classifier.

* GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models `OpenAI`
	> Text-conditional diffusion model to OpenAI that beats GANs (DALI-E).

* Retrieval Augmentation Reduces Hallucination in Conversation `ACL` `Facebook`
	> use retrieval-augmentated generation (RAG) to reduce hallucination. Using query, find related document. Feed the document along with the query to a language model.

* Quantifying and leveraging predictive uncertainty for medical image assessment `MIA` `Siemens` `Harvard`
	> Quantify uncertainty without sampling (dropout/ensemble) using Evidential DL. The estimated uncertainty is used to identify challenging samples in the test split and noisy samples in the training split.

* Second opinion needed: communicating uncertainty in medical machine learning `Nature` `NPJ Digital Medicine`
	> Review for different uncertainty estimation approaches. The paper motivates the importance of decision abstention

* AI-based Strategies to Reduce Workload in Breast Cancer Screening with Mammography and Tomosynthesis `RSNA` `Radiology` `ES`
	> A rule-in/out evaluation for Transpara from ScreenPoint Medical on a single site in Spain. A small study population with only 113 cancers.
	
* W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training `US` `MIT` `Google` `Automatic Speech Recognition and Understanding` `Workshop`
	> Combining Contrastive learning and masking for audio representation learning. Quantization seems important along with conformer block.