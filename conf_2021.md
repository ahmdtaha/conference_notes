### arXiv 2021

* Re-labeling ImageNet:from Single to Multi-Labels, from Global to Localized Labels -- `PI_Reading_Grp`
* Bottleneck Transformers for Visual Recognition -- `PI_Reading_Grp` Technicalities
* Towards General Purpose Vision Systems -- `PI_arXiv_Grp` Technicalities
* Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision -- `PI_arXiv_Grp` Technicalities
* Learning Rate Grafting: Transferability of Optimizer Tuning
	> Optimize using two optimizers: one optimizer provides the magnitude while the other provides the direction.
* MLP-Mixer: An all-MLP Architecture for Vision `Arch_Design`
* FNet: Mixing Tokens with Fourier Transforms  `Arch_Design`
* Learning Open-World Object Proposals without Learning to Classify `Detection`
* nnFormer: Interleaved Transformer for Volumetric Segmentation `Detection`
* Early Convolutions Help Transformers See Better  `Transformer`
* Escaping the Big Data Paradigm with Compact Transformers `Transformer`
* Efficient Training of Visual Transformers with Small-Size Datasets  `Transformer`
* VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning `SSL`
* Swin transformer: Hierarchical vision transformer using shifted windows `Transformer`
* ResNet strikes back: An improved training procedure in timm 
* Do Self-Supervised and Supervised Methods Learn Similar Visual Representations?
* Self-supervised pretraining of visual features in the wild `FAIR`
* Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers `Google/DeepMind`
* ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases `FAIR` `Transformer`
* Open-Set Recognition: A Good Closed-Set Classifier is All You Need
* Extrapolating from a Single Image to a Thousand Classes using Distillation
* Are Large-scale Datasets Necessary for Self-Supervised Pre-training? `META` `arXiv`
* Zero-Shot Text-to-Image Generation `arXiv` `dall-e` `OpenAI` `dVAE` `Transformers`
* MPViT : Multi-Path Vision Transformer for Dense Prediction `arXiv` `Transformer`
* Masked Feature Prediction for Self-Supervised Visual Pre-Training `Video` `SSL` `Meta`

* DeepViT: Towards Deeper Vision Transformer `ViT`
	> Nice analysis
* SCD: A Stacked Carton Dataset for Detection and Segmentation
	> 16,136 (8401 online + 7735 collected) with 250,000 instance masks (semantic segmentation).
* Deep learning through the lens of example difficulty `Google` `Nice`
	> the prediction depth
* An Attention Free Transformer `Apple`
	> I am not sure if this scales beyond tiny models/inputs.
* SA-GD: Improved Gradient Descent Learning Strategy with Simulated Annealing 
	> Apply gradient ascent stochastically at later stage of training to escape local minima/saddle points.
* Benchmarking detection transfer learning with vision transformers `Meta`
	> MAE and BEiT pre-training benefit detection.
* Rethinking Self-Supervised Learning: Small is Beautiful
	> The paper promotes small dataset/resolution/arch learning to achieve competitive performance with limited compute. Not sure if the paper conclusions/findings are valid, but the SSL pre-training on combined small and large resolutions makes sense.
* Vision Transformer Pruning
	> Pruning ViT by learning an importance score for every dimension d durnig training. unimportant dimensions are pruned and a final fine-tuned stage is performance to recover lost accuracy. An l1-regularization is applied on importance score.
* Bad Global Minima Exist and SGD Can Reach Them `Mila` `Canada` `Greece` `USA` `Academia`
	> Bad minima can be reached by pre-training on noisy dataset (with noisy label) then using vanilla SGD without any augmentation or reguralization or momentum. During noisy pre-training, the network learns a complex features and a complex decision boundary. During the second training stage, SGD stays close to inferior initialization -- it doesn't generalize to test/val splits.
	
### Generic 2021

* Why AI is Harder Than We Think `arXiv`

* ClipCap: CLIP Prefix for Image Captioning `arXiv`
	> Combine CLIP model and a LLM to train a light-weight captioning model.
* Extracting Training Data from Large Language Models `usenix` `LLM`
* How Much Can CLIP Benefit Vision-and-Language Tasks? `arXiv` `CLIP`
* Evaluating Large Language Models Trained on Code `arXiv` `OpenAI`
* Vision Transformers with Patch Diversification `arXiv` `Transformers` `Nice`
	> Apply patch-level regularization.
* MetaFormer is Actually What You Need for Vision `arXiv` `Transformers`
	> Probably wrong conclusion
* RegNet: Self-Regulated Network for Image Classification `arXiv` `Architecture`
* RoFormer: Enhanced Transformer with Rotary Position Embedding `arXiv` `Transformers`
* TransAttUnet: Multi-level attention- guided u-net with transformer for medical image segmentation `arXiv` `Transformers` `Medical` 
* Florence: A New Foundation Model for Computer Vision `arXiv`
* Stochastic Training is Not Necessary for Generalization `arXiv`
* TransClaw U-Net: Claw u-net with transformers for medical image segmentation. `arXiv` `Transformers`  `Medical` 
* TransUNet: Transformers make strong encoders for medical image segmentation `arXiv` `Transformers`  `Medical` 
* Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation `Transformers`  `Medical` 
* TransFuse: Fusing transformers and cnns for medical image segmentation `MICCAI` `Transformers`
* Convolution-free medical image segmentation using transformers `MICCAI` `Transformers`
* Medical Transformer: Gated Axial-Attention for Medical Image Segmentation `MICCAI` `Transformers`
* Multi-view Analysis of Unregistered Medical Images Using Cross-View Transformers `MICCAI` `Transformers`
* Token Pooling In Vision Transformers `arXiv` `Transformers` 
* MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer `arXiv` `Transformers`
* LocalViT: Bringing Locality to Vision Transformers `arXiv` `Transformers`
	> Encode location using DW Conv. Important for early layers
* Not All Knowledge Is Created Equal `arXiv` 
* Patches are all you need `OpenReview` 
	> Smaller patches boost performance
* Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity `arXiv` `DETR` 
* When Vision Transformers Outperform Resnets Without Pre-Training Or Strong Data Augmentations `arXiv` `Transformers` 

* Ranger21: a synergistic deep learning optimizer `arXiv` `Adam` `Optimizer`
	> Combine eight-optimizer extensions to Adam optimizers to deliver a stabler/better version (ranger2021).
* Do Vision Transformers See Like Convolutional Neural Networks? `arXiv` `Transformers` 
* CvT: Introducing convolutions to vision transformers `arXiv``Transformers` 
* Augmenting Convolutional networks with attention-based aggregation `Meta` `arXiv`
	> Seems to have stability issues 
* SLIP: Self-supervision meets Language-Image Pre-training `Meta` `arXiv`
* Object-aware Contrastive Learning for Debiased Scene Representation `NIPS` 
* All Tokens Matter: Token Labeling for Training Better Vision Transformers `NIPS` `ByteDance`
	> Use pre-trained models to distill knowledge and apply classification loss in all tokens and not just the class-token.

* Efficient Training of Visual Transformers with Small-Size Datasets `NIPS`
	> Training on hybrid architectures (Conv+Attentiion) on small dataset and randomly initialized networks. Add a self-supervised loss to speed convergence.

* Unsupervised object-level representation learning from scene images `NIPS` 
	> Use ssl on image-pretraining to retrieval KNN for images. Perform selective search to extract regions from every image. Match regions from one images to regions from a paired (KNN) images. Perform ssl on region-pretraining.

* Intriguing Properties of Vision Transformers `NIPS` `Nice` `Google`
	> ViTs are neither shape nor textured biased. They just less biased towards local textures. There is a shape-bias vs. robustness!

* DualNet: Continual Learning, Fast and Slow `NIPS` `Salesforce` `SG`
	> Propose a continual learning architecture based on Complementary Learning Systems (CLS) theory. CLS states that there are two learning systems: (1) fast in hippocampus, (2) slow in neocortex. the hippocampus’s memories are transferred to the neocortex over time to form a more general representation that supports long-term retention and generalization to new experiences. The proposed architecture has two components: fast and slow subnetworks. The slow subnetwork is trained with a SSL objective (Barlow Twins) using data from episodic memory. Contrary, the fast subnetwork is trained with a fully supervised objective (classification) on online data. The fast subnetwork adapts the features from the slow subnetwork to learn supervised tasks.

* XCiT: Cross-Covariance Image Transformers `NIPS` `Faceboook`
	> Replace Attention Kernel (Gram) with Co-variance matrix. Mixing channels instead of mixing tokens. "communication between patches is only implicit through the shared statistics".

* CoAtNet: Marrying Convolution and Attention for All Data Sizes `NIPS`
* Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers `NIPS` `RNNs`
* Twins: Revisiting the Design of Spatial Attention in Vision Transformers `NIPS` `ViT` `PvT` `MvT`
	> Use conditional position encoding (CPE) instead of PE + Subsample with Conv. Twins-PCPVT-L seems promising. 
* Overinterpretation reveals image classification model pathologies `Nice` `NIPS` 
* Does Knowledge Distillation Really Work? `NIPS`
* ReAct: Out-of-distribution Detection With Rectified Activations `NIPS` `WR` `JRC`
	> A simple idea to identify OOD samples using penultimate-layers activations in pre-trained models. post-hoc ood method that requires gray-box access for trained models. Seems to assume batchnorm layers.
* Is the Number of Trainable Parameters All That Actually Matters?  `Workshop` `NIPS` `NICE`
	> Only trainable dense-weights count; frozen/structured weights don't

* Focal attention for long-range interactions in vision transformers. `Nice` `MSR`
	> Dedicated arch for object detection. Pyramid-ViT with better performance but more compute (FLops).
* Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks `Nice` `NIPS` 
	> Replace feed-forward stages inside ResNet with a single recurrent stage.
* SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers
	> A segmentation decoder on top of PVT.
* Learning Debiased Representation via Disentangled Feature Augmentation `Nice` `NIPS` 
	> Train two networks: (1) learns bias features, (2) learns intrinsic features.
* Deep Learning on a Data Diet: Finding Important Examples Early in Training `NIPS` 
	> Grad and error ||y-\hat{y}|| norms can identify important examples. All experiments done on small datasets (CIFAR).
* Align before Fuse: Vision and Language Representation Learning with Momentum Distillation `SalesForce` `VLP`
* One Pass ImageNet `NIPS`  `Workshop`  
* Understanding Self-Supervised Learning Dynamics without Contrastive Pairs `ICML`
* Perceiver: General Perception with Iterative Attention `ICML`
* EfficientNetV2: Smaller Models and Faster Training `ICML`
	> progressively increase image-size + augmentation; replace DWConv at early layers with Fused-MBConv; add more layers to later stages (stage 5/6 in EfficientNet)
* Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision `ICML` `Google`
* Relative Positional Encoding for Transformers with Linear Complexity `ICML` `Transformers`  -- Tricky one
* Barlow Twins: Self-Supervised Learning via Redundancy Reduction `ICML` `Facebook` `US`
	> The proposed objective maximize mutual information between positive samples while minimizing cross-correlation between negative samples. The proposal still requires negative samples but not a large mini-batch. The paper operates on the cross-correlation matrix between augmented views inside a mini-batch. The paper is inspired by redundancy-reduction concept from neuroscience, where inputs with highly correlated dimensions (e.g., images) are encoding into independent dimensions.
* Self-training Improves Pre-training for Natural Language Understanding **#naacl2021**
* An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization -- `NYU` **Medical image analysis**
* Robust breast cancer detection in mammography and digital breast tomosynthesis using an annotation-efficient -- `DeepHealth` `Nature Medicine`
* An improved mammography malignancy model with self-supervised learning -- `WR-AI` `SPIE Medical Imaging`
* MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation -- `WR-AI` `MLHC`
* CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer -- `KTH` `openreview`
* Evaluating Subgroup Disparity Using Epistemic Uncertainty In Mammography `ICML` `workshop`
* A Data Set and Deep Learning Algorithm for the Detection of Masses and Architectural Distortions in Digital Breast Tomosynthesis **#nlm2021** 
* Are Convolutional Neural Networks or Transformers more like human vision? `CogSci`
* TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning `Computer Methods and Programs in Biomedicine`
* Evaluation of deep learning-based artificial intelligence techniques for breast cancer detection on mammograms: Results from a retrospective study using a BreastScreen Victoria dataset `Medical`
* Toward robust mammography-based models for breast cancer risk `Medical` `Sci Transl Med` `Nice`
* Stand-Alone Use of Artificial Intelligence for Digital Mammography and Digital Breast Tomosynthesis Screening: A Retrospective Evaluation `RSNA` `Medical`
* PonderNet: Learning to Ponder `ICMLW`
* Learning Transferable Visual Models From Natural Language Supervision `OPENAI`
* LambdaNetworks: Modeling long-range Interactions without Attention `openreview2` -- Seems like a good idea, poorly written.
* Is space-time attention all you need for video understanding? `ICML`
	> Train a divided temporal-spatial attention on top of 2d frame representations.
* Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples `IsoMax` `IEEE Neural Networks and Learning Systems`
* Public patient views of artificial intelligence in healthcare: A nominal group technique study `Medical` `Healthcare` `WR`
* Unifying vision-and-language tasks via text generation `VLP` `PMLR`
* Deep Transfer Learning for Multi-Source Entity Linkage via Domain Adaptation `AMZN` `EL` `VLDB`
* Image Composition Assessment with Saliency-augmented Multi-pattern Pooling `BMVC`
	> Evaluate image aesthetic using composition patterns

* [Receptance Weighted Key-Value] https://github.com/BlinkDL/RWKV-LM `Git`
	> followup on `Apple` Attention Free Transformer (AFT) 

* Medical Image Dataset Rebalancing via Conditional Deep Convolutional Generative Adversarial Networks (cDCGANs) `WR` `Stanford` `Intern`
	> Applications of conditional GANs in medical images.

* Trainable summarization to improve breast tomosynthesis classification. `MICCAI` `WR`
	> Temporal attention across slices before feeding into a classifier.

* GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models `OpenAI`
	> Text-conditional diffusion model to OpenAI that beats GANs (DALI-E).

* Retrieval Augmentation Reduces Hallucination in Conversation `ACL` `Facebook`
	> use retrieval-augmentated generation (RAG) to reduce hallucination. Using query, find related document. Feed the document along with the query to a language model.

* Quantifying and leveraging predictive uncertainty for medical image assessment `MIA` `Siemens` `Harvard`
	> Quantify uncertainty without sampling (dropout/ensemble) using Evidential DL. The estimated uncertainty is used to identify challenging samples in the test split and noisy samples in the training split.

* Second opinion needed: communicating uncertainty in medical machine learning `Nature` `NPJ Digital Medicine`
	> Review for different uncertainty estimation approaches. The paper motivates the importance of decision abstention

* AI-based Strategies to Reduce Workload in Breast Cancer Screening with Mammography and Tomosynthesis `RSNA` `Radiology` `ES`
	> A rule-in/out evaluation for Transpara from ScreenPoint Medical on a single site in Spain. A small study population with only 113 cancers.
	
* W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training `US` `MIT` `Google` `Automatic Speech Recognition and Understanding` `Workshop`
	> Combining Contrastive learning and masking for audio representation learning. Quantization seems important along with conformer block.
	
* The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, and Stress `DE` `UK`
	> Emotion prediction and classification using audio, video, and text. Two datasets are annotated (MUSE-Car and Ulm-TSST. Investigate multiple features embedding per modality.
	
* VREED: Virtual Reality Emotion Recognition Dataset Using Eye Tracking and Physiological Measures `GB` `UK` `ACM` `Interactive Mobile`
	> A new affection dataset. The dataset collects psychological (questionnaire), behavioral (eye-tracking), physiological data (heart rate, and skin conductance). The paper used classicial ML SVM classifier in a fully supervised manner. Fusion all features achieved the best performance.
	
* U-Sleep: resilient high-frequency sleep staging `DK` `Copenhagen`
	> The paper propose a U-Net architecture that segment sleep stages (awake, light sleep, intermediate sleep, deep sleep, and REM). The proposed network process polysomnography (PSG) data in the form of electroencephalography (EEG) and electrooculography (EOG) signal. Fully supervised approach, trained on a relatively large dataset, with good generalization performance. 

* Weighted boxes fusion: Ensembling boxes from different object detection models `Image and Vision Computing` `RU` `SG`
	> The paper propose a NMS-variant for combining bbox from an ensemble detector model.
	
### CVPR 2021
* SLADE: A Self-Training Framework For Distance Metric Learning
	> LSD	Self-supervised pretraining beats ImageNet + Sample unlabeled data using similarity distribution loss. lsd :hash:industry :hash:amazon
* Efficient Object Embedding for Spliced Image Retrieval
	> LSD	:hash:industry :hash:facebook
* knowledge Evolution Neural Networks
	> Mine :) "Classification, Small datasets"
* Unsupervised Discovery of the Long-Tail in Instance Segmentation Using Hierarchical Self-Supervision
	> I should try this hyperbolic embeddings
* Embedding Transfer with Label Relaxation for Improved Metric Learning
	> Now, we have to train two models for metric learning
* Dynamic Metric Learning: Towards a Scalable Metric Space to Accommodate Multiple Semantic Scales
	> Multiple margin hyperparameters
* Deep Compositional Metric Learning -- `Technicalities`
* Asymmetric metric learning for knowledge transfer
* Nearest Neighbor Matching for Deep Clustering -- `Technicalities`
* Understanding the Behaviour of Contrastive Loss
	> Nice paper; the uniformity-tolerance dilemma
* Unsupervised Hyperbolic Metric Learning
* StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval
	> To many loss terms!
* Learning Cross-Modal Retrieval with Noisy Labels
	> Focal-loss inverse for noisy labels
* Relative Order Analysis and Optimization for Unsupervised Deep Metric Learning
	> Interesting till Sec 3.3 & 3.4!

* Jigsaw Clustering for Unsupervised Visual Representation Learning
	> Seems like clustering is doing all the heavy work.
* Playable Video Generation -- `PI_arXiv_Ch` 
	> Too many loss terms
* Representation Learning via Global Temporal Alignment and Cycle-Consistency -- `PI_arXiv_Ch` 
	> Differentiable time-warping for videos
* Read and Attend: Temporal Localisation in Sign Language Videos
	> assume access to a sparse collection of annotation. Use Transformer attention to localize actions in videos!
* Unsupervised Visual Representation Learning by Tracking Patches in Video
* Spatially Consistent Representation Learning
	> Interesting idea!! 
* Improving Unsupervised Image Clustering With Robust Learning
	> To many networks (g \phi, h \psi, f1 \theta, f2 \theta)
* Dense contrastive learning for self-supervised visual pre-training. `Nice` `ByteDance` `WR` `JRC`
	> DenseCL, find correspondence (anchor-positive) between two views using argmax. Not sure this thing will work for medical images.
	
* Humble Teachers Teach Better Students for Semi-Supervised Object Detection
	> Semi-supervised learning approach :hash:industry :hash:amazon
* BoxInst: High-Performance Instance Segmentation with Box Annotations
	> FCOS Team
* You Only Look One-level Feature -- `WR-AI`
* Multiresolution Knowledge Distillation for Anomaly Detection
* VarifocalNet: An IoU-aware Dense Object Detector -- `WR-AI`
* Leveraging Large-Scale Weakly Labeled Data for Semi-Supervised Mass Detection in Mammograms 
	> Object Detection + Weakly supervised learning -- `WR-AI`
* Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection
* Dynamic Head: Unifying Object Detection Heads with Attentions

* RepVGG: Making VGG-style ConvNets Great Again -- `PI_arXiv_Ch`

* Learning Student Networks in the Wild
	> Interesting but Q is concerning


* Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation
	> Reduce Pseudo label resolution to mitigate noise.

* Open World Compositional Zero-Shot Learning
	> zero-shot formulated as metric learning problem
	
* DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort

* IIRC: Incremental Implicitly-Refined Classification
* DER: Dynamically Expandable Representation for Class Incremental Learning 

* The Affective Growth of Computer Vision
	> Nice reading but worried about the selection bais in informants.
* InverseForm: A Loss Function for Structured Boundary-Aware Segmentation
	> Segmentation
* On Feature Normalization and Data Augmentation
	> data augmentation techniques
* Primitive Representation Learning for Scene Text Recognition
	> Text Recognition -- Technicalities
* Cross-Iteration Batch Normalization 
* Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning `Medical` `MIL`
* Positive-Congruent Training: Towards Regression-Free Model Updates
	> Important problem but a limited solution.
* General Multi-label Image Classification with Transformers `Classification` `Transformer`
* Differentiable Patch Selection for Image Recognition `High` `Resolution` `Nice`
* Rethinking Channel Dimensions for Efficient Model Design `RexNet`
* Learning To Count Everything
	> Dataset paper -- Nice problem formulation.
* Simple copy-paste is a strong data augmentation method for instance segmentation. `Google`
	> Augmentation for instance segmentation
	
### ICCV 2021

* Is it Time to Replace CNNs with Transformers for Medical Images? `Workshop`
* DINO: Emerging properties in self-supervised vision transformers. `Facebook` 
	> DINO is a contrastive learning approach but with a distillation loss, instead of contrastive loss. Instead of two siamese network, we have a teacher-student network, where the teacher is a moving average (ensemble) of the student network. To avoid model collapse, the paper utilize centering and softmax. To learn a better representation for KNN, the paper emphasizes the importance of multi-crop augmentation. The paper observe key properties that happens with ViT _only_ with self-supervised learning
	1. Self-supervised ViT features explicitly contain the scene layout and, in particular, object boundaries
	2. Self-supervised ViT features perform particularly well with a basic nearest neighbors classifier (k-NN)
	
* End-to-End Semi-Supervised Object Detection with Soft Teacher. `Microsoft` 
* Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet
	> propose overlapping token -- poorly written
* Pyramid vision transformer: A versatile backbone for dense prediction without convolutions -- `Technicalities` `ViT` `Det`
* Big Self-Supervised Models Advance Medical Image Classifications `Medical` `Google` `Nice`
	> Use ImageNet as initialization before self-supervised learning to speed up convergence. Use positive crops for multi-view images. partial (light-weight) augmentationis enough if positive pairs are sampled efficiently (Tab. B.2). SSL help with model robustness (Fig. 4).
* Divide and Contrast: Self-supervised Learning from Uncurated Data `DeepMind` `SSL`
* With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations `SSL` `DeepMind` `Google`
* Dynamic detr: End-to-end object detection with dynamic attention `Microsoft` `Technicalities`
	> Same team and ideas from Dynamic Head (CVPR 2021)
* Visual Transformers: Where Do Transformers Really Belong in Vision Models? `Facebook`	> Tricky	
* Going deeper with Image Transformers `Facebook` `Deep` `Transformers`
* Rethinking imagenet pre-training `FAIR` `Nice` `Detection`
* LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference `Transformers` `Facebook`
* Co-Scale Conv-Attentional Image Transformers `Transformers` `detection` `Oral`
* Detail Me More: Improving GAN’s photo-realism of complex scenes `GANs` `AMZN`
* When do GANs replicate? On the choice of dataset size `GANs` `AMZN`
* Multiscale Vision Transformers `FAIR` `Video`
* CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification `MvT` `IBM`
	> Multiscale vision transformer evaluated using classification task only!
* Pretrained ViTs Yield Versatile Representations For Medical Images `Workshop` `Medical` `ViTs`
	> ViTs attention is remarkable
* Instances as Queries `Segmentation` `Detection` `Nice`
	> Use the same query objects (as in DETR) inside DynamicConv to generate object masks. one query -> one object -> mask correspondence. Well Written
* Effectively leveraging attributes for visual similarity.
	> conditional metric learning done using joint image-pair feature embedding.
* Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models
	> Composed image retrieval using pre-trained VLP models.
* An Empirical Study of Training Self-Supervised Vision Transformers `Meta` `Oral`
	> Moco v3 drops key queue, bz=4096 seems enough! Besides backbone+projection, a prediction MLP is added. AdamW seems more stable compared to LAMB. Random patch projection is enough to learn a good representation even without positional embedding. 
* Region Similarity Representation Learning `UC-Ber` `WR` `JRC`
	> Contrastive learning on region-level for localization tasks. This paper relates to `2202.04639`
* ViViT: A Video Vision Transformer `Google`
	> Train Video Video on tubelets. Factorize attention operation to reduce the model's computational cost. E.g., spatial then temporal attention or splits heads so that half perform spatial while the other half perform temporal attention.

* Contrast and Order Representations for Video Self-supervised Learning
	> On the importance of modeling temporal order for videos. 

* Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification `USA` `Academia`
	> AUC loss formulated as a ranking loss: The positive points should rank higher than negative points by a margin m. The high level idea is interesting, but the technical details is a hurdle. The paper formulates the problem as a min-max optimization. Thus, a special optimizer (e.g., PESG) is required. Furthermore, the proposal requires a two-stage training process: (1) using cross-entropy, (2) then using AUC loss!


### WACV 2021
* ResNet or DenseNet? Introducing Dense Shortcuts to ResNet -- `Technicalities`
* Conflicting Bundles: Adapting Architectures Towards the Improved Training of Deep Neural Networks
* Multi-path Neural Networks for On-device Multi-domain Visual Classification
* Dynamic Routing Networks
	> Nice but the wording is a bit confusing. Code not available!
* MPRNet: Multi-Path Residual Network for Lightweight Image Super Resolution -- Technicalities	

### MICCAI 2021

* CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation
	> Transformers + Medical Images


### ICLR 2021
* Concept Learners For Few-Shot Learning 
* SEED: Self-supervised Distillation For Visual Representation
* Unbiased Teacher for Semi-Supervised Object Detection
* Rethinking Attention With Performers `Google`
* Sharpness-Aware Minimization For Efficiently Improving Generalization -- `WR-AI` `Google`
* Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning 
	> Did it before it was cool

* What Should Not Be Contrastive In Contrastive Learning
	> Brute Force	
* Grokking: Generalization beyond Overfitting on small algorithmic datasets `Workshop` 
	> Grokking can be observed on toy datasets and transformer models. Weight decay help speedup generalization (Grokking).
* Deformable DETR: Deformable Transformers for End-to-End Object Detection `Oral`
	> Replace vanilla self-attention in DETR with deformable conv to speed up convergence and reduce computational complexity.
* NFNets: High-Performance Large-Scale Image Recognition Without Normalization `DeepMind`
* An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale `PI_Reading_Grp` `Google`
* On the geometry of generalization and memorization in deep neural networks `Nice` `Intel`
	> explain double-descent, early layers generalize while late layers memorize, lack of gradient magnitude differences between early and late layers

* Does enhanced shape bias improve neural network robustness to common corruptions? `Nice`
	> Nice analysis; style augmentation improves robustness corruptions, while enhancing shape bias makes networks more vulnerable
	
* The Intrinsic Dimension of Images and Its Impact on Learning `UMD` `US` `Nice`
	> Natural images has low intrinsic dimensionality (ID). This low dimensionality vital for neural networks learning (faster convergence, better generalization). The extrinsic dimensionality is less relevant for learning/generalization. The paper used an MLE for ID estimation. The key idea is to monitor/measure the scale in number of nearest neighbors as the radius increases. The paper also leveraged GANs to empirically validate the MLE method for ID estimation.
	
* The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers `Google` `Harvard`
	> The traditional generalization framework is flawed, i.e., Test-err = Train-err + (Test-err - Train-err). It provides no useful insights if Train-err~=0, i.e., Test-err = Test-err!. This paper propose a new framework for generalization: Test-err = Test-err-online + (Test-err - Test-err-online). In different words, the generalization err = online learning error + Bootstrap error where Bootstrap error denotes the gap between online learning and offline learning. The paper shows that Bootstrap error is generally small for a large range of problems. This makes online learning error an interesting metric to inspect and evaluate. The paper shows that superior architectures in online learning are also superior in offline learning.