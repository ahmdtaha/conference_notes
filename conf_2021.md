### ICCV 2021

* Is it Time to Replace CNNs with Transformers for Medical Images? `Workshop`
* DINO: Emerging properties in self-supervised vision transformers. `Facebook` 
	> Revisited
* End-to-End Semi-Supervised Object Detection with Soft Teacher. `Microsoft` 
* Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet
	> propose overlapping token -- poorly written
* Pyramid vision transformer: A versatile backbone for dense prediction without convolutions -- `Technicalities` `ViT` `Det`
* Big Self-Supervised Models Advance Medical Image Classifications `Medical` `Google`
* Divide and Contrast: Self-supervised Learning from Uncurated Data `DeepMind` `SSL`
* With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations `SSL` `DeepMind` `Google`
* Dynamic detr: End-to-end object detection with dynamic attention `Microsoft` `Technicalities`
	> Same team and ideas from Dynamic Head (CVPR 2021)
* Visual Transformers: Where Do Transformers Really Belong in Vision Models? `Facebook`	> Tricky	
* Going deeper with Image Transformers `Facebook` `Deep` `Transformers`
* Rethinking imagenet pre-training `FAIR` `Nice` `Detection`
* LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference `Transformers` `Facebook`
* Co-Scale Conv-Attentional Image Transformers `Transformers` `detection`
* Detail Me More: Improving GANâ€™s photo-realism of complex scenes `GANs` `AMZN`
* When do GANs replicate? On the choice of dataset size `GANs` `AMZN`
* Multiscale Vision Transformers `FAIR` `Video`
* CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification `MvT` `IBM`
	> Multiscale vision transformer evaluated using classification task only!
* Pretrained ViTs Yield Versatile Representations For Medical Images `Workshop` `Medical` `ViTs`
	> ViTs attention is remarkable

### WACV 2021
* ResNet or DenseNet? Introducing Dense Shortcuts to ResNet -- `Technicalities`

### MICCAI 2021

* CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation
	> Transformers + Medical Images


### ICLR 2021
* Concept Learners For Few-Shot Learning 
* SEED: Self-supervised Distillation For Visual Representation
* Unbiased Teacher for Semi-Supervised Object Detection
* Rethinking Attention With Performers `Google`
* Sharpness-Aware Minimization For Efficiently Improving Generalization -- `WR-AI` `Google`
* Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning 
	> Did it before it was cool
* What Should Not Be Contrastive In Contrastive Learning
	> Brute Force	
* Grokking: Generalization beyond Overfitting on small algorithmic datasets `Workshop` 
* Deformable DETR: Deformable Transformers for End-to-End Object Detection `Detection`
* NFNets: High-Performance Large-Scale Image Recognition Without Normalization `DeepMind`
* An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale `PI_Reading_Grp` `Google`
* On the geometry of generalization and memorization in deep neural networks `Nice` `Intel`
	> explain double-descent, early layers generalize while late layers memorize, lack of gradient magnitude differences between early and late layers


	
### Generic 2021
* Why AI is Harder Than We Think `arXiv`
* How Much Can CLIP Benefit Vision-and-Language Tasks? `arXiv` `CLIP`
* Evaluating Large Language Models Trained on Code `arXiv` `OpenAI`
* Vision Transformers with Patch Diversification `arXiv` `Transformers` `Nice`
	> Apply patch-level regularization.
* MetaFormer is Actually What You Need for Vision `arXiv` `Transformers`
	> Probably wrong conclusion
* RegNet: Self-Regulated Network for Image Classification `arXiv` `Architecture`
* RoFormer: Enhanced Transformer with Rotary Position Embedding `arXiv` `Transformers`
* TransAttUnet: Multi-level attention- guided u-net with transformer for medical image segmentation `arXiv` `Transformers` `Medical` 
* Florence: A New Foundation Model for Computer Vision `arXiv`
* Stochastic Training is Not Necessary for Generalization `arXiv`
* TransClaw U-Net: Claw u-net with transformers for medical image segmentation. `arXiv` `Transformers`  `Medical` 
* TransUNet: Transformers make strong encoders for medical image segmentation `arXiv` `Transformers`  `Medical` 
* Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation `Transformers`  `Medical` 
* TransFuse: Fusing transformers and cnns for medical image segmentation `MICCAI` `Transformers`
* Convolution-free medical image segmentation using transformers `MICCAI` `Transformers`
* Medical Transformer: Gated Axial-Attention for Medical Image Segmentation `MICCAI` `Transformers`
* Multi-view Analysis of Unregistered Medical Images Using Cross-View Transformers `MICCAI` `Transformers`
* Token Pooling In Vision Transformers `arXiv` `Transformers` 
* MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer `arXiv` `Transformers`
* LocalViT: Bringing Locality to Vision Transformers `arXiv` `Transformers`
	> Encode location using DW Conv. Important for early layers
* Not All Knowledge Is Created Equal `arXiv` 
* Patches are all you need `OpenReview` 
	> Smaller patches boost performance
* Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity `arXiv` `DETR` 
* When Vision Transformers Outperform Resnets Without Pre-Training Or Strong Data Augmentations `arXiv` `Transformers` 
* Do Vision Transformers See Like Convolutional Neural Networks? `arXiv` `Transformers` 
* CvT: Introducing convolutions to vision transformers `arXiv``Transformers` 
* Augmenting Convolutional networks with attention-based aggregation `Meta` `arXiv`
	> Seems to have stability issues 
* SLIP: Self-supervision meets Language-Image Pre-training `Meta` `arXiv`
* Object-aware Contrastive Learning for Debiased Scene Representation `NIPS` 
* CoAtNet: Marrying Convolution and Attention for All Data Sizes `NIPS`
* Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers `NIPS` `RNNs`
* Twins: Revisiting the Design of Spatial Attention in Vision Transformers `NIPS` `ViT` `PvT` `MvT`
	> Use conditional position encoding (CPE) instead of PE + Subsample with Conv. Twins-PCPVT-L seems promising. 
* Overinterpretation reveals image classification model pathologies `Nice` `NIPS` 
* Does Knowledge Distillation Really Work? `NIPS`
* Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks `Nice` `NIPS` 
* Learning Debiased Representation via Disentangled Feature Augmentation `Nice` `NIPS`  
* Align before Fuse: Vision and Language Representation Learning with Momentum Distillation `SalesForce` `VLP`
* One Pass ImageNet `NIPS`  `Workshop`  
* Understanding Self-Supervised Learning Dynamics without Contrastive Pairs `ICML`
* Perceiver: General Perception with Iterative Attention `ICML`
* Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision `ICML` `Google`
* Relative Positional Encoding for Transformers with Linear Complexity `ICML` `Transformers`  -- Tricky one
* Barlow Twins: Self-Supervised Learning via Redundancy Reduction `ICML`
* Self-training Improves Pre-training for Natural Language Understanding **#naacl2021**
* An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization -- `NYU` **Medical image analysis**
* Robust breast cancer detection in mammography and digital breast tomosynthesis using an annotation-efficient -- `DeepHealth` **Nature Medicine**
* An improved mammography malignancy model with self-supervised learning -- `WR-AI` **SPIE Medical Imaging**
* MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation -- `WR-AI` **MLHC2021**
* CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer -- `KTH` **openreview2021**
* Evaluating Subgroup Disparity Using Epistemic Uncertainty In Mammography **#icml2021** `workshop`
* A Data Set and Deep Learning Algorithm for the Detection of Masses and Architectural Distortions in Digital Breast Tomosynthesis **#nlm2021** 
* Are Convolutional Neural Networks or Transformers more like human vision? **#CogSci2021**
* TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning **@ Computer Methods and Programs in Biomedicine** 
* Evaluation of deep learning-based artificial intelligence techniques for breast cancer detection on mammograms: Results from a retrospective study using a BreastScreen Victoria dataset `Medical`
* Toward robust mammography-based models for breast cancer risk `Medical` `Sci Transl Med` `Nice`
* Stand-Alone Use of Artificial Intelligence for Digital Mammography and Digital Breast Tomosynthesis Screening: A Retrospective Evaluation `RSNA` `Medical`
* PonderNet: Learning to Ponder `ICMLW`
* Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples `IsoMax` `IEEE Neural Networks and Learning Systems`
* Public patient views of artificial intelligence in healthcare: A nominal group technique study `Medical` `Healthcare` `WR`
* Unifying vision-and-language tasks via text generation `VLP` `PMLR`
* Deep Transfer Learning for Multi-Source Entity Linkage via Domain Adaptation `AMZN` `EL` `VLDB`
* Image Composition Assessment with Saliency-augmented Multi-pattern Pooling `BMVC`
	> Evaluate image aesthetic using composition patterns
* [Receptance Weighted Key-Value] https://github.com/BlinkDL/RWKV-LM `Git`
	> followup on `Apple` Attention Free Transformer (AFT) 