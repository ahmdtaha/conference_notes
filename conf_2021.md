### ICCV 2021

* Is it Time to Replace CNNs with Transformers for Medical Images? `Workshop`
* DINO: Emerging properties in self-supervised vision transformers. `Facebook` 
* End-to-End Semi-Supervised Object Detection with Soft Teacher. `Microsoft` 
* Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet 
* Pyramid vision transformer: A versatile backbone for dense prediction without convolutions -- `Technicalities` 
* Big Self-Supervised Models Advance Medical Image Classifications `Medical` `Google`
* Divide and Contrast: Self-supervised Learning from Uncurated Data `DeepMind` `SSL`
* With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations `SSL` `DeepMind` `Google`
* Dynamic detr: End-to-end object detection with dynamic attention `Microsoft` `Technicalities`
	> Same team and ideas from Dynamic Head (CVPR 2021)
* Visual Transformers: Where Do Transformers Really Belong in Vision Models? `Facebook`	> Tricky	

### WACV 2021
* ResNet or DenseNet? Introducing Dense Shortcuts to ResNet -- `Technicalities`

### MICCAI 2021

* CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation
	> Transformers + Medical Images


### ICLR 2021
* Concept Learners For Few-Shot Learning **#iclr2021**
* SEED: Self-supervised Distillation For Visual Representation
* Unbiased Teacher for Semi-Supervised Object Detection **#iclr2021**
* Rethinking Attention With Performers **#iclr2021** `Google`
* Sharpness-Aware Minimization For Efficiently Improving Generalization **#iclr2021** -- `WR-AI` `Google`
* Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning **#iclr2021**
	> Did it before it was cool
* What Should Not Be Contrastive In Contrastive Learning
	> Brute Force	
* Grokking: Generalization beyond Overfitting on small algorithmic datasets `Workshop` 

	
### Generic 2021
* Why AI is Harder Than We Think `arXiv`
* MetaFormer is Actually What You Need for Vision `arXiv` `Transformers`
	> Probably wrong conclusion
* RegNet: Self-Regulated Network for Image Classification `arXiv` `Architecture`
* RoFormer: Enhanced Transformer with Rotary Position Embedding `arXiv` `Transformers`
* TransAttUnet: Multi-level attention- guided u-net with transformer for medical image segmentation `arXiv` `Transformers` `Medical` 
* Florence: A New Foundation Model for Computer Vision `arXiv`
* TransClaw U-Net: Claw u-net with transformers for medical image segmentation. `arXiv` `Transformers`  `Medical` 
* TransUNet: Transformers make strong encoders for medical image segmentation `arXiv` `Transformers`  `Medical` 
* Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation `Transformers`  `Medical` 
* TransFuse: Fusing transformers and cnns for medical image segmentation `MICCAI` `Transformers`
* Convolution-free medical image segmentation using transformers `MICCAI` `Transformers`
* Medical Transformer: Gated Axial-Attention for Medical Image Segmentation `MICCAI` `Transformers`
* Multi-view Analysis of Unregistered Medical Images Using Cross-View Transformers `MICCAI` `Transformers`
* Token Pooling In Vision Transformers `arXiv` `Transformers` 
* MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer `arXiv` `Transformers`
* Not All Knowledge Is Created Equal `arXiv` 
* Patches are all you need `OpenReview` 
	> Smaller patches boost performance
* Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity `arXiv` `DETR` 
* When Vision Transformers Outperform Resnets Without Pre-Training Or Strong Data Augmentations `arXiv` `Transformers` 
* Do Vision Transformers See Like Convolutional Neural Networks? **#arXiv2021** `Transformers` 
* CvT: Introducing convolutions to vision transformers **#arXiv2021** `Transformers` 
* Object-aware Contrastive Learning for Debiased Scene Representation **#nips2021**
* Overinterpretation reveals image classification model pathologies `Nice` `NIPS`  
* One Pass ImageNet `NIPS`  `Workshop`  
* Understanding Self-Supervised Learning Dynamics without Contrastive Pairs `ICML`
* Relative Positional Encoding for Transformers with Linear Complexity `ICML` `Transformers`  -- Tricky one
* Barlow Twins: Self-Supervised Learning via Redundancy Reduction `ICML`
* Self-training Improves Pre-training for Natural Language Understanding **#naacl2021**
* An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization -- `NYU` **Medical image analysis**
* Robust breast cancer detection in mammography and digital breast tomosynthesis using an annotation-efficient -- `DeepHealth` **Nature Medicine**
* An improved mammography malignancy model with self-supervised learning -- `WR-AI` **SPIE Medical Imaging**
* MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation -- `WR-AI` **MLHC2021**
* CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer -- `KTH` **openreview2021**
* Evaluating Subgroup Disparity Using Epistemic Uncertainty In Mammography **#icml2021** `workshop`
* A Data Set and Deep Learning Algorithm for the Detection of Masses and Architectural Distortions in Digital Breast Tomosynthesis **#nlm2021** 
* Are Convolutional Neural Networks or Transformers more like human vision? **#CogSci2021**
* TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning **@ Computer Methods and Programs in Biomedicine** 
* Evaluation of deep learning-based artificial intelligence techniques for breast cancer detection on mammograms: Results from a retrospective study using a BreastScreen Victoria dataset `Medical`
* Toward robust mammography-based models for breast cancer risk `Medical` `Sci Transl Med` `Nice`