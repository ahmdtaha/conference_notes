# Interesting ECCV2020 papers
> and my superficial opinion -- just one scan


### Metric Learning
* Hard negative examples are hard, but useful
	> Simple idea -- Selective Contrastive Triplet loss 
* Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval 
	> Nice paper
* A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses	
* You Are Here: Geolocation by Embedding Maps and Images
	> Autonomous car
* A Metric Learning Reality Check -- `PI_Reading_Grp`
* The Group Loss for Deep Metric Learning 
* Improving Face Recognition by Clustering Unlabeled Faces in the Wild


### Continuous Learning
* REMIND Your Neural Network to Prevent Catastrophic Forgetting 
	> Nice paper, with neuroscience literature citations

### Few/Zero-shot learning
* Rethinking Few-shot Image Classification: A Good Embedding is All You Need
* Prototype Rectification for Few-Shot Learning -- `PI_Reading_Grp`
* Associative Alignment for Few-shot Image Classification

### Representation Disentanglement
* Interpretable Neural Network Decoupling
	> Has some good ideas
* The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement
	> Nice and Simple
* Adversarial Continual Learning
	> Shared and specific features


### Energy-Based Models
* Energy-Based Models for Deep Probabilistic Regression
	> Interesting paper with nice illustration
	
### Self/Un-supervised representation learning
* Memory-augmented Dense Predictive Coding for Video Representation Learning 
	> Technicalities
* Contrastive Learning for Weakly Supervised Phrase Grounding 
	> Assume BERT
* Learning Visual Representations with Caption Annotations 
	> Assume BERT
	
### Pruning
* Dynamic Group Convolution for Accelerating Convolutional Neural Networks
	> -- `PI_Reading_Grp` soft vs hard attention ?
* Channel selection using Gumbel Softmax 

### GANs and Image Generation
* Contrastive Learning for Unpaired Image-to-Image Translation -- `PI_Reading_Grp`
* Unsupervised Sketch to Photo Synthesis
	> Technicalities
### Knowledge-distillation
* Knowledge Distillation Meets Self-Supervision 
* Feature Normalized Knowledge Distillation for Image Classification
	> Confusing samples (near the boundary) has small norm. In this case, the teacher output dominates.

### NAS
* Are Labels Necessary for Neural Architecture Search?

### Misc
* Grounded Situation Recognition
* Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets -- `Image Captioning`
* Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification -- `Imbalance datasets`
* Big Transfer (BiT): General Visual Representation Learning `Google`
	> Tips for pre-training and fine-tuning
* Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks -- `PI_Reading_Grp` !!
* Conditional convolutions for instance segmentation -- `Object Detection`
* End-to-End Object Detection with Transformers -- `Object Detection` `FB AI`
* Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation `Transformers` `Google`
* Gradient Centralization: A New Optimization Technique for Deep Neural Networks `Optimizer` `Adam` `Alibaba`
	> Normalize gradient the way BatchNorm normalize features. This promote stability and constrain the weights within a hyperplane which reduces overfitting.