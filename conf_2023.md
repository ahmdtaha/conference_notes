# arXiv
* Toolformer: Language Models Can Teach Themselves to Use Tools `Meta`
* ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders `Meta` `MAE`
* Language Is Not All You Need: Aligning Perception with Language Models `MultiModal` `LLM` `Microsoft`
* ViperGPT: Visual Inference via Python Execution for Reasoning `Nice`
	> High level **generic** reasoning system
* The effectiveness of MAE pre-pretraining for billion-scale pretraining `Meta`
* ActiveLab: Active Learning with Re-Labeling by Multiple Annotators
* Cut and Learn for Unsupervised Object Detection and Instance Segmentation `Meta` `Det` `Nice` `NCut`
	> Using normalized cuts to generate multiple masks, Conditional Random Field (CRF) to generate bboxes
* Segment Anything `Meta` `Seg`

# Generic
* Dataset Distillation for Medical Dataset Sharing `AAAI` `Workshop` `DD`
* Conditional Positional Encodings for Vision Transformers `ICLR` `ViT`
	> relative positional embedding with latent absolute positional information (zero-padding)
* On the duality between contrastive and non-contrastive self-supervised learning `ICLR`
	> sample contrastive vs. dimension contrastive. Revise assumptions about batch-size and embedding dimension
	
# CVPR
* MVImgNet: A Large-scale Dataset of Multi-view Images `Dataset`