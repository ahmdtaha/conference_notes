# arXiv
* Toolformer: Language Models Can Teach Themselves to Use Tools `Meta`
* ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders `Meta` `MAE`
* ViperGPT: Visual Inference via Python Execution for Reasoning `Nice`
	> High level **generic** reasoning system
* The effectiveness of MAE pre-pretraining for billion-scale pre-training `Meta`
* ActiveLab: Active Learning with Re-Labeling by Multiple Annotators
* Explainable Image Quality Assessment for Medical Imaging `arXiv`
	> Perform various GardCAM visualization techniques on top of IQA classifiers
	
* Cut and Learn for Unsupervised Object Detection and Instance Segmentation `Meta` `Det` `Nice` `NCut`
	> Using normalized cuts to generate multiple masks, Conditional Random Field (CRF) to generate bboxes
* Segment Anything `Meta` `Seg`
	> Motivation: Develop a foundational model for interactive segmentation tasks. Develop a large dataset/data collection pipeline needed for training the foundational model. Design flexible segmentation prompts to support points, bounding boxes, and masks as user input.
	
	> Contribution#1: The paper proposes SAM, segment anything model, which takes both an image and a segmentation prompt, then generates an output mask. SAM contains: (1) a global image encoder that encodes a high-resolution image into a 64x64 spatial feature map; (2) a mask encoder, a CNN that encodes a binary mask -- from the user -- into the same 64x64 image resolution; (3) a prompt encoder that encodes points and bounding boxes provided as a user input. Input points are encoded using positional encodings, then fed as token(s) to the mask decoder; (4) a lightweight mask decoder which is responsible for generating output mask given the image feature (64x64 feature map) and segmentation prompt tokens (e.g., points and bounding boxes). The mask decoder is ambiguity-aware, meaning the decoder generates multiple output masks (whole, part, and subpart) given an ambiguous segmentation prompt (e.g., a single foreground point). Similar to detectors with a centerness head, the mask decoder has an IOU token head serving as a confidence score.
	
	> Contribution#2: The paper publishes a 1.1B segmentation masks for 11M image. This dataset, SA-1B, provides high-resolution, licensed, privacy-protecting images (blurred faces and license plates), and high-quality segmentation masks.
	
	> Contribution#3: The paper describes its data collection pipeline. This is a three-stage pipeline that goes from an initial model-assisted manual annotation stage to a fully automated stage. This pipeline is a major feat, tackling multiple engineering challenges.
	
* SAM Struggles in Concealed Scenes -- Empirical Study on "Segment Anything"

* Choose Your Weapon: Survival Strategies for Depressed AI Academics
	> Should read again
* Symbolic Discovery of Optimization Algorithms `Google`
	> LION optimizer using evolutionary algorithm (mutation, insertion, deletion).
* What does CLIP know about a red circle? Visual prompt engineering for VLMs
	> Rare events can be learned with large models when trained on large datasets!
*  A Watermark for Large Language Models `LLM` `UMD` `Nice`
*  Dropout Reduces Under-fitting `Meta` `Nice`
	> Nice paper with nice analysis. Use Early dropout for small (under-fitting) models and late dropout for large (overfitting) models.
* RWKV: Reinventing RNNs for the Transformer Era `LLM`
	> W == relative positional embedding (bias). WKV == Weighted Key-Value, R = Receptence gate (forget gate). 
* A PhD Student’s Perspective on Research in NLP in the Era of Very Large Language Models `NLP` `LLMs`
	> Future research directions in the field of NLP
* Model Dementia: Generated Data Makes Models Forget
	> Early dementia loses the tail distribution, late dementia becomes a delta 
* Reverse Engineering Self-Supervised Learning `NYU` `MIT`
	> SSL learns to cluster samples based on samples/classes/super-classes.
* Masked Image Modeling Advances 3D Medical Image Analysis `WACV`
	> MAE applied on 3D medical images
* VanillaNet: the Power of Minimalism in Deep Learning
	> Multiple non-linear activations; AlexNet-style architecture
* Segment Anything in High Quality `HuggingFace`
* R-MAE: Regions Meet Masked Autoencoders `FAIR` 
	> Extend MAE to learn become region aware; for object detection tasks; #revisited
* Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture `Meta` `US` `NYU` `CA`
	> Yann Lecun's jepa component; not generative (no pixel-prediction); no augmentations; focus on semantics; computationally efficient compared to MAE.

	> Motivation: Both in-variance-based and generative self-supervised approaches have limitations. in-variance-based: (Pros) achieves superior performace when using the correct bias, (Cons) typically for image modality only, require the correct bias (augmentation) for downstream tasks. generative: (Pros) generalize beyond image modality, neither prior or bias is required about downstream tasks, achieves superior performance with full fine-tuning on large datasets (cons) underperform in-variance approaches on linear probing and few-shot fine-tuning benchmarks.
	
	> Method: The paper proposes, joint embedding predictive architecture, I-JEPA, a self-supervised predictive approach for images. I-JEPA is neither contrastive nor generative approach. Despite that, it resemble MAE. They key difference is that I-JEPA predicts "re-construct" the target through a high semantic feature representation, while MAE's target is a low-level representation (RGB pixels). This difference seems marginal, but it requires architectural changes to work properly without model collapse. E.g., I-JEPA uses a two encoders, one updated using gradient descent while the other copies weight using exponential moving average. 
	
	> Results: I-JEPA combines the advantages of both contrastive and generative self-supervised approaches: (1) it achieves superior performance without assuming certain inductive bias (augmentation). It works for a wider set of tasks.
	
	
* AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder
	> SAM extension by introducing a custom prompt encoder.
* FasterViT: Fast Vision Transformers with Hierarchical Attention `Nvidia`
	> Local window attention + global summary (carrier) token attention. Conv at early stages and attention at later stages. dense convolution is compute-intense compared to depth-wise/sparse counterparts. non-linearity, pooling, BNs are memory bound; shall be reduced at early network stages.
* MIMIC: Masked Image Modeling with Image Correspondences `AI2`
	> Use CV techniques (e.g., SIFT features, homography matrix, RANSAC matching, etc) to curate a dataset of pair/multi-view images.
* PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel `Meta`
* Meta-Transformer: A Unified Framework for Multimodal Learning
	> 12-modal transformer. Use LAION dataset to pre-train a ViT shared network. Add modal-specific adapters to transform data into sequence while freezing the pre-trained ViT network.
* Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language
	> multiple vision modules provides a prompt to a LLM (e.g., GPT) which acts as a reasoning module and answers questions.
* Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing
	> ViTs are better are ignoring out-of-context patches, propose Patch Mixing to boost CNNs
* AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos? `Honda` `Nice`
	> Feed sequence-actions for LLM to predict goal. Sequence-actions are generated using CLIP applied on extracted features/embeddings
* FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
		> Split-Q instead of K, parallel over the sequence length, reduce non-matmul operations (apply the statistic once at the end of the row).
* Predicting masked tokens in stochastic locations improves masked image modeling `Meta`
	> MAE with stochastic positional embedding (to model uncertainty); predict feature-space instead of pixel-space 
* Retentive Network: A Successor to Transformer for Large Language Models `MSR`
	> Parallel/Recursive/Chunked-recursive attention formulation -- assume causality.
* From Sparse to Soft Mixtures of Experts `Google` `Nice`
	> Avoid hard assignment to experts. Use soft experts that process a linear combination of input tokens.
* How is ChatGPT's behavior changing over time? 
	> In progress report showing how ChatGPT behavior changes over time.
* Masked Diffusion as Self-supervised Representation Learner
	> Replace denoising from diffusion with masking from Masked Auto-encoder. Use UNet for self-supervised learning.
* Nougat: Neural Optical Understanding for Academic Documents `Meta`
	> Parse scientific pdf to create scientific datasets with mathematical formulas and equations
* CoCa: Contrastive Captioners are Image-Text Foundation Models `Google`
	> Train a single foundational model using both contrastive loss and auto-regressive loss. The paper's objective is to build a model with both alignment (contrastive) and generative (auto-regressive) capabilities
* PARTICLE: Part Discovery and Contrastive Learning for Fine-grained Recognition 
	> Part-based contrastive learning in an iterative fashion using pre-trained models as a starting point. I don't like the iterative natural of this approach, and I doubt its usefulness beyond natural images.
* Explaining grokking through circuit efficiency `Google` `DeepMind`
	> A neural network comprises multiple circuits that either generalize or memorize. Those memorizing circuits are easier to learn with a small training dataset. Yet, as the dataset size increases, they become less efficient, i.e., needs further updates to memorize newly added points. In contrast, generalizing circuits can easily support new training points without needing any updates. Grokking happens when a network switches from  a memorizing circuit to a generalizing circuit. Since generalizing circuit are harder to learn, it takes long training time (many iterations) for the network to learn the generalize circuit and disable/switch-off (regularize) the memorizing circuits.
* Demystifying CLIP Data `Meta`
	> How to curate data for CLIP 
* Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency
	> Use unpaired data -- beside paired data -- to train generative model using a cyclic loss.
* Progress measures for grokking via mechanistic interpretability
	> Show grokking behavior using transformers on modular-addition task; propose progress metric to monitor the learning progress of neural networks. These progress metrics show the network's transition between three phases: memorization phase, circuit formation, and cleanup. Regularization is important for cleanup and generalization.
* LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models
	> Evaluation benchmark for LVLM using image-classification, VQ task; online arena; zero-shot evaluations.
* Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning
	> A variant of Fortuitous forgetting but using a Meta-learning perspective.
* Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization `UMD` `Nice`
	> A Second order optimizer that avoid computing matrix inverse. I am worried AdamW optimizer evaluations are omitted. Code not released yet!
* MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning
	> Add task-specific token for MiniGPT to make task less ambiguous and boost MiniGPT performance.
* Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing `Microsoft`
	> Train CLIP model for medical images using 15M images from journal papers!
* LLaMA: Open and Efficient Foundation Language Models `Meta`
	> Large language model trained on open-datasets; The paper promotes better models for lower inference cost. LLaMA is shortfor Large Language Model by Meta AI.

* Simplifying Transformer Blocks `Nice` `ETH`
	> A new simpler transformer block with parallel attention/FFN layers. No value and output projections. Code released.
* Contrastive Decoding Improves Reasoning in Large Language Models `Meta`
	> More evidence that contrastive decoding is helpful for LLMs (reasoning tasks).
* StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners `Google` `MIT`
	> Synthetic image datasets -- generated by StableDiffusion -- can help outperform SimCLR on real datasets. Synthetic datasets enable multi-positive contrastive learning.
* Learning Vision from Models Rivals Learning Vision from Data `Google`
	> Synthetic text-image curated datasets can help outperform SimCLR on real datasets. Multi-cropping is very helpful, along with EMA and iBoT. ViT evaluations only -- ResNet is dropped!
* Full Parameter Fine-tuning for Large Language Models with Limited Resources `Nice`
	> Lomo optimizer saves memory by updating weights with gradient once computed, then free-ing that layer's gradient before moving to -- and updating -- the next layer.
* AdaLomo: Low-memory Optimization with Adaptive Learning Rate `Nice`
	> gradient's 2nd momentum is more important for the 1st momentum. So AdaLomo = Lomo + gradient's 2nd momentum. Furthermore, normalized gradient using group (per layer) norm instead of global (all layers) norm. 
* When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement `FAIR` `Google` `Samsung`
	> Propose a refinement learning stage after an initial learning stage. The initial learning stage provides a sequence of gradients that is used to set the lr schedule automatically during the second -- refinement -- stage. The paper shows that lr linear-decay emulates the effects of iterate averaging. The paper also shows that lr warm-up is derived from their lr-scheduling proposal. The paper strongly promotes linear-decay lr as a solid scheduler!
* Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection `Nice` `IDEA`
	> open-set object detection model by IDEA lab from China. Leverages both text and image inputs for an object detection model. Train the model using a contrastive loss between object queries and text (categories).
* Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks `Microsoft`
	> Learning a vision architecture for a variety of vision tasks using text (promot) as input and also text as output.
* Contrastive masked autoencoders are stronger vision learners `China` `Academia`
	> Combine contrastive learning with masked autoencoder. Besides pixel-decoder, add feature decoder. Train the feature decoder through a contrastive loss using augmented views fed into an EMA Encoder/backbone. The paper highlights the importance of their proposed feature decoder and a particular augmentation pipeline to make CL valuable.
* OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection `USA` `Academia`
	> A benchmark for ood detection. Evaluate three families of OOD: post-hoc methods, regularized training without outliers, training with outliers. evaluate performance on near-OOD: similar semantic but different style.
* Training a Large Video Model on a Single Machine in a Day `AustinTexas`
	> Use flash-attention, move augmentation to GPUs and save videos in small chunks on disk
	
* Coincident Learning for Unsupervised Anomaly Detection `US` `Stanford`
	> Unsupervised anomaly detection technique that assumes two _indepedent_ measurements for the same sample (input). The two measurements passes through two models. The models are training to maximize alignment ("True positive") and minimize disagreement ("False positives). 
	
* ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders `Google` `US`
	> The paper proposes a general purpose vision-language models for chest X-ray applications. The target applications are: zero-shot classification, data-efficient classification, semantic search, visual question-answering, radiology report quality assurance (QA). The radiology report quality assurance is performed using an LLM judge that compares the models output with the original rad report. The proposed architecture (ELIXR) is influenced by CLIP and BLIP-2 architectures.
	
* BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs `ByteDance` `CN`
	> Confusing paper. The main contribution seems to be a multi-modal architecture that supports audio, image, and text modality. To train such architecture, the paper uses Q-former and a linear projection layer to align image and audio features with the text features. The paper uses Vicuna LLMs. In contrast to related literature, this paper freezes Vicuna during all training/fine-tuning stages -- maybe because it is an already instruct-tuned LLM. To train the proposed tri-modal architecture, the paper prepares a tri-modal instruct-tuning dataset, which seems like another contribution by the paper. Finally, the paper Visual Grounding Pipeline, which is an important contribution that is casually mentioned throughout the paper with little detail. The Visual Grounding Pipeline has three components: (1) tagging module using Recognize Anything Model (RAM), (2) grounding module using Grounding DINO followed by  Segment Anything Model (SAM), and finally (3) entity-matching module which is a GPT model that matches LLM (vicuna) output with the grounded-tags from the tagging and grounding modules. I wish more technical details were presented.
	
* Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V `Microsoft` `US`
	> The authors observed an improved GPT-4V performance on multi-modal tasks (e.g., grounding captions) when the input image is overlayed with a set-of-markers. Building on this, the authors used a set of segmentation (masking) models to extract regions (masks) with different levels of granularity. Each region is assigned an alphanumerics marker. GPT-4V process the overlayed image -- instead of the original text -- and produce regular text output. The paper evaluates the proposed approach on multiple VL tasks: (1) Open-vocabulary Image Segmentation, (2) Referring Segmentation, (3) Phrase Grounding, (4) Video Object Segmentation. Notably, this observation seems to work for GPT-4V model only! 
	
* Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic `SenseTime` `CN`
	> The paper introduces the task of referential dialogues where questions and responses include positioning information (bbox or center) refering to objects. This way, an LLM's response is more specific when it refers to an object (e.g., person) while answering a question. Of course, the paper curated a specific training dataset to enable such features into Shikra, its proposed LLM. The authors found that uses numerical representation (3 decimal) to represent coordinates is enough and achieve superior performance. Yet, the authors highlight the high computational complexity of this approach as it uses more tokens. Interestingly, the paper found that training with on referential dialogues reduces model hallucination! On top of that, training on referential dialogues enables new tasks for VLM, e.g., referring expression captioning, generation (spot captioning), and PointQA.
		
# Generic
* Dataset Distillation for Medical Dataset Sharing `AAAI` `Workshop` `DD`
* 	Koala: A Dialogue Model for Academic Research `Berkeley` `LLMs`
*  Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes `ACL` `LLM`
	> fine-tune LM using labels datasets and (unlabeled) rationals from LLMs 

* MedViT: A Robust Vision Transformer for Generalized Medical Image Classification `Med` `Computers in Biology and Medicine`
	> Pooling for global attention + DWConv for relative-pos embedding with FFN + PONO augmentation for robustness + MHCA to encode high-frequency features!. Eval on MedMNIST
* Transformer-based deep neural network for breast cancer classification on digital breast tomosynthesis `RSNA`
	> Apply TimeSformer on frozen backbone (spatial+temporal attention).
* Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles `Meta` `ICML`
	> Efficient Hierarchical Vision Transformer. Edit MViTv2 such that no overlapping happen between mask-unit. This way, it is possible to pre-train with MAE.
* Internet Explorer: Targeted Representation Learning on the Open Web `ICML`
	> 	Draw concepts from WordNet, use GPT to add descriptor to concept; perform text-to-image search; compute reward for returned image, use reward to adjust distribution over WordNet concepts; iterate again.
* Semi-Supervised and Long-Tailed Object Detection with CascadeMatch `IJCV`
	> Semi-supervised object detection. Use detection stages (heads) as an ensemble; use per-class adaptive threshold.
* VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography
	> New public dataset from Vietnam with 5000 exams (20K images); with both image-level and bounding box annotations.

* A Competition, Benchmark, Code, and Data for Using Artificial Intelligence to Detect Lesions in Digital Breast Tomosynthesis
	> Detector evaluations on DBT. NYU achieves best performance but uses extra internal data. They use recall@FP=1,2,3,4 as a metric. NYU is using EfficientDet, while ZeDuS is using RetinaNet. 
* Visual attention network `Computational Visual Media`
	> Lightweight alternative to PVT; uses depth-wise dilated conv layers
* ObjectLab: Automated Diagnosis of Mislabeled Images in Object Detection Data `JournalClub` `ICMLW`
		> Quantify the quality of bbox annotations assuming three possible mistakes: Swapped label, lousy box, missing box.
* Visual Instruction Tuning `NIPS` `Microsoft` 
	>  The paper proposes a method to create multi-modal instruction-following/tuning dataset automatically using GPT-4. The proposed method is applied on COCO images dataset to generate three types of multi-modal QA: Conversation, Detailed description, and Complex reasoning. Then, the paper propose a light-weight multi-modal architecture, LLaVA. LLaVA is trained using a two-stage pipeline: (1) training an image-2-text projection layer on image-text pairs (subset of CC3M), (2) fine-tuning the projection layer and the LLM on collected instruction-following/tuning dataset.

* OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents `NIPS`
	> The paper presents OBELICS, an open-source image-text interleaved documents dataset that has been crawled from 25 CommonCrawl snapshots. The paper meticulously (carefully) curates the collected dataset by -- to name a few -- identifying high-quality documents, filtering text/image based on usefulness, deduplicating image/text, and filtering NSFW material. Finally, the paper evaluates the utility of OBELICS by training a VLM -- following Flamingo -- and comparing performance on similar baselines.
	
* Siamese Masked Autoencoders `NIPS` `Oral`
	> The paper proposes MAE for videos (data with a temporal dimension). The paper proposes a Siamese MAE architecture that operates on two frames (slices) from an input data point. Both frames are processed by the same encoder, by with an asymmetric masking ratio. 95% of the future frame is masked and reconstructed (in pixel-domain) using the tokens (patches) from the past frame. Unlike the vanilla MAE decoder that leverages self-attention only, the Siamese MAE uses both cross-attention and self-attention within its decoder. The cross-attention operates by mimicking the self-correspondence matrix between different frames.
	
* Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks `NIPS` `UMD` `Meta` `WR` `JRC`
	> Seems to promote CNNs over Transformers, and supervised training vs. self-supervised training. Not sure if its findings will stand the test of time.
	
* VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks `NIPS` `SenseTime`
	> While this seems like an important paper, I found it hard to read! A lot of technical details are missing, and the GitHub repo exists but is empty! To my understanding, VisionLLM is a step beyond vision-specific models (e.g., DETR) and visual-prompt-based models (e.g., SegGPT). VisionLLM tasks a natural language prompt as input and generates vision-specific outputs (e.g., <cls> <x1> <y1> <x2> <y2>” for object detection). The paper introduces a couple of interesting ideas, but with minimal technical details. The following list summarizes three confusing issues with this paper: 
	 (1) The VisionLLM decoder generates offset -- not absolute -- coordinates relative to "the center point". It is not clear what the center point is. Is it the image-center point or the segmented object center point? If it is the latter, which component provides the center to interpret the offset coordinates?
	 (2) The paper introduces a language-guided image tokenizer, an interesting idea that outputs both semantic and positional information. Yet, I don't know what is the dimensionality of the positional token; is this a 2d coordinate or a d-dimensional vector?   
	 (3) The paper introduces output-format-as-query decoding, another interesting idea. But it is not clear how that works. The paper states, "We first use _LLMs_ to parse the structural output format from the task instructions"; what LLMs? Is that an off-the-shelf LLM?
	 
 > It is worth noting that VisionLLM seems to have two types of language instruction: (1) vision language tasks, which align with standard VLMs, (2) Vision-Only tasks, which deviate from VLMs, that seem to use  output-format-as-query decoding!

* Blockwise Parallel Transformer for Large Context Models `NIPS` `UCBerkeley`
	> Fuse FPN with attention within a blockwise formulation.
* Alpaca: A Strong, Replicable Instruction-Following Model `Blog` `Stanford`
	> Create an instruction-following dataset following self-instruct paper. Fine-tune LLaMA on this dataset to create both a light-weight and an efficient alternative model for GPT-3.5.
* Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality `Blog` `LMSYS` `CMU`
	> The paper tackles two problems: (1) How to acquire instruction-following dataset for fine-tuning LLama model, (2) How to evaluate the trained model. The paper solves the first problem by using ShareGPT dataset, a user-shared conversations collected from ShareGPT.com. To solve the second problem, the paper uses GPT-4 model to rate answers from different models. The training and deployment technicalities seems borrowed from Alpaca (from Stanford).
	
* Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning `TMLR` `Meta` `SSL`
	> SSL approaches learn a bag-of-patch representation which reflects their co-occurrence.
* Contrastive Decoding: Open-ended Text Generation as Optimization `ACL` `FAIR`
	> Reduce LM hallucination/repetaition by comparing expert and amateur models output tokens.
* Ariadne's Thread:Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray images `MICCAI`
	> Use text embedding to create pseudo annotations for medical images.
* LLMs Accelerate Annotation for Medical Information Extraction `ML4H` `Google`
	> Use LLM to generate pseudo annotations for humans to refine. Mostly text only datasets
* Population-wide evaluation of artificial intelligence and radiologist assessment of screening mammograms `Denmark` `Universities` `WR` `JRC`
	> Evaluate AI as a substitute for the first reader in dual-reader mammograms (Europe).
* Three Towers: Flexible Contrastive Learning with Pretrained Image Models `NIPS` `Google`
	> Train both image and text towers from scratch but use a third frozen pre-trained image tower.

* 4M: Massively Multimodal Masked Modeling `NIPS` `Apple`
	> Train MultiModal MAE with both text and images. Every modality is tokenized.

* HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face `NIPS` `Microsoft` `CN`
	> Motivation: Vanilla LLMs can handle complex tasks (prompts) that involve multiple sub-tasks and modalities. In addition, Vanilla LLMs are inferior to some expert models (e.g., task-specific fine-tuned models). 
	> The paper leverages ChatGPT3.5 as a task planner to orchestrate task execution across different expert models. Basically, after receiving a user prompt, ChatGPT3.5 (1) plans tasks to be executed, (2) selects expert models to execute these tasks, (3) performs task execution, (4) generates a response. 
	> Evaluation: Evaluating HuggingGPT is not trivial. The paper mainly evaluates HuggingGPT in terms of task selection, and not the final output. To do so, the authors curated a dataset that contains user prompts and the correct tasks to be executed. This is not a trivial feat. So, Kudos on that effort.
	> Results: ChatGPT3.5 delivers promising performance, acting as an orchestrator between expert models.

* ClusterFormer: Clustering As A Universal Visual Learner `NIPS` `Meta`
	> Perform feature clustering during fully supervised learning! It would have been more interesting if applied for SSL. I didn't like the paper's writing style.

* Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution `NIPS`
	> a ViT model for any aspect ratio and resolution. It borrows the example-packing idea from NLP literature. The proposed model supports random patch masking (dropping) while still seeing some complete images. This reduces train/inference discrepancy.

* Are Vision Transformers More Data Hungry Than Newborn Visual Systems? `NIPS`
	> The paper compares ViTs with chicks raised in impoverished environment. The paper concludes that ViTs are not data hungry and comparable to chicks. My main concern is that the evaluation task (testing phase) is a very naive binary classification task!
* Delving into Noisy Label Detection with Clean Data `WR` `JRC` `ICML`
	> Sort noisy samples using prediction confidence as a score. Use a sophisticated p-value threshold to identify corrupt samples.
* SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot `ICML`
	> Pruning LLMs. 50% sparsity without a significant performance drop is possible. Post-training pruning -- mask a set of weights and adjust these weights to minimize construction loss || W X - (M . W) X ||.

* Automatic correction of performance drift under acquisition shift in medical image classification 
	> Propose cumulative distribution matching between reference and target domains to align predictions made by AI models undergoing distribution shift (e.g., Scanner or software change).

* UniFormer: Unifying Convolution and Self-attention for Visual Recognition `SenseTime` `TPAMI2023`
	> PVTv2 alternative -- use conv attention instead of vanilla attention in early stages.

* Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding `CN` `Alibaba`
	> Motivation: No single MLLM integrates both vision and audio with a chat LLM.
	
	> Method: The paper proposes Video-LLaMa, a multi-modal LLM. It supports both video/image and audio inputs, in addition to the standard language input. The paper uses frozen pre-trained image and audio encoders. Besides the frozen encoders, the paper utilizes both temporal positional embedding and Q-former components. The positional embedding guide the model to learn temporal dynamics in a video, while the Q-former learns a fixed number of embeddings. This reduces the computational complexity of Video-LLaMa. While the architecture is typical, the training stage is interesting. The paper follows the expected two-stage training process, where the first stage pre-trains the model on a large amount of data for vision-text alignment, then the second stage fine-tunes the model on a small amount of high-quality instruction-following data. Due to the limited amount of audio-text data, the paper uses ImageBind (encoders) to train the audio branch (Audio Q-former and adaptor) without audio data. Specifically, Vidoe-LLaMA uses the image-text data and the image encoder from ImageBind to train the Audio Q-former. This surprising idea depends on the fact that ImageBind aligns the vision and audio embedding space.

* GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints `Google` `EMNLP`
	> A compromise between multi-head and multi-query attention. Intsead of h keys and values, the paper propose g keys and values, where g < h. The goal is to speedup inference without sacrificing performance.

* CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training `MICCAI`
	> Train a clip model for medical images. Leverage augmentation techniques for both image and text to tackle data shortage.

* What’s “up” with vision-language models? Investigating their struggle with spatial reasoning `EMNLP` `AI2`
	> large VLM struggle with simple spatial reasoning (up or down questions). Current benchmark inflate their performance. Little spatial reasoning data available during training contribute to such poor performance.

* Med-Flamingo: a Multimodal Medical Few-shot Learner `USA` `Stanford` `WR` `JRC` `PMLR`
	> fine-tuning open-Flamingo to medical datasets extracted from medical books.
	
* SIM-CNN: Self-supervised Individualized Multimodal Learning for Stress Prediction on Nurses Using Biosignals `US` `ICML` `Workshop`
	> Per-individual (nurse) self-supervised learning approach that predicts next step (time) features. Basically, an old implementation for autoregressive task. Seems like the nurses dataset has a lot of noise 
	
* Impact of Artificial Intelligence–driven Quality Improvement Software on Mammography Technical Repeat and Recall Rates `Rad AI` `WR`
	> The paper used AI model, not to give real-time feedback to Techs and Rads, but to give feedback within regular feedback sessions. Techs are able to view their Image Quality scores and identify areas of improvements. The paper focused on Positioning and compression problems only and not blur. Quantitative evaluation is performed between `baseline` period and `current` period, i.e., no direct comparison between AI and Rad. The proposed AI Model -- Volpara Analytics -- is aimed at training technicans over long-period of time and not giving them real-time feedback.
	
* Image Captioners Are Scalable Vision Learners Too `Google` `DeepMind`
	> Image Captioners (Cap) lags on zero-shot classification benchmarks compared to contrastive-based models (CLIP). Yet, this performance gap decreases as the model scales. Beyond zero-shot classification, Cap models are better on few-shot classification benchmarks and multimodal downstream tasks: captioning, OCR, and VQA tasks. The paper proposes parallel prediction to mitigate autoregressive training limitations: 'To predict the first few tokens of a caption, the decoder can benefit a lot from using the image information, while to predict later tokens it can rely more and more on already predicted tokens'. One key advantage for Cap -- over CLIP -- is that it doesn't need computations across devices (e.g., gather mini-batches features across GPUs)
	
* Scaling Open-Vocabulary Object Detection `Google` `DeepMind`
	> The paper proposes a recipe for pre-training open-world localization (OWL) models. The recipe has three steps: (1) generate pseudo labels from an open-world localization model -- trained in a fully supervised manner on combining standard bbox annotated datasets; (2) pre-trained a new OWL model using the pseudo labels with a low filtering threshold (a moderate threshold of 0.3 works well); (3) optionally fine-tuned the pre-trained model on fully annotated datasets.
	
* Machine Learning for Image-Based Radiotherapy Outcome Prediction `DK` `DIKU`
	> A chapter on radiotherapy listing imaging technologies and biomarkers. For imaging modalities, the paper lists Computed Tomography (CT), Magnetic Reasoning (MR), and Positron emission tomography (PET). Both MR and PET deliver lower-resolution images compared to CT. Yet, both use less radiation, which makes them safer compared to CT. Interestingly, malignant tumor (cells) consume more glucose (fluorodeoxyglucose) because cancer cells' metabolic activity and accordingly need more glucose (fuel). This is the basic idea behind PET. After listing imaging modalities, the paper lists biomarkers. Different biomarkers have different values that depend on their predictive power and repeatability. 
	
* Space Debris: Are Deep Learning-based Image Enhancements part of the Solution? `LU` `SnT` `ISCS` `International Symposium on Computational Sensing`
	> The paper proposes a UNet architecture to enhance monocular images with space artifacts, e.g., blurring, exposure issues, poor contrast, and noise.
	
* LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day `Microsoft` `NIPS`	
	> LLaMA => LLaVA => LLaVA-Med. The paper (1) extracts image-text (caption) pairs from PubMed dataset, (2) creates instruction-following dataset automatically using  GPT-4, (3) fine-tune a generalized VLM model (LLaVA) into the Biomedical domain using a two-stage pipeline. The first stage aligns vision and text embedding using an auto-regressive task: Given an image and an instruction to explain the image, the model generates the image caption. The second stage trains the model on instruction-following data to master open-ended conversations.

* Language Is Not All You Need: Aligning Perception with Language Models `MultiModal` `MLLM` `Microsoft` `NIPS`
	> The paper proposes an MLLM using LLM as a universal task layer and modality-specific (e.g., vision) encoders to encode other modalities. Modality-specific encoders are frozen -- except the last layer -- during training while the LLM is trained on image-text and interleaved text and images datasets. After pre-training on image-text datasets, the MLLM is further fine-tuned on instruction-following datasets (e.g., Unnatural Instructions and FLANv2). The paper uses Resampler (a.k.a Perceiver IO and Q-Former) to reduce the number of image embeddings. The proposed MLLM, Kosmos, is evaluated on a new multi-modal benchmark called Raven. Raven IQ is based on Raven’s Progressive Matrices and evaluates the capability of nonverbal reasoning for MLLMs.


* BLIP-2: bootstrapping language-image pretraining with frozen image encoders and large language models `Salesforce` `US` `ICML`
	> The paper proposes a two-stage pipeline to train a VLM model on top of frozen vision and LLMs. Thus, the key contribution is computational efficiency during training. To achieve their objective, the paper proposes a Q-former (query-former); this component acts like a linear-projection between the visual and language representations. Yet, it has more responsibility (functionality) since the LLM is completely frozen in BLIP-2, a constraint that has been relaxed in follow-up VLM papers. Yet, the core idea here is that BLIP trains a Q-former only while keeping the vision and language model frozen. The Q-former is trained in two stages: (1) this first stage bootstrap vision-language representation Learning using three objectives previously proposed in BLIP, i.e., ITC, ITM, LM; then, (2) the second stage bootstrap vision-to-language generative learning.

* Improving Language Plasticity via Pretraining with Active Forgetting `NIPS` `Meta` `Cohere` `UK` `Nice`
	> The paper proposes active forgetting for pre-training language models (PLMs). In active forgetting, the embedding layer is reset every K steps while the PLM's body (transformer) is learned in a standard manner. The paper shows that active forgetting improves the PLM's ability to adapt to a new language, especially within a low-data regime and within a limited computational budget. The paper shows that active forgetting is particularly beneficial when adapting to a highly dissimilar language. The paper hypothesizes that active forgetting (1) encodes more universal knowledge in the transformer body, while standard PMLs may encode more “shortcut” knowledge; (2) gradually locates the PML body (transformer) on a particular manifold, where it can easily cooperate with new embeddings. In simpler words, active forgetting simulates language switching during pretraining.
	
* Segment Everything Everywhere All at Once `NIPS` `Microsoft`
	> The paper proposes SEEM, segmentation everything everywhere in an image. SEEM supports both text and spatial prompt (points, bbox, scribbles) prompts. SEEM supports all segmentation tasks such as semantic, instance, and panoptic (semantic+instance). The paper introduce a VisualSampler that encodes all spatial querys (points, bbox, scribbles) using a single encoder.
	Besides standard one-shot segmentation, SEEM supports interactive segmentation. To perform interactive segmentation efficiently, the paper proposes memory prompt to keep track of previous segmentatin masks. I like how the paper matched text prompt with class embedding and spatial prompt with mask embedding.
	
* Evaluating Object Hallucination in Large Vision-Language Models `CN`
	> Motivation: There is a need for a scalable, consistent metric for evaluating VLMs -- or LVLMs -- on object hallucination. CHAIR is an established metric, but it has limitations, e.g., it is sensitive to prompt and output sequence length.
	
	> Method: The paper proposes,Polling-based Object Probing Evaluation (POPE) evaluation pipeline. Instead of evaluating output captions, POPE queries VLMs with binary (YES/NO) questions about objects inside an image, then benchmarks different models using accuracy, F1-score metrics. The paper finds that VLMs are likely to hallucinate frequent -- and co-occurring -- objects within visual instruction datasets. To quantify this limitation, POPE devises three sampling strategies for `No` questions: (1) The negative object is sampled randomly, (2) the negative object occurs frequently within the tuning datasets  (e.g., person), (3) the negative object co-occurs frequently (e.g., keyboard and mouse).
	
	> Results: The paper evaluates various VLMs using POPE. POPE delivers a more stable, scalable consistent evaluation pipeline.  
	

	
# CVPR
* MVImgNet: A Large-scale Dataset of Multi-view Images `Dataset`
* Simulated Annealing in Early Layers Leads to Better Generalization `KE` `LLF` `Mila`

* Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention
	> Efficient Local attention at early layers and regular global attention at late layers
* Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models `Diffusion` `UMD`

* IMAGEBIND: One Embedding Space To Bind Them All `Meta`
	> A single embedding for image, text, depth, audio, Inertial Measurement Unit (IMU)
* GeneCIS: A Benchmark for General Conditional Image Similarity `Meta`
	> Collect a new dataset (benchmark) for conditional image similarity evaluations. Use combiner (I+T) to adapt the retrieval task according to a text condition T. 
* Revealing the Dark Secrets of Masked Image Modeling 
	> Why MIM fine-tunes better compared to supervised and self-supervised (MoCov2) models.
* Teaching Matters: Investigating the Role of Supervision in Vision Transformers `UMD` `Nice`
	> Deep analysis on the impact of different training techniques (fully supervised vs. contrastive vs. reconstruction)
* SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer `MIT`
	> Prune Window Attention on Swin-T for high-resolution inputs.

* Hyperbolic Contrastive Learning for Visual Representations beyond Objects `UMD` `Nice`
	> Add hyperbolic term to contrastive learning to enforce object-scene graph/relationship in the learned representation.
* Semi-DETR: Semi-Supervised Object Detection with Detection Transformers `WR-JRC` `Semi-supervised`
	> one-to-many mapping from strong augmentation to weak-augmentation. Propose a workaround for consistency loss in DETRs. Mine more pseudo bbox for consistency loss term.
* Revisiting Class Imbalance for End-to-end Semi-Supervised Object Detection `CVPRW` `WR-JRC`
	> A bunch of tricks for Semi-supervised Object detection.
* InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions `Nice`
	> PVT 1st author; Foundational model with deformable conv operators. Seems superior on small scale only!
* DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection 
	> Add class token and foreground tokens to make weakly-supervised cross-domain object detection possible.
* DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training `Nice`
	> Memory efficient implementation of contrastive loss (code released)!
* PaReprop: Fast Parallelized Reversible Back-propagation `Workshop` `Spotlight` `Nice`
	> Re-compute activations in parallel while doing backward pass (compute gradient and update weights).

* Label-Free Liver Tumor Segmentation `Medical` `WR` `JRC`
	> Create synthesized tumors using hard-crafted pipeline. Use the synthesized tumors to evaluate SOTA model on various dimensions (size, texture, intensity, etc).

* Images Speak in Images: A Generalist Painter for In-Context Visual Learning
	> Define in-context prompts using image pairs. E.g., input image + depth estimation is an in-context prompt that specifies depth estimation as required task.

* Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors `SenseTime`
	> Sample sparse Multi-Scale Features to introduce FPN into Transformer-Based detectors with minimal memory overhead.

* VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking `China` `Labs`
	> Mask tubelets during pre-training. Use asymmetric encoder-decoder architecture to reduce the decoder computational cost; Only a subset of masked tokens (patches) are re-constructed. Construct and Train on a large video dataset.

* Understanding Masked Image Modeling via Learning Occlusion Invariant Feature `China`
	> MAE can be regarded as occlusion invariant learning. The paper claims that MAE learns data-agnostic initialization; thats why MAE suffers on linear-probing while perform best after fine-tuning.

* Siamese Image Modeling for Self-Supervised Vision Representation Learning `China` `HongKong`
	> MAE applied on different view. Reconstruct latent feature instead of pixels. Achieve both  spatial sensitivity and semantic alignment.
	
* Stitchable Neural Networks `Nice` 
	> Stitching different networks vertically using an adapter layer. The paper motivate is to interpolate the accuracy/efficiency performance between pre-trained network variants (e.g., Tiny/Small/Base)
	
* DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation `Google` `Boston` `US`
	> How to fine-tune stable diffusion model on subject specific few-shots images without overfitting. The paper introduce unique identifier to the subject specific object and a secondary loss term to preserve semantic prior.
	
* Fine-tuned CLIP Models are Efficient Video Learners `MBZ` `Video` `UAE` `AU`
	> Motivation: How to efficiently fine-tune (adapt) CLIP models on video tasks with neither overfitting nor high computational complexity
	
	> Method: The paper proposes Video Fine-tuned CLIP (ViFi-CLIP), a simple but competitive video fine-tuning approach. The key idea is to avoid complex modeling components, and use a simple average across frames for temporal modeling. This simple approach is computationally cheap and achieves superior performance compared to more complex approaches. Yet, it only works when a large amount of data is available to tune the entire CLIP (vision + text encoders) model. For a low-data regime, the authors propose a two-stage solution: (1) Fine-tune on large amounts of data using the proposed temporal modeling approach, then (2) use prompt tuning to adapt the fine-tuned CLIP (from the first stage) on small video datasets. 
	
# ICCV
* DETR Doesn't Need Multi-Scale or Locality Design `DETR` `Microsoft` `Nice`
	>  (1) Introduce bbox bias (similar to rel-pos bias) to help DETR focus on relevant parts of the images
	(2) MIM for pre-training, (3) tweak bbox regression loss to detect high quality small bboxes.
* FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization `Apple`
	> Focus on reducing latency during inference using reparameterization. Replace expensive operations (e.g., self-attention) with conv operations. Replace dense KxK with linear train-time overparameterization.
* Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement `Apple`
	> Feed augmented samples to superior ensemble model; save the augmentation and sparse Top-K probabilities. Use saved samples to train future architecture.
* Sigmoid Loss for Language Image Pre-Training `Nice` `Google`
	> Replace softmax in CLIP with Sigmoid. This eliminates the need for two all-gather calls and reduces the memory required by large batch-size, i.e., avoids the NxN similarity matrix.
* SparseDet: Improving Sparsely Annotated Object Detection with Pseudo-positive Mining `UMD`
	> Use self-supervised consistency loss to tackle Sparsely Annotated Object Detection problem. I doubt the proposed solution supports DETR-style based detector. The paper proposal seems to depend on a clear groundtruth-proposal alignment which is not the case for DETR-style detectors -- these depend on bipartite matching.
* DreamTeacher: Pre-training Image Backbones with Deep Generative Models `Nvidia`
	> Use Generative models (e.g., diffusion model) to pre-train CNN backbones (e.g., ConvNext). 
* Scale-MAE: A Scale-Aware Masked Autoencoder for Multi-scale Geospatial Representation Learning `META`
	> extend MAE for remote sensing data; Add round Sample Distance Positional Encoding to patches; decode/reconstruct image at multiple scales (l2 loss on small scale, l1 loss on high scale).
* Scale-Aware Modulation Meet Transformer
	> Use modulated conv operations in early layers to reduce computational complexity.
* What can a cook in Italy teach a mechanic in India? Action Recognition Generalization Over Scenarios and Locations
	> Action recognition evaluation across different domains (domain/scenario shift).
* Subclass-balancing Contrastive Learning for Long-tailed Recognition 
	> Solving imbalance classification problem by sub-classing head (majority) classes and using contrastive learning.
* Multi-Label Self-Supervised Learning with Scene Images
	> Casting SSL on scene images as multi-label classification problem.
* Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers `WR` `JRC`
	> Update a sparse set of tokens in ViTs instead of every token for computational efficiency in Video applications.
* Identity-Consistent Aggregation for Video Object Detection
	> Extend DETR for video object detection. Propose consistent aggregation between queries across frames. Not sure why it is called ClipVID!
* EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction `MIT`
	> Conv-Attention Hybrid architecture. Global Attention applied in stage 3 and 4 only. Also proposed using efficient attention, i.e., RELU instead of Softmax. Nice multi-scale architecture. Not suitable for MAE pre-training.
* Flatten Transformer: Vision Transformer using Focused Linear Attention
	> Replace softmax quadratic complexity with Relu linear complexity attention. Use element-wise power operation to increase Relu attention focus (sharpness). Use a follow-up depth-wise convolution to pump the attention matrix rank.
* MULLER: Multilayer Laplacian Resizer for Vision `Google` `Nice`
	> Learns a down-sampler (resizer) to boost classification performance. Works for various architectures and introduces a small (handful) number of parameters. Only TF implementation is released. I converted it to PyTorch which generate identical TF result. Yet, it is seems slow!
	

# ICLR

* Avoiding spurious correlations via logit correction
	> Train two models: the first model learns the joint-probability of label and attribute P(y,a), while the second model leverage the first model to reduce the impact of spurious correlations. The paper also uses Mixup augmentation which do most of the heavy lifting (Tab. 3)

* Conditional Positional Encodings for Vision Transformers `ViT`
	> relative positional embedding with latent absolute positional information (zero-padding)
* On the duality between contrastive and non-contrastive self-supervised learning
	> sample contrastive vs. dimension contrastive. Revise assumptions about batch-size and embedding dimension
* Visual classification via description from large language models `Nice`
	> Extract visual description \phi from LLMs; use \phi as text for CLIP models to compute similarity
* Learning Object-Language Alignments for Open-Vocabulary Object Detection
	> Train a detector using both bbox annotations and image-text pairs.
* Advancing radiograph representation learning with masked record modeling 
	> A combination of MAE + BERT for radiography.
* What Algorithms can Transformers Learn? A Study in Length Generalization
	> Transformers can generalize on algorithms that can be represented using RASP-L language. Ironically, RASP language is designed to define functions which are “easy to represent” for Transformers!
* Quantifying Memorization Across Neural Language Models `Google`
	> LLMs memorize their training data. Large models memorize more, duplicates in dataset increases memorization. It _seems_ like Masked modeling memorize less compared to auto-regressive models.

* Omnigrok: Grokking Beyond Algorithmic Data
	> Grokking can happen on natural images (e.g., mnist dataset). Grokking is related to the initialization weight norm.
* Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling
	> MAE + CNNs is possible thanks to sparse convolution operation.
* Masked Frequency Modeling for Self-Supervised Visual Pre-Training `Singapore`
	> Masked-autoencoder that re-construct (low/high) frequencies instead of pixels. No pixel-masking is performed, so this approach works for both CNNs and ViTs. One strange thing: the paper compares their proposal (MFM) with vanilla (MAE) using the same number of pre-training epochs! I think MAE should be pre-trained for 4x epochs when using 0.75 masking ratio. 
* Extremely Simple Activation Shaping for Out-of-Distribution Detection `ML Collective` `Google`
	> ASH (activation shaping) for ood detection. ASH is a post-hoc ood method that Prunes intermediate features. ASH assumes gray-box access to the model.
	
* TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second `GE` `Academia` `Bosch`
	> Motivation: Prior Fitted Network (PFN) has demostrated promising results measuring the posterior predictive distribution efficiently on tiny and small setups (e.g., Gaussian Processes). Time to expect PFN to real datasets, e.g., Tabular datasets.
	
	> Method: The paper presents TabPFN, which performs Bayesian inference on tabular datasets efficiently. This paper depends heavily on the previous PFN paper for the mathematical foundation. Thus, this paper focuses mainly on tabular data, i.e., how to inject/embed/pick a suitable prior for Tabular data and how to evaluate TabPFN on standard benchmarks. TabPFN samples synthetic data from both artificial Bayesian neural networks (BNN) and Structural causal models (SCM).
	
	> Results: TabPFN achieves superior performance on tabular data compared to state-of-the-art methods (e.g., Decision Trees). Besides the better performance, TabPFN achieves its superior performance without fine-tuning and with signficant efficiency (e.g., 230×-5700× speedup). While TabPFN doesn't require fine-tuning on real-datasets, TabPFN can be fine-tuned to further achieve higher performance.
	
	
# WACV

* Accelerating Self-Supervised Learning via Efficient Training Strategies `Nice`

	> Fixed 1-cycle Learning Rate Schedule (warmup for lr + inverse warmup for momentum) + Progress learning in terms of image-size and augmentation. Use dummy forward to find hard/useful pairs for training.
