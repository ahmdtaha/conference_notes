# arXiv
* Toolformer: Language Models Can Teach Themselves to Use Tools `Meta`
* ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders `Meta` `MAE`
* Language Is Not All You Need: Aligning Perception with Language Models `MultiModal` `LLM` `Microsoft`
* ViperGPT: Visual Inference via Python Execution for Reasoning `Nice`
	> High level **generic** reasoning system
* The effectiveness of MAE pre-pretraining for billion-scale pretraining `Meta`
* ActiveLab: Active Learning with Re-Labeling by Multiple Annotators
* Cut and Learn for Unsupervised Object Detection and Instance Segmentation `Meta` `Det` `Nice` `NCut`
	> Using normalized cuts to generate multiple masks, Conditional Random Field (CRF) to generate bboxes
* Segment Anything `Meta` `Seg`
* SAM Struggles in Concealed Scenes -- Empirical Study on "Segment Anything"
* Choose Your Weapon: Survival Strategies for Depressed AI Academics
	> Should read again
* DINOv2: Learning Robust Visual Features without Supervision
	> A lot of technical tricks to speed training and reduce memory
* Symbolic Discovery of Optimization Algorithms `Google`
	> LION optimizer using evolutionary algorithm (mutation, insertion, deleteion).
* What does CLIP know about a red circle? Visual prompt engineering for VLMs
	> Rare events can be learned with large models when trained on large datasets!
*  A Watermark for Large Language Models `LLM` `UMD` `Nice`
*  Dropout Reduces Underfitting `Meta` `Nice`
	> Nice paper with nice analysis. Use Early dropout for small (underfitting) models and late dropout for large (overfitting) models.
* RWKV: Reinventing RNNs for the Transformer Era `LLM`
	> W == relative positional embedding (bias). WKV == Weighted Key-Value, R = Receptence gate (forget gate). 
* A PhD Studentâ€™s Perspective on Research in NLP in the Era of Very Large Language Models `NLP` `LLMs`
	> Future research directions in the field of NLP
* Model Dementia: Generated Data Makes Models Forget
	> Early dementia loses the tail distribution, late dementia becomes a delta 
* Reverse Engineering Self-Supervised Learning `NYU` `MIT`
	> SSL learns to cluster samples based on samples/classes/super-classes.
* Masked Image Modeling Advances 3D Medical Image Analysis `WACV`
	> MAE applied on 3D medical images
* VanillaNet: the Power of Minimalism in Deep Learning
	> Multiple non-linear activations; AlexNet-style architecture
* Segment Anything in High Quality `HuggingFace`
* R-MAE: Regions Meet Masked Autoencoders `FAIR`
	> Extend MAE to learn become region aware; for object detection tasks
* Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture `Meta`
	> Yann Lecun's jepa component; not generative (no pixel-prediction); no augmentations; focus on semantics; computationally efficient compared to MAE.
* AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder
	> SAM extension by introducing a custom prompt encoder.
* FasterViT: Fast Vision Transformers with Hierarchical Attention `NVidia`
	> Local window attention + global summary (carrier) token attention. Conv at early stages and attention at later stages.
* MIMIC: Masked Image Modeling with Image Correspondences `AI2`
	> Use CV techniques (e.g., SIFT features, homography matrix, RANSAC matching, etc) to curate a dataset of pair/multiview images.


# Generic
* Dataset Distillation for Medical Dataset Sharing `AAAI` `Workshop` `DD`
* 	Koala: A Dialogue Model for Academic Research `Berkeley` `LLMs`
*  Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes `ACL` `LLM`
	> fine-tune LM using labels datasets and (unlabeled) rationals from LLMs 
* MedViT: A Robust Vision Transformer for Generalized Medical Image Classification `Med` `Computers in Biology and Medicine`
	> Pooling for global attention + DWConv for relative-pos embedding with FFN + PONO augmentation for robustness + MHCA to encode high-fequencey features!. Eval on MedMNIST
* Transformer-based deep neural network for breast cancer classification on digital breast tomosynthesis `RSNA`
	> Apply TimeSformer on forzen backbone (spatial+temporal attention).

	
# CVPR
* MVImgNet: A Large-scale Dataset of Multi-view Images `Dataset`
* Simulated Annealing in Early Layers Leads to Better Generalization `KE` `LLF` `Mila`
* Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention
	> Efficient Local attention at early layers and regular global attention at late layers
* Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models `Diffusion` `UMD`
* IMAGEBIND: One Embedding Space To Bind Them All `Meta`
	> A single embedding for image, text, depth, audio, Inertial Measurement Unit (IMU)
* GeneCIS: A Benchmark for General Conditional Image Similarity `Meta`
	> Collect a new dataset (benchmark) for conditional image similarity evaluations. Use combiner (I+T) to adapt the retrieval task according to a text conidition T. 
* Revealing the Dark Secrets of Masked Image Modeling 
	> Why MIM fine-tunes better compared to supervised and self-supervised (MoCov2) models.
* Teaching Matters: Investigating the Role of Supervision in Vision Transformers `UMD` `Nice`
	> Deep analysis on the impact of different training techniques (fully supervised vs. contrastive vs. reconstruction)
* SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer `MIT`
	> Prune Window Attention on Swin-T for high-resolution inputs.
* Hyperbolic Contrastive Learning for Visual Representations beyond Objects `UMD` `Nice`
	> Add hyperbolic term to contrastive learning to enforce object-scene graph/relationship in the learned representation.

# ICLR
* Avoiding spurious correlations via logit correction
	> Train two models: the first model learns the joint-probability of label and attribute P(y,a), while the second model leverage the first model to reduce the impact of spurious correlations. The paper also uses Mixup augmentation which do most of the heavy lifting (Tab. 3)
* Conditional Positional Encodings for Vision Transformers `ViT`
	> relative positional embedding with latent absolute positional information (zero-padding)
* On the duality between contrastive and non-contrastive self-supervised learning
	> sample contrastive vs. dimension contrastive. Revise assumptions about batch-size and embedding dimension
