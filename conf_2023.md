# arXiv
* Toolformer: Language Models Can Teach Themselves to Use Tools `Meta`
* ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders `Meta` `MAE`
* Language Is Not All You Need: Aligning Perception with Language Models `MultiModal` `LLM` `Microsoft`
* ViperGPT: Visual Inference via Python Execution for Reasoning `Nice`
	> High level **generic** reasoning system
* The effectiveness of MAE pre-pretraining for billion-scale pretraining `Meta`
* ActiveLab: Active Learning with Re-Labeling by Multiple Annotators
* Cut and Learn for Unsupervised Object Detection and Instance Segmentation `Meta` `Det` `Nice` `NCut`
	> Using normalized cuts to generate multiple masks, Conditional Random Field (CRF) to generate bboxes
* Segment Anything `Meta` `Seg`
* SAM Struggles in Concealed Scenes -- Empirical Study on "Segment Anything"
* Choose Your Weapon: Survival Strategies for Depressed AI Academics
	> Should read again
* DINOv2: Learning Robust Visual Features without Supervision
	> A lot of technical tricks to speed training and reduce memory
* Symbolic Discovery of Optimization Algorithms `Google`
	> LION optimizer using evolutionary algorithm (mutation, insertion, deleteion).
* What does CLIP know about a red circle? Visual prompt engineering for VLMs
	> Rare events can be learned with large models when trained on large datasets!
*  A Watermark for Large Language Models `LLM` `UMD` `Nice`
*  Dropout Reduces Underfitting `Meta` `Nice`
	> Nice paper with nice analysis. Use Early dropout for small (underfitting) models and late dropout for large (overfitting) models.
* RWKV: Reinventing RNNs for the Transformer Era `LLM`
	> W == relative positional embedding (bias). WKV == Weighted Key-Value, R = Receptence gate (forget gate). 
* A PhD Studentâ€™s Perspective on Research in NLP in the Era of Very Large Language Models `NLP` `LLMs`
	> Future research directions in the field of NLP
* Model Dementia: Generated Data Makes Models Forget
	> Early dementia loses the tail distribution, late dementia becomes a delta 
* Reverse Engineering Self-Supervised Learning `NYU` `MIT`
	> SSL learns to cluster samples based on samples/classes/super-classes.
* Masked Image Modeling Advances 3D Medical Image Analysis `WACV`
	> MAE applied on 3D medical images
* VanillaNet: the Power of Minimalism in Deep Learning
	> Multiple non-linear activations; AlexNet-style architecture
* Segment Anything in High Quality `HuggingFace`
* R-MAE: Regions Meet Masked Autoencoders `FAIR`
	> Extend MAE to learn become region aware; for object detection tasks
* Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture `Meta`
	> Yann Lecun's jepa component; not generative (no pixel-prediction); no augmentations; focus on semantics; computationally efficient compared to MAE.
* AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder
	> SAM extension by introducing a custom prompt encoder.
* FasterViT: Fast Vision Transformers with Hierarchical Attention `NVidia`
	> Local window attention + global summary (carrier) token attention. Conv at early stages and attention at later stages. dense convolution is compute-intense compared to depth-wise/sparse counterparts. non-linearity, pooling, BNs are memory bound; shall be reduced at early network stages.
* MIMIC: Masked Image Modeling with Image Correspondences `AI2`
	> Use CV techniques (e.g., SIFT features, homography matrix, RANSAC matching, etc) to curate a dataset of pair/multiview images.
* PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel `Meta`
* Meta-Transformer: A Unified Framework for Multimodal Learning
	> 12-modal transformer. Use LAION dataset to pre-train a ViT shared network. Add modal-specific adapters to transform data into sequence while freezing the pre-trained ViT network.
* Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language
	> multiple vision modules provides a prompt to a LLM (e.g., GPT) which acts as a reasoning module and answers questions.
* Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing
	> ViTs are better are ignoring out-of-context patches, propose Patch Mixing to boost CNNs
* AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos? `Honda` `Nice`
	> Feed sequence-actions for LLM to predict goal. Sequence-actions are generated using CLIP applied on extracted features/embeddings
* FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
		> Split-Q instead of K, parallel over the sequence length, reduce non-matmul operations (apply the statistic once at the end of the row).
* Predicting masked tokens in stochastic locations improves masked image modeling `Meta`
	> MAE with stochastic positoinal embedding (to model uncertainty); predict feature-space instead of pixel-space 	
* Retentive Network: A Successor to Transformer for Large Language Models `MSR`
	> Parallel/Recursive/Chunked-recursive attention formulation -- assume causality.
* From Sparse to Soft Mixtures of Experts `Google` `Nice`
	> Avoid hard assignment to experts. Use soft experts that process a linear combination of input tokens.
* How is ChatGPT's behavior changing over time? 
	> In progress report showing how ChatGPT behavior changes over time.
* Masked Diffusion as Self-supervised Representation Learner
	> Replace denoising from diffusion with masking from Masked Auto-encoder. Use UNet for self-supervised learninig.
* Nougat: Neural Optical Understanding for Academic Documents `Meta`
	> Parse scientific pdf to create scientific datasets with mathematical formulas and equations
* CoCa: Contrastive Captioners are Image-Text Foundation Models `Google`
	> Train a single foundational model using both contrastive loss and auto-regressive loss. The paper's objective is to build a model with both alignment (contrastive) and generative (auto-regressive) capabilities
* PARTICLE: Part Discovery and Contrastive Learning for Fine-grained Recognition 
	> Part-based contrastive learning in an iterative fashion using pre-trained models as a starting point. I don't like the iterative natural of this approach, and I doubt its usefulness beyond natural images.
* Explaining grokking through circuit efficiency `Google` `DeepMind`
	> A neural network comprises multiple circuits that either generalize or memorize. Those memorizing circuits are easier to learn with a small training dataset. Yet, as the dataset size increases, they become less efficient, i.e., needs further updates to memorize newly added points. In contrast, generalizing circuits can easily support new training points without needing any updates. Grokking happens when a network switches from  a memorizing circuit to a generalizing circuit. Since generalizing circuit are harder to learn, it takes long training time (many iterations) for the network to learn the generalize circuit and disable/switch-off (regularize) the memorizing circuits.
* Demystifying CLIP Data `Meta`
	> How to curate data for CLIP 
* Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency
	> Use unpaired data -- beside paired data -- to train generative model using a cyclic loss.
* Progress measures for grokking via mechanistic interpretability
	> Show grokking behavior using transformers on modular-addition task; propose progress metric to monitor the learning progress of neural networks. These progress metrics show the network's transition between three phases: memorization phase, circuit formation, and cleanup. Regularization is important for cleanup and generalization.
* LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models
	> Evaluation benchmark for LVLM using image-classification, VQ task; online arena; zero-shot evaluations.
* Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning
	> A variant of Fortuitous forgetting but using a Meta-learning perspective.
* Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization `UMD` `Nice`
	> A Second order optimizer that avoid computing matrix inverse. I am worried AdamW optimizer evaluations are omitted. Code not released yet!
* MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning
	> Add task-specific token for MiniGPT to make task less ambiguous and boost MiniGPT performance.

# Generic
* Dataset Distillation for Medical Dataset Sharing `AAAI` `Workshop` `DD`
* 	Koala: A Dialogue Model for Academic Research `Berkeley` `LLMs`
*  Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes `ACL` `LLM`
	> fine-tune LM using labels datasets and (unlabeled) rationals from LLMs 
* MedViT: A Robust Vision Transformer for Generalized Medical Image Classification `Med` `Computers in Biology and Medicine`
	> Pooling for global attention + DWConv for relative-pos embedding with FFN + PONO augmentation for robustness + MHCA to encode high-fequencey features!. Eval on MedMNIST
* Transformer-based deep neural network for breast cancer classification on digital breast tomosynthesis `RSNA`
	> Apply TimeSformer on forzen backbone (spatial+temporal attention).
* Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles `Meta`
	> Efficient Hierarchical Vision Transformer
* Internet Explorer: Targeted Representation Learning on the Open Web `ICML`
	> 	Draw concepts from WordNet, use GPT to add descriptor to concept; perform text-to-image search; compute reward for returned image, use reward to adjust distribution over WordNet concepts; iterate again.
* Semi-Supervised and Long-Tailed Object Detection with CascadeMatch `IJCV`
	> Semi-supervised object detection. Use detection stages (heads) as an ensemble; use per-class adaptive threshold.
* VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammograph
	> New public dataset from Vietnam with 5000 exams (20K images); with both image-level and bounding box annotations.
* A Competition, Benchmark, Code, and Data for Using Artificial Intelligence to Detect Lesions in Digital Breast Tomosynthesis
	> Detector evaluations on DBT. NYU achieves best performance but uses extra internal data. They use recall@FP=1,2,3,4 as a metric. NYU is using EfficientDet, while ZeDuS is using RetinaNet. 
* Visual attention network `Computational Visual Media`
	> Lightweight alternative to PVT; uses depth-wise dilated conv layers

	
# CVPR
* MVImgNet: A Large-scale Dataset of Multi-view Images `Dataset`
* Simulated Annealing in Early Layers Leads to Better Generalization `KE` `LLF` `Mila`
* Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention
	> Efficient Local attention at early layers and regular global attention at late layers
* Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models `Diffusion` `UMD`
* IMAGEBIND: One Embedding Space To Bind Them All `Meta`
	> A single embedding for image, text, depth, audio, Inertial Measurement Unit (IMU)
* GeneCIS: A Benchmark for General Conditional Image Similarity `Meta`
	> Collect a new dataset (benchmark) for conditional image similarity evaluations. Use combiner (I+T) to adapt the retrieval task according to a text conidition T. 
* Revealing the Dark Secrets of Masked Image Modeling 
	> Why MIM fine-tunes better compared to supervised and self-supervised (MoCov2) models.
* Teaching Matters: Investigating the Role of Supervision in Vision Transformers `UMD` `Nice`
	> Deep analysis on the impact of different training techniques (fully supervised vs. contrastive vs. reconstruction)
* SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer `MIT`
	> Prune Window Attention on Swin-T for high-resolution inputs.
* Hyperbolic Contrastive Learning for Visual Representations beyond Objects `UMD` `Nice`
	> Add hyperbolic term to contrastive learning to enforce object-scene graph/relationship in the learned representation.
* Semi-DETR: Semi-Supervised Object Detection with Detection Transformers `WR-JRC` `Semi-supervised`
	> one-to-many mapping from strong augmentation to weak-augmentation. Propose a workaround for consistency loss in DETRs. Mine more pseduo bbox for consistency loss term.
* Revisiting Class Imbalance for End-to-end Semi-Supervised Object Detection `CVPRW` `WR-JRC`
	> A bunch of tricks for Semi-supervised Object detection.
* InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions `Nice`
	> PVT 1st author; Foundational model with deformable conv operators. Seems superior on small scale only!
* DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection 
	> Add class token and foreground tokens to make weakly-supervised cross-domain object detection possible.
* DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training `Nice`
	> Memory efficient implementation of contrastive loss (code released)!

# ICCV
* DETR Doesn't Need Multi-Scale or Locality Design `DETR` `Microsoft` `Nice`
	>  (1) Introduce bbox bias (similar to rel-pos bias) to help DETR focus on relevant parts of the images
	(2) MIM for pre-training, (3) tweak bbox regression loss to detect high quality small bboxes.
* FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization `Apple`
	> Focus on reducinig latency during inference using reparameterization. Replace expensive operations (e.g., self-attention) with conv operations. Replace dense KxK with linear train-time overparameterization.
* Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement `Apple`
	> Feed augmentated samples to superior ensemble model; save the augmenttaion and sparse Top-K probabilities. Use saved samples to train future architecture.
* Sigmoid Loss for Language Image Pre-Training `Nice` `Google`
	> Replace softmax in CLIP with Sigmoid. This eliminates the need for two all-gather calls and reduces the memory required by large batch-size, i.e., avoids the NxN similarity matrix.
* SparseDet: Improving Sparsely Annotated Object Detection with Pseudo-positive Mining `UMD`
	> Use self-supervised consistency loss to tackle Sparsely Annotated Object Detection problem. I doubt the proposed solution supports DETR-style based detector. The paper proposal seems to depend on a clear groundtruth-proposal alignment which is not the case for DETR-style detecors -- these depend on bipartite matching.
* DreamTeacher: Pretraining Image Backbones with Deep Generative Models `Nvidia`
	> Use Generative models (e.g., diffusion model) to pre-train CNN backbones (e.g., ConvNext). 
* Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning `META`
	> extend MAE for remote sensing data; Add round Sample Distance Positional Encoding to patches; decode/reconstruct image at multiple scales (l2 loss on small scale, l1 loss on high scale).
* Scale-Aware Modulation Meet Transformer
	> Use modulated conv operations in early layers to reduce computational complexity.
* What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations
	> Action recognition evaluation across different domains (domain/scenario shift).
	

# ICLR
* Avoiding spurious correlations via logit correction
	> Train two models: the first model learns the joint-probability of label and attribute P(y,a), while the second model leverage the first model to reduce the impact of spurious correlations. The paper also uses Mixup augmentation which do most of the heavy lifting (Tab. 3)
* Conditional Positional Encodings for Vision Transformers `ViT`
	> relative positional embedding with latent absolute positional information (zero-padding)
* On the duality between contrastive and non-contrastive self-supervised learning
	> sample contrastive vs. dimension contrastive. Revise assumptions about batch-size and embedding dimension
* Visual classification via description from large language models `Nice`
	> Extract visual description \phi from LLMs; use \phi as text for CLIP models to compute similarity
* Learning Object-Language Alignments for Open-Vocabulary Object Detection
	> Train a detector using both bbox annotations and image-text pairs.
	
	
# WACV
* Accelerating Self-Supervised Learning via Efficient Training Strategies `Nice`
	> Fixed 1-cycle Learning Rate Schedule (warmup for lr + inverse warmup for momentum) + Progress learning in terms of image-size and augmentation. Use dummy forward to find hard/useful pairs for training.
