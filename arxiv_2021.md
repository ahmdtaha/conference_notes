* Learning Transferable Visual Models From Natural Language Supervision **#openai2021**
* LambdaNetworks: Modeling long-range Interactions without Attention **#openreview2021** -- Seems like a good idea, poorly written.
* Re-labeling ImageNet:from Single to Multi-Labels, from Global to Localized Labels **#arXiv2021** -- `PI_Reading_Grp`
* Bottleneck Transformers for Visual Recognition **#arXiv2021** -- `PI_Reading_Grp` Technicalities
* Towards General Purpose Vision Systems **#arXiv2021** -- `PI_arXiv_Grp` Technicalities
* Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision **#arXiv2021** -- `PI_arXiv_Grp` Technicalities
* MLP-Mixer: An all-MLP Architecture for Vision **#arXiv2021** `Arch_Design`
* FNet: Mixing Tokens with Fourier Transforms **#arXiv2021** `Arch_Design`
* Learning Open-World Object Proposals without Learning to Classify **#arXiv2021** `Detection`
* nnFormer: Interleaved Transformer for Volumetric Segmentation **#arXiv2021** `Detection`
* Early Convolutions Help Transformers See Better **#arXiv2021** `Transformer`
* Escaping the Big Data Paradigm with Compact Transformers **#arXiv2021** `Transformer`
* Efficient Training of Visual Transformers with Small-Size Datasets **#arXiv2021** `Transformer`
* VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning **#arXiv2021** `SSL`
* Swin transformer: Hierarchical vision transformer using shifted windows **#arXiv2021** `Transformer`
* ResNet strikes back: An improved training procedure in timm **#arXiv2021** 
* Do Self-Supervised and Supervised Methods Learn Similar Visual Representations? **#arXiv2021** 
* Self-supervised pretraining of visual features in the wild `FAIR`
* Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers `Google/DeepMind`
* ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases `FAIR` `Transformer`
* Open-Set Recognition: A Good Closed-Set Classifier is All You Need
* Extrapolating from a Single Image to a Thousand Classes using Distillation
* Are Large-scale Datasets Necessary for Self-Supervised Pre-training? `META` `arXiv`
* Zero-Shot Text-to-Image Generation `arXiv` `dall-e` `OpenAI` `dVAE` `Transformers`
* MPViT : Multi-Path Vision Transformer for Dense Prediction `arXiv` `Transformer`
* Masked Feature Prediction for Self-Supervised Visual Pre-Training `Video` `SSL` `Meta`
* DeepViT: Towards Deeper Vision Transformer `ViT`
	> Nice analysis
* SCD: A Stacked Carton Dataset for Detection and Segmentation
	> 16,136 (8401 online + 7735 collected) with 250,000 instance masks (semantic segmentation).
* Deep learning through the lens of example difficulty `Google` `Nice`
	> the prediction depth