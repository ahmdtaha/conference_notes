# Generic

* LASER: Attention with Exponential Transformation `Google` `UAustinTexas`
	> Tweak self-attention so larger gradient get propagated through the model. A simple proposal that works on top of both vanilla and flash attention.

* Triaging mammography with artificial intelligence: an implementation study `Breast Cancer Research and Treatment` `US`
	> Use triaging software to speed-up additional images and diagnostic procedure through a prioritized workflow stream.
	
* Welcome to the Era of Experience `Richard S. Sutton` `Alan` `Turning` `DeepMind` `CA` `GB` `UK`
	> Going beyond Human-supervised AI, into experience-supervsied AI. In the experience era, AI will recieve inputs from many modalities and sensors -- unlike LLMs. The AI models/agents will be supervised using streams of experience -- not input,output pairs. These streams will span long durations of experience, not just short snippets of interaction. These models/agents reward/loss function will come from the environment, not a human defined ground-truth output. These models/agents will plan and reason about the experience, in some latent space and not solely in human terms (language tokens).
	
# ICLR 

* Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint `US` `WR`
	> Propose Expected Calibration Error (ECE) as an evlaution metric for DNNs trained on stochastic processes (e.g., forest fire, stock market).

* MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models `US` 
	> A new dataset that highlight limitations of medical multi-modal LLMs (MLLMs). Surprisingly, commerical generic MLLM beats medical models on this dataset. The dataset highlights medical MLLMs limitations when presented with nuanced questions -- outside the general medical knowledge domain.