# Generic

* LASER: Attention with Exponential Transformation `Google` `UAustinTexas`
	> Tweak self-attention so larger gradient get propagated through the model. A simple proposal that works on top of both vanilla and flash attention.