# Generic

* LASER: Attention with Exponential Transformation `Google` `UAustinTexas`
	> Tweak self-attention so larger gradient get propagated through the model. A simple proposal that works on top of both vanilla and flash attention.

* Triaging mammography with artificial intelligence: an implementation study `Breast Cancer Research and Treatment` `US`
	> Use triaging software to speed-up additional images and diagnostic procedure through a prioritized workflow stream.
	
* Welcome to the Era of Experience `Richard S. Sutton` `Alan` `Turning` `DeepMind` `CA` `GB` `UK`
	> Going beyond Human-supervised AI, into experience-supervsied AI. In the experience era, AI will recieve inputs from many modalities and sensors -- unlike LLMs. The AI models/agents will be supervised using streams of experience -- not input,output pairs. These streams will span long durations of experience, not just short snippets of interaction. These models/agents reward/loss function will come from the environment, not a human defined ground-truth output. These models/agents will plan and reason about the experience, in some latent space and not solely in human terms (language tokens).
	
* MedVAE: Eﬃcient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders `MIDL` `Oral` `US` `Stanford` `NL`
	> The paper propose an two-stage Variational autoencoder to reduce the computational cost for training on high-resolution medical images. During the first stage, the VAE is trained to minimize the re-construction loss (using a perceptual loss [54], a patch-based adversarial objective [24], and a domain-specific embedding consistency loss). During the second stage, the VAE embeddings is aligned with embeddings from BiomedCLIP to refine the latent space such that clinically-relevant features are preserved across modalities. The paper delivers a good set of quantiaitive and qualitative evaluation, as well as ablation studies. Yet, the training objective is not clearly presented. The paper mentions `a domain-specific embedding consistency loss` but without any implementation details. In a github PR, the authors states that "in addition to the perceptual loss and patch-based adversarial objective, we also implemented an L1 loss" which " [They] don't explicitly mention it in the paper"! It is not easy to replicate their work.
	
* Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity `US` `METR`
	> AI tools can slow experienced developers on complex projects because
	1. Developers have high expectation from AI tool. So, they overuse these tools, then regret/revert back when presented with reality.
	2. When developer are highly familiar with the task context and needed background, they excel on their own, i.e., AI tools slow them due with extra steps (e.g., writing a prompt, revising the generated code). AI tools indeed helps developers with low background knowledge about a task.
	3. AI tools struggles with large and complex repositories. These tools can edit unrelated  (random) parts of the repo.
	4. developers accept <44% of the generations which indicates low AI reliability. This should increase with time -- hopefully.
	5. AI tools lacks "tacit knowledge that experienced developers rely on when completing their work". [AI tools] don't know how to take care of a weird case for backwards compatibility.

* Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift `ICML` `CA`
	> The paper takes the problem if population shift between training and testing.
	The paper identify four fundamental sources of problems: (1) spurious correlation: False correlation during training (e.g., scanner type/clinical markers); (2) attribute Imbalance: demographic (race) imbalance between training and testing; (3) Class imbalance: positive cases are less common than negative (disease) cases; (4) Attribute Generalization: different attributes (weather conditions) during testing not seen during training. To tackle population shift, the paper leverage ensembling with explicit diversification tricks. E.g., the paper leverages an explicit loss term to minimize similarlity between different ensemble-classifiers. Also, the paper trains different ensemble-models on different data subsets.
	
* CAD-Recode: Reverse Engineering CAD Code from Point Clouds `ICCV` `SNT` `LU`
	> The paper tackle the problem to re-constructing 3D CAD models from a point-cloud. To solve this problem, the paper cast the 3D CAD re-construction problem into writing/auto-regressing python code. The python code is generated by LLM that processes point-cloud -- in the embedding space -- as input. To fine-tung the LLM, the paper generate a training dataset of CadQuery Python code.
	
* CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers `ICCV` `SNT` `LU`
	> Developing a LangChain/Graph replica for computer aided design. VLLM (e.g., GPT-4o) are used a planners that generate both plans and code. Besides the context (CAD-model), the VLLM has access to a python interperter and a bunch of CAD tools to execute the code generated by the planner-VLLM. 
	
* Vulnerability-Aware Spatio-Temporal Learning for Generalizable Deepfake Video Detection `ICCV` `SNT` `LU`
	> The paper uses a modified TimeSfomer to classify whether a video is fake or not. Besides the standard binary classification loss (fake vs. real), the paper proposes two supervision signals to boost generalization and robustness. The proposed signals: (1) a spatial, and (2) temporal fine-grained teach the model to localize the fake regions (spatial + temporal) in a video. 
	
* CXR-LLaVA: a multimodal large language model for interpreting chest X-ray images `Imaging Informatics and Artificial Intelligence` `KR`
	> Train VLM model on public chest-xray data. The training pipeline has two stages: Stage-1 focus on training a strong vision encoder for medical images. Then, stage 2 fine-tunes the LLMs (+ VLM projection layer) on generating rads reports plus answering medical questions. To train the VLM (CXR-LLaVA) on medcial questions, the authors collected a synthetic dataset with multi-turn question-answer dialogues using GPT-4.
	
* Canaries in the Coal Mine? Six Facts about the Recent Employment Effects of Artificial Intelligence `Stanford` `US`
	> The paper shares Six Facts about the Recent Employment Effects of Artificial Intelligence.
	1. substantial declines in employment for early-career workers (ages 22-25) in occupations most exposed to AI, such as software developers and customer service representatives. 
	2. 22- to 25-year-olds declining employment are driving weak overall employment growth. 
	3. Entry-level employment has declined in applications of AI that _automate_ work (e.g., software engineering), but not those that most _augment_ it (e.g., nursing aides).
	4. The observed employment trends are not driven by differential shocks to firms. They are driven by AI replacing humans.
	5. little difference in annual salary trends by age or exposure quintile, suggesting possible wage stickiness. If so, AI may have larger effects on employment than on wages, at least initially. 
	6. The  results are not driven solely by computer occupations or by occupations susceptible to remote work and outsourcing. They also hold for both occupations with a high share of college
graduates and ones with a low college share. 
	
* DINOv3 `Meta` `Inria`
	> The paper explains how to scale data and model for DINO. Furthermore, the paper propose Gram achoring to prevent dense feature degradation. Key observations:
	1. SSL models are more robust to distribution shifts with strong global and local features.
	2. Compared to text-language alignment SSL, DINO requires no metadata, no human intervention, suitable for lifelong learning and growing volume of web data.
	3. DINOv3 shows how to scale model and data size without representation degradation.
	4. DINOv3 propose Gram anchoring to improve dense features.
	5. DINOv3 uses modern position embeddings (axial RoPE) and RoPE-box jittering as a regularization.
	6. DINOv3 uses constant learning rate (hyperparameter schedule) because it is impractical to know the optimization horizon a priori.
	7. 
	
# arXiv

* Scaling Language-Free Visual Representation Learning `FAIR` `NYU` `US`
	> Vision contrastive SSL pre-trained models (e.g., DINO) are competitive/superior to vision-language models (e.g., CLIP) at the right data and model scales. Masked Autoencoder methods (e.g., MAE) is competitive as well, but slightly inferior to contrastive SSL models. Web-scale data -- with text content -- boost SSL models in OCR and chart performance tasks. At the right data and model scale, the representation learned by vision only models becomes similar (increasingly aligned) with the representation learned by vision-language models.
	
* Towards Conversational AI for Disease Management `Google` `DeepMind`
	> This google paper optimize on a previous google paper that proposed AIME, Articulate Medical Intelligence Explorer. AIME is originally proposed for conversational diagnostic: just talk with a patient to take history and ask question and make a disease diagnosis. Now, this paper extends AIME beyond disease diagnosis to perform disease management: disease progression, therapeutic response, and safe medication prescription. To do so, the paper tackles novel challenges on top of the challenges tackled in the original AIME paper. The original paper tackled challenges in terms of data curation, designing a simulation environment for evaluation and a building an evaluation rubic. This new paper tackled new challenges in terms of medication reasoning, grounding its reasoning in authoritative clinical knowledge. To tackle these challenges, the paper employs the following ideas
	1. Employs an agentic AI system with two LLMs : one for conversation with patient (System 1) and another for reasoning (System 2).
	2. Use in-context retreival to ground reason into authoritative knowledge
	3. Use Coarse Retrieval to avoid encoding all authoritative knowledge (10.5 millions of tokens) which exceeds Gemini’s two million context window
	4. Leverage a chain-of-reasoning as follows:  Plan Response, Generate Response, and Refine Response.
	5. Leverage decoding constraints for structured output generation to make sure that output follows a json schema. The json schema is important for interpretability and traceability or to serve an interface between two components

* AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges `US` `GR` `Cornell`
	> The paper formalizes Agents and Agentic terms in literature. 
	1. AI Agents: These are LLMs with capabilities for external tool use, function calling, and sequential reasoning. These agents perform goal-directed tasks, operate within explicitly defined scopes.

		a.  Agents' challenges: hallucination, prompt brittleness, limited planning ability, and lack of causal understanding.
		b. Agent's three foundational characteristics: autonomy, task-specificity, reactivity-with-adaptation. 
	2. Agentic AI: These are multi-agent systems in which specialized agents collaboratively decompose goals, communicate, and coordinate toward shared objectives.
		a. Agentic's (Higher) challenges: inter-agent misalignment, error propagation, unpredictability of emergent behavior, explainability deficits, and adversarial vulnerabilities.
		
	3. The paper distinguishes between AI Agents and Generative AI (LLMs). Agents = LLMs + additional infrastructure such as tool-calling APIs, reasoning chains to bridge the gap between passive response generation and active task completion. LLMs => AI agents === from content creation to autonomous
utility.
	
	4. While AI agents involves percetion reasoning and actions, Agentic AI involves collaboration between multiple agents, advanced reasoning, presistent memory, and orchestration (meta/CEO-agent).

	5. Examples for AI Agents: customer support, Internal enterprise search on company knowledge, Email filtering and triaging, Personalized content recommendation, autonomous scheduling (Email + Calendar integration).

* MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation  Tasks `SnT` `LU`
	> Multi-modal multi-task pre-training on earth-observation data.
	
	
# ICLR 

* Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint `US` `WR`
	> Propose Expected Calibration Error (ECE) as an evlaution metric for DNNs trained on stochastic processes (e.g., forest fire, stock market).

* MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models `US` 
	> A new dataset that highlight limitations of medical multi-modal LLMs (MLLMs). Surprisingly, commerical generic MLLM beats medical models on this dataset. The dataset highlights medical MLLMs limitations when presented with nuanced questions -- outside the general medical knowledge domain.
	
* PhiNets: Brain-inspired Non-contrastive Learning Based on Temporal Prediction Hypothesis `JP` `Tokyo`
	> The paper propose an architecture to simulate predictive coding theory for self-supervised learning. The paper builds on top of SimSiam, introduce new comparator/predictor component to predict future signal from past experience. The propose architecture demonstrate strong online and continual performance. Cerebral cortex is the outermost layer of a brain, while hippocampus is an inner layer of a brain. Hippocampus is known for "fast" learning and forming memories, while neocortex (inside Cerebral cortex) is known for slow learning and generalizable representation. The Complementary Learning Systems (CLS) theory explains how human brians depends on two complementary learning systems: (1) fast -- more rudimentary within hippocampus, (2) slow -- more "evolutionarily" within neocortex. 
	
# CVPR

* Improving Accuracy and Calibration via Differentiated Deep Mutual Learning `CN`
	> The paper propose technical tricks to boost (reduce) ensemble calibration error, without compromising performance (accuracy).
	
* DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment `Meta` `FR` `POSTECH`
	> The paper shows how to use a frozen DINOv2 model for building a VLM.
		1. Freeze DINOv2 backbone, but add a couple of trainable transformer blocks on top of the backbone.
		2. Use contrastive loss to align text and image feature representation.
		3. For the image feature, the paper propose to contact the CLS_Token with the average-pooled patch-level tokens.
		4. The paper emphasize the importance of high quality balanced image-text pairs for training this VLM. The paper curates both balaned text caption and images distributions.