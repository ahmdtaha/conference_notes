# Generic

* LASER: Attention with Exponential Transformation `Google` `UAustinTexas`
	> Tweak self-attention so larger gradient get propagated through the model. A simple proposal that works on top of both vanilla and flash attention.

* Triaging mammography with artificial intelligence: an implementation study `Breast Cancer Research and Treatment` `US`
	> Use triaging software to speed-up additional images and diagnostic procedure through a prioritized workflow stream.
	
* Welcome to the Era of Experience `Richard S. Sutton` `Alan` `Turning` `DeepMind` `CA` `GB` `UK`
	> Going beyond Human-supervised AI, into experience-supervsied AI. In the experience era, AI will recieve inputs from many modalities and sensors -- unlike LLMs. The AI models/agents will be supervised using streams of experience -- not input,output pairs. These streams will span long durations of experience, not just short snippets of interaction. These models/agents reward/loss function will come from the environment, not a human defined ground-truth output. These models/agents will plan and reason about the experience, in some latent space and not solely in human terms (language tokens).
	
* MedVAE: Eﬃcient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders `MIDL` `Oral` `US` `Stanford` `NL`
	> The paper propose an two-stage Variational autoencoder to reduce the computational cost for training on high-resolution medical images. During the first stage, the VAE is trained to minimize the re-construction loss (using a perceptual loss [54], a patch-based adversarial objective [24], and a domain-specific embedding consistency loss). During the second stage, the VAE embeddings is aligned with embeddings from BiomedCLIP to refine the latent space such that clinically-relevant features are preserved across modalities. The paper delivers a good set of quantiaitive and qualitative evaluation, as well as ablation studies. Yet, the training objective is not clearly presented. The paper mentions `a domain-specific embedding consistency loss` but without any implementation details. In a github PR, the authors states that "in addition to the perceptual loss and patch-based adversarial objective, we also implemented an L1 loss" which " [They] don't explicitly mention it in the paper"! It is not easy to replicate their work.
	
* Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity `US` `METR`
	> AI tools can slow experienced developers on complex projects because
	1. Developers have high expectation from AI tool. So, they overuse these tools, then regret/revert back when presented with reality.
	2. When developer are highly familiar with the task context and needed background, they excel on their own, i.e., AI tools slow them due with extra steps (e.g., writing a prompt, revising the generated code). AI tools indeed helps developers with low background knowledge about a task.
	3. AI tools struggles with large and complex repositories. These tools can edit unrelated  (random) parts of the repo.
	4. developers accept <44% of the generations which indicates low AI reliability. This should increase with time -- hopefully.
	5. AI tools lacks "tacit knowledge that experienced developers rely on when completing their work". [AI tools] don't know how to take care of a weird case for backwards compatibility.

* Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift `ICML` `CA`
	> The paper takes the problem if population shift between training and testing.
	The paper identify four fundamental sources of problems: (1) spurious correlation: False correlation during training (e.g., scanner type/clinical markers); (2) attribute Imbalance: demographic (race) imbalance between training and testing; (3) Class imbalance: positive cases are less common than negative (disease) cases; (4) Attribute Generalization: different attributes (weather conditions) during testing not seen during training. To tackle population shift, the paper leverage ensembling with explicit diversification tricks. E.g., the paper leverages an explicit loss term to minimize similarlity between different ensemble-classifiers. Also, the paper trains different ensemble-models on different data subsets.
	
* CAD-Recode: Reverse Engineering CAD Code from Point Clouds `ICCV` `SNT` `LU`
	> The paper tackle the problem to re-constructing 3D CAD models from a point-cloud. To solve this problem, the paper cast the 3D CAD re-construction problem into writing/auto-regressing python code. The python code is generated by LLM that processes point-cloud -- in the embedding space -- as input. To fine-tung the LLM, the paper generate a training dataset of CadQuery Python code.
	
* CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers `ICCV` `SNT` `LU`
	> Developing a LangChain/Graph replica for computer aided design. VLLM (e.g., GPT-4o) are used a planners that generate both plans and code. Besides the context (CAD-model), the VLLM has access to a python interperter and a bunch of CAD tools to execute the code generated by the planner-VLLM. 
	
* Vulnerability-Aware Spatio-Temporal Learning for Generalizable Deepfake Video Detection `ICCV` `SNT` `LU`
	> The paper uses a modified TimeSfomer to classify whether a video is fake or not. Besides the standard binary classification loss (fake vs. real), the paper proposes two supervision signals to boost generalization and robustness. The proposed signals: (1) a spatial, and (2) temporal fine-grained teach the model to localize the fake regions (spatial + temporal) in a video. 

* RetailAction: Dataset for Multi-View Spatio-Temporal Localization of Human-Object Interactions in Retail `ICCV` `Workshop` `USA`
	> Motivation: There is a lack of a large-scale human-action interaction dataset in real-world retail.
	
	> Method: The paper creates a multi-view spatio-temporal dataset for human-object interaction in retail. The dataset contains pairs of clips from two cameras along with (1) precise interaction points, (2) temporal range, and (3) action categories. Besides the dataset, the paper describes a baseline for action spatio-temporal localization. The baseline model leverages a shared spatio-temporal backbone (e.g., CNN or ViT) to extract spatio-temporal features. Following the backbone, a shared transformer block processes the extracted features from each clip independently before concatenating the transformed features. Finally, the two-clip features are processed by a DETR decoder to generate final outputs. The model's final outputs are action category (C classes -- take, put, touch), a temporal range (2-dimensional feature), and spatial coordinates (2D feature) for the human-object interaction. One interesting idea presented in the paper is that all video clips are trimmed to a fixed length of 32 frames. To achieve this, the authors propose a dynamic sampling approach that assigns higher scores to frames containing human-object interactions and lower scores to frames that are irrelevant (e.g., a shopper simply walking).
	
* CXR-LLaVA: a multimodal large language model for interpreting chest X-ray images `Imaging Informatics and Artificial Intelligence` `KR`
	> Train VLM model on public chest-xray data. The training pipeline has two stages: Stage-1 focus on training a strong vision encoder for medical images. Then, stage 2 fine-tunes the LLMs (+ VLM projection layer) on generating rads reports plus answering medical questions. To train the VLM (CXR-LLaVA) on medcial questions, the authors collected a synthetic dataset with multi-turn question-answer dialogues using GPT-4.
	
* Canaries in the Coal Mine? Six Facts about the Recent Employment Effects of Artificial Intelligence `Stanford` `US`
	> The paper shares Six Facts about the Recent Employment Effects of Artificial Intelligence.
	1. substantial declines in employment for early-career workers (ages 22-25) in occupations most exposed to AI, such as software developers and customer service representatives. 
	2. 22- to 25-year-olds declining employment are driving weak overall employment growth. 
	3. Entry-level employment has declined in applications of AI that _automate_ work (e.g., software engineering), but not those that most _augment_ it (e.g., nursing aides).
	4. The observed employment trends are not driven by differential shocks to firms. They are driven by AI replacing humans.
	5. little difference in annual salary trends by age or exposure quintile, suggesting possible wage stickiness. If so, AI may have larger effects on employment than on wages, at least initially. 
	6. The  results are not driven solely by computer occupations or by occupations susceptible to remote work and outsourcing. They also hold for both occupations with a high share of college
graduates and ones with a low college share. 
	
* DINOv3 `Meta` `Inria`
	> The paper explains how to scale data and model for DINO. Furthermore, the paper propose Gram achoring to prevent dense feature degradation. Key observations:
	1. SSL models are more robust to distribution shifts with strong global and local features.
	2. Compared to text-language alignment SSL, DINO requires no metadata, no human intervention, suitable for lifelong learning and growing volume of web data.
	3. DINOv3 shows how to scale model and data size without representation degradation.
	4. DINOv3 propose Gram anchoring to improve dense features.
	5. DINOv3 uses modern position embeddings (axial RoPE) and RoPE-box jittering as a regularization.
	6. DINOv3 uses constant learning rate (hyperparameter schedule) because it is impractical to know the optimization horizon a priori.

* Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images `MICCAI` `WR` `JRC` `DE` `academia`
	> Yet another paper that claims that VLMs are blind. VLMs are biased by the world "standard" prior; they tend to overload the image input. So, VLMs struggle with medical images that deviate from typical anatomical patterns due to in-situs inversus, post-surgical alterations, or tumor displacement.
	
* Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models `ICML` `US` `Academia`
	> The paper generates a low-dimensional embedding for 3D medical volumes using random projections. Concretely, A 3D medical volume is processed using a pre-trained 2D foundational model, the generated features are pooled then projected into a low dimensionl space (K << d) to reduce computational complexity. Once generated, these embeddings can be used for classification and regression tasks. The paper used Pearson's r^2 metric for evaluating regression tasks because its objective is low-dimensional embedding generation, rather than a particular task-specific regression or classification model.


* CLIP-KO: Knocking Out Typographic Attacks in Contrastive Language-Image Pre-Training `Github` `zer0int`
	> The paper fine-tunes a CLIP model to mitigate typographic attacks. In these attacks, the vision encoder pays more attention to the text in an image than to the image content itself. The paper tackles this problem with many complementary approaches without an ablation study. So, it is hard to tell which approach contributes the most. Yet, here is a list of the employed approaches
	1. Attention Head Dropout
	2. Geometric Parametrization (GmP): This parameterization employs polar (radial and angular)  instead of Cartesian representation to improve optimization. GmP claims that polar representations are stabilier during training. In contrast, standard representation is unstable as small changes in the magnitude lead to large shifts in the angular direction of the activation boundary.
	3. Key Projection Orthogonality Loss: decorrelate attention heads by imposing an orthogonal  constraint (loss) on the key Projection matrix from heads.
	4. A better fine-tuning dataset: The paper uses COCO-SPRIGHT-40k which has 40K images with spatially right LLM-generated captions
	5. Adversarial dataset: This dataset contains hard negative pairs that maximize CLIP-bias towards typographic attacks.

* Revisiting Continuity of Image Tokens for Cross-Domain Few-shot Learning `ICML` `CN` `Huazhong` `WR` `JRC`
	> The paper tackles the problem of cross-domain few-shot learning (CDFSL). The paper observes that by shuffling image patches, we break image continuity, decrease a model's performance on the source domain, but reduce the distribution gap between the source and target domains. The authors' intuition is that local patterns transfer easily across domains, while large patterns often struggle to transfer to target domains. The paper proposes creating image continuity by training on shuffled image patches. Yet, to reduce the task (learning local patterns) complexity and maximize performance boost, the paper propose a complicated training schedule that involves: (1) Warm-Up Spatial-Domain Disruption to reduce task complexity, (2) Balanced Frequency-Domain Disruption to maximize continuity disrupt even in images with a dominant repeating patterns (e.g., water or sky or black background in medical images).
	
* Influence of Mammography Acquisition Parameters on AI and Radiologist Interpretive Performance  `WR` `JRC` `RadAI`
	> The paper studies the influence of seven image acquisition parameter on AI and Rads performance. The paper evaluates the impact of seven parameters: Scanner type (a.k.a. machine version), Kilovoltage Peak (kVp), Exposure Delivered, Relative x-ray exposure, paddle size, compression force, and breast thickness. The AI and Rads performance are evaluated using sensitivity and precision metrics. Indeed, the paper found that the studied parameter impact AI and Rads performance. It was easier to observe significant impact for parameters on specificity, compared to sensitivity, given the large number of negative samples, compared to positive samples. For instance, the paper observed a significant negtaive impact for high exposure on AI, but no impact on Rads performance. Furthermore, new scanner boosted both Rads and AI performance significantly. The paper didn't explain the reasons behind these impact but proposed hypothesis. E.g., AI models uses the default windowing levels while Rads can dynamically adjust the display window. This might explain why overexposure would have high impact on AI, but no impact of Rads.

* Accurate predictions on small data with a tabular foundation model `GE` `Academia` `PriorLabs` `Nature`
	> Motivation: TabPFN achieves competitive performance on tabular datasets, but it is time to scale this approach to (1) support larger datasets with more samples and features, (2) cope with missing values, (3) be robust to uninformative features.
	
	> Method: The paper scales TabPFN to support large datasets by incorporating mixed-precision, flash-attention, KV caching, and activation checkpointing. Besides computational complexity, the authors focused on structural causal models (SCM) and omitted Bayesian NN when sampling from the prior. It seems that this TabPFN uses a more sophisticated SCM prior to boost performance on tabular data, cope with missing values, and be robust to uninformative features. The paper also utilizes/proposes new augmentation techniques for tabular data, such as shuffling features, turning 2% into outliers, etc.
	
	> Results: The paper achieves superior performance on tabular datasets against a wide range of state-of-the-art approaches (e.g, Gradient boosted Decision trees). Furthermore, TabPFN achieves even superior performance when combined inside an ensemble of classical approaches (e.g., decision trees).
	
# arXiv

* Scaling Language-Free Visual Representation Learning `FAIR` `NYU` `US`
	> Vision contrastive SSL pre-trained models (e.g., DINO) are competitive/superior to vision-language models (e.g., CLIP) at the right data and model scales. Masked Autoencoder methods (e.g., MAE) is competitive as well, but slightly inferior to contrastive SSL models. Web-scale data -- with text content -- boost SSL models in OCR and chart performance tasks. At the right data and model scale, the representation learned by vision only models becomes similar (increasingly aligned) with the representation learned by vision-language models.
	
* Towards Conversational AI for Disease Management `Google` `DeepMind`
	> This google paper optimize on a previous google paper that proposed AIME, Articulate Medical Intelligence Explorer. AIME is originally proposed for conversational diagnostic: just talk with a patient to take history and ask question and make a disease diagnosis. Now, this paper extends AIME beyond disease diagnosis to perform disease management: disease progression, therapeutic response, and safe medication prescription. To do so, the paper tackles novel challenges on top of the challenges tackled in the original AIME paper. The original paper tackled challenges in terms of data curation, designing a simulation environment for evaluation and a building an evaluation rubic. This new paper tackled new challenges in terms of medication reasoning, grounding its reasoning in authoritative clinical knowledge. To tackle these challenges, the paper employs the following ideas
	1. Employs an agentic AI system with two LLMs : one for conversation with patient (System 1) and another for reasoning (System 2).
	2. Use in-context retreival to ground reason into authoritative knowledge
	3. Use Coarse Retrieval to avoid encoding all authoritative knowledge (10.5 millions of tokens) which exceeds Gemini’s two million context window
	4. Leverage a chain-of-reasoning as follows:  Plan Response, Generate Response, and Refine Response.
	5. Leverage decoding constraints for structured output generation to make sure that output follows a json schema. The json schema is important for interpretability and traceability or to serve an interface between two components

* AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges `US` `GR` `Cornell`
	> The paper formalizes Agents and Agentic terms in literature. 
	1. AI Agents: These are LLMs with capabilities for external tool use, function calling, and sequential reasoning. These agents perform goal-directed tasks, operate within explicitly defined scopes.

		a.  Agents' challenges: hallucination, prompt brittleness, limited planning ability, and lack of causal understanding.
		b. Agent's three foundational characteristics: autonomy, task-specificity, reactivity-with-adaptation. 
	2. Agentic AI: These are multi-agent systems in which specialized agents collaboratively decompose goals, communicate, and coordinate toward shared objectives.
		a. Agentic's (Higher) challenges: inter-agent misalignment, error propagation, unpredictability of emergent behavior, explainability deficits, and adversarial vulnerabilities.
		
	3. The paper distinguishes between AI Agents and Generative AI (LLMs). Agents = LLMs + additional infrastructure such as tool-calling APIs, reasoning chains to bridge the gap between passive response generation and active task completion. LLMs => AI agents === from content creation to autonomous
utility.
	
	4. While AI agents involves percetion reasoning and actions, Agentic AI involves collaboration between multiple agents, advanced reasoning, presistent memory, and orchestration (meta/CEO-agent).

	5. Examples for AI Agents: customer support, Internal enterprise search on company knowledge, Email filtering and triaging, Personalized content recommendation, autonomous scheduling (Email + Calendar integration).

* MultiMAE Meets Earth Observation: Pre-training Multi-modal Multi-task Masked Autoencoders for Earth Observation  Tasks `SnT` `LU`
	> Multi-modal multi-task pre-training on earth-observation data.
	
* Vision Transformers Don’t Need Trained Registers `US` `UC Berkeley`
	> The paper propose a remedy for attention sink token (patches) in ViT architecture. It has been found that some patch tokens have high norm and act as attention sink summarizing global context and discarding their own local information. A previous solution tackled this problem by including extra register tokens _during training_. This solution requires re-training ViT models which is an expensive process. Accordinglu, this paper proposes an alternative solution that requires no-training. The paper proposes an approach to automatically identify these attention sinks (inferior dimension) during an inference forward pass. Once identified, these attention-sinks can be moved (shift). Concretely, the inferior embedding dimension, suffering high norm, is shifted to untrained (inference-only) token(s). 
	
* Can Multimodal Large Language Models Truly Perform Multimodal In-Context Learning? `WACV` `Siemens` `UK` `US` `GE` `WR` `JRC`
	> The paper evaluates the impact of in-context learning (ICL) for mulit-modal LLMs (a.k.a VLMs). The paper finds that visual modality has minimal impact on the performance and text modality dominates information in ICL. The paper attributes the limited vale of visual modality -- within ICL demos -- to the cross-attention module where text attends to previous image only. This limits the information passed from previous images to the image-text query pairs. To boost the added value of visual modality, the paper propose MMICES: an ICL demo selection strategy. The key idea to find similar image-text pairs  using image retrieval -- & cosine similarity -- to the query image-text pair, then rank (sort) these image-text pairs using text retrieval (similarity).


* nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN  `GE` `PFNs`
	> The paper delivers a minimal implementation for TabPFN without all the bells and whistles. Thus, no handling for missing values, categorical values, regression tasks. nanoTabPFN disable ensembling and use a pre-computed prior. While it takes less than a minute to train nanoTabPFN, it actually competitive performance to TabPFNv2 which was pre-trained for two weeks!
	
* Next-Embedding Prediction Makes Strong Vision Learners `USA` `NYU` `Princeton`
	> Motivation: Vision pre-trainining is different from LLM pre-training: Vision pre-trainining are feature extractors while LLM pre-training are generative and predictive systems. is it possible to close this gap?
	
	> Method: The paper proposes a generative objective to train ViT models using causal masking and stop-gradient operation. The proposed method, Next-Embedding Predictive Autoregression (NEPA), pre-trains ViT to predict the next input-embedding in the feature embedding space and not in the pixel space.
	
* ARC Is a Vision Problem `MIT` `US`
	> Motivation: The Abstraction and Reasoning Corpus (ARC) benchmark is currently solved using LLMs (language based approaches), while this is supposedly a vision reasoning benchmark.
	
	> Method: The paper propose a vision model (ViT/UNet) to tackle this benchmark. The paper trains the model on prior data (seen tasks), then evaluates the model on new data (unseen tasks). The model takes images as input. The images are generated by mapping the ARC grids on a canvas. Using these images/canvases, the model use typical vision augmentation techniques (e.g., translation and scaling). The vision models are trained from scratch, without any internet data. Despite that, the vision model achieves competitive results compared to leading LLMs baseline.

* VL-JEPA: Joint Embedding Predictive Architecture for Vision-language `US` `Meta` `HK` `FAIR`
	> Motivation: Autoregressive VLMs are computational expensive during inference (decoding) due to their discrete token-based output. Accordingly, switching from one thought (semantic) to another is a slow process, i.e., the VLM has to finish the first sentence before switching to another sentence. In addition, the discrete token-based output makes it non-trivial to identify semnatic (context) switch. E.g., two output sentence with different words could basically mean the same thing!
	
	> Method: The paper proposes a non-generative VLM model that evaluates learned representations in a high-level (semnatic) embedding space, instead of the low level (pixel/token) representations. The proposed architecture, VL-JEPA, is inspired to JEPA where a hidden target y is predicted using an observable context x and hinting latent variable z. In VLM context, x is an image/video, z is a query, and y is the response (e.g., caption or VQA answer). VL-JEPA is trained over two-stages: (1) pre-training on vision-caption (no query) dataset, (2) then supervised fine-tuned on vision+QA dataset. VL-JEPA is susceptible to model (representation) collapse, e.g., if both the predictor and y-encoder generate constant feature. Accordingly, VL-JEPA training objective has two terms: (1) an alignment loss to pull paired vision-text representation together, (2) a uniformity regularization term that pushes embedding apart inside a mini-batch.

# ICLR 

* Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint `US` `WR`
	> Propose Expected Calibration Error (ECE) as an evlaution metric for DNNs trained on stochastic processes (e.g., forest fire, stock market).

* MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models `US` 
	> A new dataset that highlight limitations of medical multi-modal LLMs (MLLMs). Surprisingly, commerical generic MLLM beats medical models on this dataset. The dataset highlights medical MLLMs limitations when presented with nuanced questions -- outside the general medical knowledge domain.
	
* PhiNets: Brain-inspired Non-contrastive Learning Based on Temporal Prediction Hypothesis `JP` `Tokyo`
	> The paper propose an architecture to simulate predictive coding theory for self-supervised learning. The paper builds on top of SimSiam, introduce new comparator/predictor component to predict future signal from past experience. The propose architecture demonstrate strong online and continual performance. Cerebral cortex is the outermost layer of a brain, while hippocampus is an inner layer of a brain. Hippocampus is known for "fast" learning and forming memories, while neocortex (inside Cerebral cortex) is known for slow learning and generalizable representation. The Complementary Learning Systems (CLS) theory explains how human brians depends on two complementary learning systems: (1) fast -- more rudimentary within hippocampus, (2) slow -- more "evolutionarily" within neocortex. 
	
* Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models `CN`
	> Motivation: Multimodals LLM (MLLs) that process high resolution images achieve better performance, but are computationally expenseive
	
	> Method: The paper proposes an MLLM with two visual encoders: (1) a base ViT operating on lower resolution inputs, (2) a supplementary ConvNext operating on high resolution inputs. The ViT intermediate features are grafted with the high-resolution features from the ConvNext. Accordingly, the convnext injects fine-grained details into the ViT features. Only the final ViT features (tokens) are fed into the decoder LLM (through an adaptor). Accordingly, the decoder LLM always operates on a small number of visual tokens (from the low resolution ViT). 
	
# CVPR

* Improving Accuracy and Calibration via Differentiated Deep Mutual Learning `CN`
	> The paper propose technical tricks to boost (reduce) ensemble calibration error, without compromising performance (accuracy).
	
* DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment `Meta` `FR` `POSTECH`
	> The paper shows how to use a frozen DINOv2 model for building a VLM.
		1. Freeze DINOv2 backbone, but add a couple of trainable transformer blocks on top of the backbone.
		2. Use contrastive loss to align text and image feature representation.
		3. For the image feature, the paper propose to contact the CLS_Token with the average-pooled patch-level tokens.
		4. The paper emphasize the importance of high quality balanced image-text pairs for training this VLM. The paper curates both balaned text caption and images distributions.

* Words or Vision: Do Vision-Language Models Have Blind Faith in Text? `SG` `NUS`
	> The paper shows that VLM over-relies on text modality when answering questions even when the text is corrupted or irrelevant to the task. The paper suggest few methods to mitigate this behavior: (1) instruction tuning, where the prompt explicitly state to focus on image, (2) feeding image tokens per text tokens because VLMs assigns more attention/focus to initial tokens, (3) Using larger LLMs because they are less biased toward text modality, and finally (4) supervised fine-tuning (SFT) with both text-only and image-text samples. During SFT, the authors performed text augmentation such as using corrupted or irrelevant text to reduce VLM's dependence on text modality.
	
* Just Dance with π! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection `Toyota` `FR` `INRIA`
	> The paper's main premise is that RGB modality is insufficient for weakly supervised Video anomaly detection (WS-VAD); instead, fine-grained modalities such as pose, depth, masks, and optical flow are required. Of course, it is possible to train a foundational model to process all these modalities as input, but this has two limitations: (1) This is a computationally expensive model, (2) it assumes all modalities are available during inference. To tackle these limitations, the paper trains a foundational teacher model with all modalities, then distills the teacher's model into a student model that requires only the RGB modality. The student model is computationally cheaper and requires RGB only. To avoid missing out on other fine-grained modalities, the student model is trained to generate fine-grained pseudo modality representations (pseudo pose, pseudo depth, etc) from the RGB input modality. The student model is trained in two stages: (1) predict correct fine-grained pseudo modality representation + align learned representation with the teacher model, (2) Same as stage#1 plus WSVAD supervised signal.