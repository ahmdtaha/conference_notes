* When does label smoothing help? **#nips2019**
* Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks **#nips2019**
* Energy Confused Adversarial Metric Learning for Zero-Shot Image Retrieval and Clustering **#aaai2019**
* Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network **#aaai2019**
* Res2net: A new multi-scale backbone architecture **#pami2019**
* On Mutual Information Maximization for Representation Learning **#arxiv2019**
* Momentum Contrast for Unsupervised Visual Representation Learning **#arxiv2019**
* Measuring Dataset Granularity **#arxiv2019**
* The lottery ticket hypothesis: Finding sparse, trainable neural networks **#iclr2019**
* IMAGENET-TRAINED CNNS ARE BIASED TOWARDS TEXTURE; INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS **#iclr2019**
* Contrastive Multiview Coding **#arxiv2019**
* Visualizing deep similarity networks **#wacv2019**
* Spreading vectors for similarity search **#iclr2019**
* Dynamic channel pruning: Feature boosting and suppression **#iclr2019**
* Learning deep representations by mutual information estimation and maximization **#iclr2019**
* Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask **#nips2019**
* Deep Image Prior **#bayes #cvpr2018**
* CondenseNet: An Efficient DenseNet Using Learned Group Convolutions **#cvpr2018**
* Neural baby talk **#cvpr2018**
* Blockdrop: Dynamic inference paths in residual networks **#cvpr2018**
* Hydranets: Specialized dynamic architectures for efficient inference **#cvpr2018**
* NISP: Pruning Networks Using Neuron Importance Score Propagation **#cvpr2018**
* Interpret neural networks by identifying critical data routing paths **#cvpr2018**
* Improvements to context based self-supervised learning **#cvpr2018**
* Large-scale distance metric learning with uncertainty **#cvpr2018**
* “Zero-Shot” Super-Resolution using Deep Internal Learning **#cvpr2018**
* Joint optimization framework for learning with noisy labels **#cvpr2018**
* Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination **#cvpr2018** -- interesting 
* Squeeze-and-Excitation Networks **#cvpr2018**
* Data Distillation: Towards Omni-Supervised Learning **#cvpr2018**
* Between-class learning for image classification **#cvpr2018**
* Boosting Self-Supervised Learning via Knowledge Transfer **#cvpr2018**
* Separating Style and Content for Generalized Style Transfer **#cvpr2018**
* Local descriptors optimized for average precision **#cvpr2018**
* Cleannet: Transfer learning for scalable image classifier training with label noise. **#cvpr2018**
* Mining on Manifolds: Metric Learning without Labels **#cvpr2018**
* Weakly supervised instance segmentation using class peak response -- **cvpr2018** important :hash:code
* VSE++: Improved visual-semantic embeddings. **#bmvc2018#**
* Rise: Randomized input sampling for explanation of black-box models  **#bmvc2018#**
* Compositing-aware image search **#eccv2018** :hash:industry :hash:adobe 
* Progressive neural architecture search **#eccv2018** :hash:industry :hash:Google 
* ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases **#eccv2018**
* Deep metric learning with hierarchical triplet loss **#eccv2018**
* Deep Clustering for Unsupervised Learning of Visual Features -- Nice Paper **#eccv2018**
* Sanity checks for saliency maps. **#nips2018**
* Mixup: Beyond Empirical Risk Minimization **#iclr2018**
* Born again neural networks. **#icml2018**
* Fast decoding in sequence models using discrete latent variables **#icml2018** -- Nice paper
* Accelerating deep metric learning via cross sample similarities transfer ([code](https://github.com/TuSimple/DarkRank/blob/master/PYOP/listmle_loss.py)) **#aaai2018**
* Representation Learning with Contrastive Predictive Coding **#arxiv2018**
* SpectralNet: Spectral Clustering using Deep Neural Networks **#arxiv2018**
* The Singular Values of Convolutional Layers **#arxiv2018**
* Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer **#arxiv2018**
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding **#arxiv2018**
* Large scale distributed neural network training through online distillation **#arxiv2018**
* Learning Low-Rank Representations **#arxiv2018**
* Label refinery: Improving imagenet classification through label progression **#arxiv2018**
* Stabilizing Gradients for Deep Neural Networks via Efficient SVDParameterization **#arxiv2018**
* Deep Paper Gestalt **#arxiv2018**
* Top-down neural attention by excitation backprop **#ijcv2018**
* A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning **#distill #cvpr2017**
* Focal Loss for Dense Object Detection **iccv2017**
* SVDNet for Pedestrian Retrieval **iccv2017**
* Learning efficient convolutional networks through network slimming. **iccv2017**
* Deep Metric Learning with Angular Loss **iccv2017**
* Representation Learning by Learning to Count **iccv2017**
* Dynamic coattention networks for question answering **#coattention** **#iclr2017**
* Adversarial feature learning **#iclr2017**
* A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks **#iclr2017**
* Sgdr: Stochastic gradient descent with warm restart **#iclr2017**
* Split-brain autoencoders: Unsupervised learning by cross-channel prediction **cvpr2017**
* Universal Adversarial Perturbations **cvpr2017**
* spatially adaptive computation time for residual networks **cvpr2017**
* No fuss distance metric learning using proxies **cvpr2017**
* iCaRL: Incremental classifier and representation learning **cvpr2017**
* Deep Mutual Learning **nips2017**
* Runtime neural pruning **nips2017**
* Dual Discriminator Generative Adversarial Nets **nips2017**
* VEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning **nips2017**
* Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples **nips2017**
* Learning Spread-out Local Feature Descriptors **iccv2017**
* Thinet: A filter level pruning method for deep neural network compression. **iccv2017**
* Channel pruning for accelerating very deep neural networks. **iccv2017**
* Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. **iccv2017**
* Learning Efficient Convolutional Networks through Network Slimming **iccv2017**
* Working hard to know your neighbor’s margins: Local descriptor learning loss  -- Triplet Lifted Structure loss **nips2017**
* TRACE NORM REGULARIZATION AND FASTER INFER-ENCE FOR EMBEDDED SPEECH RECOGNITION RNNS **#arxiv2017**
* SmoothGrad: removing noise by adding noise **#arxiv2017**
* MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications **#arxiv2017**
* Unsupervised learning by predicting noise. **#icml2017**
* Prototypical networks for few-shot learning **#nips2017**
* A closer look at memorization in deep networks -- Well written paper **#icml2017**
* DSD: Dense-Sparse-Dense Training for Deep Neural Networks **#iclr2017**
* Temporal ensembling for semisupervised learning **#iclr2017** -- Dropout Potential
* Pruning filters for efficient convnets **#iclr2017**
* Squeezenet: Alexnet-Level Accuracy With 50X Fewer Parameters And <0.5Mb Model Size **#iclr2017**
* Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions **#cvpr2016**
* Sketch Me That Shoe **#cvpr2016**
* Embedding Label Structures for Fine-Grained Feature Representation **#cvpr2016**
* Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions **#cvpr2016**
* Joint unsupervised learning of deep representations and image clusters **#cvpr2016**
* Unsupervised learning of visual representations by solving jigsaw puzzles **#eccv2016**
* Colorful Image Colorization **#eccv2016**
* Learning without Forgetting **#eccv2016**
* Less Is More: Towards Compact CNNs **#eccv2016**
* Ask Me Anything:
Dynamic Memory Networks for Natural Language Processing **#icml2016**
* Unsupervised Deep Embedding for Clustering Analysis **#icml2016**
* Wide Residual Networks **#bmvc2016**
* Improved Deep Metric Learning withMulti-class N-pair Loss Objective **#nips2016** 
* Residual Networks Behave Like Ensembles of Relatively Shallow Networks **#nips2016** 
* Improved techniques for training GANs **#nips2016** 
* Universal Correspondence Network **#nips2016** 
* Learning Deep Parsimonious Representations **#nips2016** 
* InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets **#nips2016** 
* PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions **#nips2016** 
* Conditional image generation with pixelcnn decoders **#nips2016** 
* Convolutional neural networks with low-rank regularization **#iclr2016** -- [code](https://github.com/chengtaipu/lowrankcnn)
* All you need is a good init **#iclr2016**
* Reducing Overfitting In Deep Networks By Decorrelating Representations **#iclr2016**
* Unifying distillation and privileged information **#iclr2016**
* Adversarially Learned Inference **#arxiv2016** 
* Pruning Filters For Efficient Convnets **#arxiv2016** 
* Understanding deep learning requires rethinking generalization **#arxiv2016**
* Understanding intermediate layers using linear classifier probes **#arxiv2016** 
* Adaptive Computation Time for Recurrent Neural Networks **#arxiv2016** 
* Unrolled generative adversarial networks **#iclr2016** 
* Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding **#iclr2016** 
* Data-dependent initializations of convolutional neural networks **#iclr2016** 
* Learning deep embeddings with histogram loss **#nips2016**
* Hierarchical question-image co-attention for visual question answering. **#coattention** **#nips2016**
* What’s the point: Semantic segmentation with point supervision **#eccv2016**
* Deep networks with stochastic depth **#eccv2016**
* Bayesian representation learning with oracle constraints. **#iclr2016**
* The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies -- **#acm2016**
* Learning to compare image patches via convolutional neural networks **#cvpr2015**
* Matchnet: Unifying feature and metric learning for patch-based matching **#cvpr2015** **#industry** 
* FaceNet: A unified embedding for face recognition and clustering **#cvpr2015** **#industry** 
* Spatial Transformer Network **#nips2015**
* Wide-area image geolocalization with aerial reference imagery **#iccv2015**
* Delving deep into rectifiers: Surpassing human-level performance on imagenet classification **#iccv2015**
* Discriminative unsupervised feature learning with exemplar convolutional neural networks **#pami2015**
* Learning both Weights and Connections for Efficient Neural Networks **#iclr2015**
* Fitnets: Hints for thin deep nets **#iclr2015**
* Highway networks  **#arxiv2015**
* Learning fine-grained image similarity with deep ranking **#cvpr2014**
* Fast dropout training **#icml2013**
* Maxout networks **#arvix2013** -- When Goodfellow was young
* Measuring the objectness of image windows **#pami2012**
* Simplifying neural networks by soft weight-sharing **#Neural computation 1992** -- I admire Hinton's writing style
* On random weights and unsupervised feature learning **#icml2011**
* Understanding the difficulty of training deep feedforward neural networks **#ai & stats 2010**
* Why does unsupervised pre-training help deep learning **#jmlr2010**
* What is the best multi-stage architecture for object recognition? **#iccv2009**
* Measuring Invariances in Deep Networks **#nips2009**
* Separating Style and Content **#nips1997**


