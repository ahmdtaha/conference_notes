* Energy Confused Adversarial Metric Learning for Zero-Shot Image Retrieval and Clustering **#aaai2019**
* Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network **#aaai2019**
* Augmenting neural networks with first- order logic **#acl2019**
* Convolutional networks with adaptive inference graphs **#ijcv2019** -- `Nice paper`
* Res2net: A new multi-scale backbone architecture **pami2019**
* Split-CNN: Splitting Window-based Operations in Convolutional Neural Networks for Memory System Optimization **asplos2019**
* Micro-Batch Training with Batch-Channel Normalization and Weight Standardization **#arxiv2019**
* Generating Diverse High-Fidelity Images with VQ-VAE-2 **#arxiv2019**
* On Mutual Information Maximization for Representation Learning **#arxiv2019**
* Faster Neural Network Training with Data Echoing **#arxiv2019** -- `Nice paper`
* Momentum Contrast for Unsupervised Visual Representation Learning **#arxiv2019**
* Measuring Dataset Granularity **#arxiv2019**
* Objects as Points **#arxiv2019**
* Gacnn: Training Deep Convolutional Neural Networks With Genetic Algorithm **#NeuralEvolutionaryComputing2019**
* Energy and policy considerations for deep learning in NLP **#arxiv2019** -- `Nice paper`
* Intriguing properties of randomly weighted networks: Generalizing while learning next to nothing **#crv2019** -- `Nice paper`
* Semi-supervised Domain Adaptation via Minimax Entropy
	> Interesting `PI_Reading_Grp`
* The lottery ticket hypothesis: Finding sparse, trainable neural networks **#iclr2019**
* Imagenet-Trained Cnns Are Biased Towards Texture; Increasing Shape Bias Improves Accuracy And Robustness **#iclr2019**
* Contrastive Multiview Coding **#arxiv2019**
* Recurrent independent mechanisms **#arxiv2019**
* A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms **#arxiv2019**
	> Heavy reading but it is worth it
* Efficient and Effective Dropout for Deep Convolutional Neural Networks **#arxiv2019**
* Visualizing deep similarity networks **#wacv2019**
* Spreading vectors for similarity search **#iclr2019**
* Proxylessnas: Direct Neural Architecture Search On Target Task And Hardware **#iclr2019**
* Dynamic channel pruning: Feature boosting and suppression **#iclr2019**
* Learning deep representations by mutual information estimation and maximization **#iclr2019**
* Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation **#icml2019**
* EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks **#icml2019** `Arch_Design`
* Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask **#nips2019**
* Transfusion: Understanding Transfer Learning for Medical Imaging **#nips2019**
	> Nice paper
* Fixing the train-test resolution discrepancy **#nips2019**
* Stand-Alone Self-Attention in Vision Models **#nips2019**
* MixMatch: A Holistic Approach to Semi-Supervised Learning **#nips2019**
* Consistency-based Semi-supervised Learning for Object Detection **#nips2019**
	> Nice paper
* SketchEmbedNet: Learning Novel Concepts by Imitating Drawings **#nips2019**
* Adversarial Training for Free **#nips2019**
* One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers **#nips2019**
* Channel Gating Neural Networks **#nips2019**
* When does label smoothing help? **#nips2019**
* Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks **#nips2019**
* Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs **#nipsWorkshop2019**
* TriResNet: A Deep Triple-stream Residual Network for Histopathology Grading **#iciar2019**
* Incremental learning in deep convolutional neural networks using partial network sharing **IEEEAccess2019**
* The NYU Breast Cancer Screening Dataset v1.0 `NYU` **#2019**
* Screening Mammogram Classification with Prior Exams `NYU` **#midl2019**
* Deep Neural Networks Improve Radiologists’ Performance in Breast Cancer Screening `NYU` **#IEEE Medical Imaging 2019**
* Language Models are Unsupervised Multitask Learners **open_ai2019** -- `GPT-2`
* Deep Image Prior **#bayes #cvpr2018**
* CondenseNet: An Efficient DenseNet Using Learned Group Convolutions **#cvpr2018**
* Neural baby talk **#cvpr2018**
* Blockdrop: Dynamic inference paths in residual networks **#cvpr2018**
* Hydranets: Specialized dynamic architectures for efficient inference **#cvpr2018**
* NISP: Pruning Networks Using Neuron Importance Score Propagation **#cvpr2018**
* Interpret neural networks by identifying critical data routing paths **#cvpr2018**
* A Style-Based Generator Architecture for Generative Adversarial Networks **#cvpr2018**
* Improvements to context based self-supervised learning **#cvpr2018**
* Large-scale distance metric learning with uncertainty **#cvpr2018**
* “Zero-Shot” Super-Resolution using Deep Internal Learning **#cvpr2018**
* Joint optimization framework for learning with noisy labels **#cvpr2018**
* Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination **#cvpr2018** -- interesting 
* Squeeze-and-Excitation Networks **#cvpr2018**
* Data Distillation: Towards Omni-Supervised Learning **#cvpr2018**
* Between-class learning for image classification **#cvpr2018**
* MobileNetV2: Inverted Residuals and Linear Bottlenecks **#cvpr2018**
* Boosting Self-Supervised Learning via Knowledge Transfer **#cvpr2018**
* Separating Style and Content for Generalized Style Transfer **#cvpr2018**
* Local descriptors optimized for average precision **#cvpr2018**
* Cleannet: Transfer learning for scalable image classifier training with label noise. **#cvpr2018**
* Mining on Manifolds: Metric Learning without Labels **#cvpr2018**
* Weakly supervised instance segmentation using class peak response -- **cvpr2018** important :hash:code
* Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution **aaai2018**
* VSE++: Improved visual-semantic embeddings. **#bmvc2018#**
* Rise: Randomized input sampling for explanation of black-box models  **#bmvc2018#**
* Compositing-aware image search **#eccv2018** :hash:industry :hash:adobe 
* Progressive neural architecture search **#eccv2018** :hash:industry :hash:Google 
* ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases **#eccv2018**
* Hierarchy of Alternating Specialist for Scene Recognition **#eccv2018**
* Bisenet: Bilateral Segmentation Network For Real-Time Semantic Segmentation **#eccv2018**
* CBAM: Convolutional Block Attention Module **#eccv2018**
* Exploring the Limits of Weakly Supervised Pretraining **#eccv2018**
* SkipNet: Learning Dynamic Routing in Convolutional Networks **#eccv2018**
* Deep metric learning with hierarchical triplet loss **#eccv2018**
* Deep Clustering for Unsupervised Learning of Visual Features -- Nice Paper **#eccv2018**
* Compositional Learning for Human Object Interaction **#eccv2018** -- `PI_Reading_Grp`
* Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks **#wacv2018**
* Sanity checks for saliency maps. **#nips2018**
* Neighbourhood Consensus Networks. **#nips2018**
* Mixup: Beyond Empirical Risk Minimization **#iclr2018**
* Meta-learning for semi-supervised few-shot classification **#iclr2018**
* Progressive growing of gans for improvedquality, stability, and variation **#iclr2018**
* Born again neural networks. **#icml2018**
* Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. **#icml2018**
* Fast decoding in sequence models using discrete latent variables **#icml2018** -- Nice paper
* Stochastic video generation with a learned prior **#pmlr2018** -- `PI_Reading_Grp`
* Accelerating deep metric learning via cross sample similarities transfer ([code](https://github.com/TuSimple/DarkRank/blob/master/PYOP/listmle_loss.py)) **#aaai2018**
* Understanding Deep Convolutional Networks through Gestalt Theory **#ist2018**
* Representation Learning with Contrastive Predictive Coding **#arxiv2018**
* Dataset Distillation -- `BAIR` **#arxiv2018**
* fastMRI: An Open Dataset and Benchmarks for Accelerated MRI -- `NYU` **#arxiv2018**
* Learning a Variational Network for Reconstruction of Accelerated MRI Data -- `NYU` **Magnetic Resonance In Medicine**
* Towards End-to-End Lane Detection: an Instance Segmentation Approach **IEEE intelligent vehicles symposium 2018**
* SpectralNet: Spectral Clustering using Deep Neural Networks **#arxiv2018**
* The Singular Values of Convolutional Layers **#arxiv2018**
* Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer **#arxiv2018**
* Stochastic Adversarial Video Prediction **#arxiv2018**
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding **#arxiv2018**
* Large scale distributed neural network training through online distillation **#arxiv2018**
* Learning Low-Rank Representations **#arxiv2018**
* Label refinery: Improving imagenet classification through label progression **#arxiv2018**
* Stabilizing Gradients for Deep Neural Networks via Efficient SVDParameterization **#arxiv2018**
* Flexible deep neural network processing **#arxiv2018**
* Deep Paper Gestalt **#arxiv2018**
* Top-down neural attention by excitation backprop **#ijcv2018**
* MonolithNet: Training monolithic deep neural networks via a partitioned training strategy **#jcvis2018**
* Detecting and classifying lesions in mammograms with Deep Learning **#nature2018**
* A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning **#distill #cvpr2017**
* From Red Wine to Red Tomato: Composition with Context **#cvpr2017**
* Focal Loss for Dense Object Detection **#iccv2017**
* SVDNet for Pedestrian Retrieval **#iccv2017**
* Revisiting unreasonable effectiveness of data in deep learning era **#iccv2017**
* Arbitrary style transfer in real-time with adaptive instance normalization. **#iccv2017**
* Learning efficient convolutional networks through network slimming. **#iccv2017**
* Deep Metric Learning with Angular Loss **#iccv2017**
* Representation Learning by Learning to Count **#iccv2017**
* Dynamic coattention networks for question answering **#coattention** **#iclr2017**
* Adversarial feature learning **#iclr2017**
* learned representation for artistic style. **#iclr2017** 
* beta-vae: Learning basic visual concepts with a constrained variational framework **#iclr2017**
* A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks **#iclr2017**
* Sgdr: Stochastic gradient descent with warm restart **#iclr2017**
* Split-brain autoencoders: Unsupervised learning by cross-channel prediction **cvpr2017**
* Universal Adversarial Perturbations **cvpr2017**
* Feature Pyramid Networks for Object Detection **cvpr2017**
* YOLO9000: Better, Faster, Stronger **cvpr2017**
* spatially adaptive computation time for residual networks **cvpr2017**
* No fuss distance metric learning using proxies **cvpr2017**
* iCaRL: Incremental classifier and representation learning **cvpr2017**
* Deep Mutual Learning **#nips2017**
* Runtime neural pruning **#nips2017**
* Dynamic Routing Between Capsules **#nips2017**
* Dual Discriminator Generative Adversarial Nets **#nips2017**
* VEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning **#nips2017**
* The Reversible Residual Network: Backpropagation Without Storing Activations **#nips2017**
* Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples **#nips2017**
* Learning Spread-out Local Feature Descriptors **#iccv2017**
* Thinet: A filter level pruning method for deep neural network compression. **#iccv2017**
* Channel pruning for accelerating very deep neural networks. **#iccv2017**
* Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. **#iccv2017**
* Learning Efficient Convolutional Networks through Network Slimming **#iccv2017**
* Demystifying Neural Style Transfer **#ijcai2017**
* Working hard to know your neighbor’s margins: Local descriptor learning loss  -- Triplet Lifted Structure loss **nips2017**
* TRACE NORM REGULARIZATION AND FASTER INFER-ENCE FOR EMBEDDED SPEECH RECOGNITION RNNS **#arxiv2017**
* SmoothGrad: removing noise by adding noise **#arxiv2017**
* High-Resolution Breast Cancer Screening with Multi-View Deep Convolutional Neural Networks -- `WR-AI` **#arxiv2017**
* Large Batch Training of Convolutional Networks **#arxiv2017**
* Neural Discrete Representation Learning **#arxiv2017**
* MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications **#arxiv2017**
* Unsupervised learning by predicting noise. **#icml2017**
* Learning to Generate Long-term Future via Hierarchical Prediction **#icml2017**
* SplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization **#icml2017**
* Prototypical networks for few-shot learning **#nips2017**
* Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results **#nips2017**
* A closer look at memorization in deep networks -- Well written paper **#icml2017**
* DSD: Dense-Sparse-Dense Training for Deep Neural Networks **#iclr2017**
* Temporal ensembling for semisupervised learning **#iclr2017** -- Dropout Potential
* Pruning filters for efficient convnets **#iclr2017**
* Squeezenet: Alexnet-Level Accuracy With 50X Fewer Parameters And <0.5Mb Model Size **#iclr2017**
* Rethinking Atrous Convolution for Semantic Image Segmentation **#arXiv2017** `Arch_Design`
* Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions **#cvpr2016**
* Sketch Me That Shoe **#cvpr2016**
* Convolutional Pose Machines **#cvpr2016**
* You Only Look Once: Unified, Real-Time Object Detection **#cvpr2016**
* Embedding Label Structures for Fine-Grained Feature Representation **#cvpr2016**
* Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions **#cvpr2016**
* Large Scale Semi-supervised Object Detection using Visual and Semantic Knowledge Transfer **#cvpr2016**
* Factors in finetuning deep model for object detection with long-tail distribution **#cvpr2016**
* Joint unsupervised learning of deep representations and image clusters **#cvpr2016**
* Unsupervised learning of visual representations by solving jigsaw puzzles **#eccv2016**
* Colorful Image Colorization **#eccv2016**
* Learning without Forgetting **#eccv2016**
* Less Is More: Towards Compact CNNs **#eccv2016**
* Ask Me Anything:
Dynamic Memory Networks for Natural Language Processing **#icml2016**
* Unsupervised Deep Embedding for Clustering Analysis **#icml2016**
* Dynamic Capacity Network **#icml2016**
* Wide Residual Networks **#bmvc2016**
* Adaptive convolutional neural network and its application in face recognition **NeuralProcessingLetters2016**
* Improved Deep Metric Learning withMulti-class N-pair Loss Objective **#nips2016** 
* Understanding the Effective Receptive Field in Deep Convolutional Neural Networks **#nips2016**  -- Nice
* Residual Networks Behave Like Ensembles of Relatively Shallow Networks **#nips2016** 
* Improved techniques for training GANs **#nips2016** 
* Dynamic Filter Networks **#nips2016** 
* Universal Correspondence Network **#nips2016** 
* Learning Deep Parsimonious Representations **#nips2016** 
* InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets **#nips2016** 
* PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions **#nips2016** 
* Conditional image generation with pixelcnn decoders **#nips2016** 
* Convolutional neural networks with low-rank regularization **#iclr2016** -- [code](https://github.com/chengtaipu/lowrankcnn)
* All you need is a good init **#iclr2016**
* Reducing Overfitting In Deep Networks By Decorrelating Representations **#iclr2016**
* Unifying distillation and privileged information **#iclr2016**
* Adversarially Learned Inference **#arxiv2016** 
* ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation **#arxiv2016** `Arch_Design`
* Gaussian Error Linear Units (GELUs) **#arxiv2016** 
* Pruning Filters For Efficient Convnets **#arxiv2016** 
* Understanding deep learning requires rethinking generalization **#arxiv2016**
* Understanding intermediate layers using linear classifier probes **#arxiv2016** 
* What makes ImageNet good for transfer learning? **#arxiv2016** 
* Adaptive Computation Time for Recurrent Neural Networks **#arxiv2016** 
* Unrolled generative adversarial networks **#iclr2016** 
* Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding **#iclr2016** 
* Data-dependent initializations of convolutional neural networks **#iclr2016** 
* Learning deep embeddings with histogram loss **#nips2016**
* Hierarchical question-image co-attention for visual question answering. **#coattention** **#nips2016**
* What’s the point: Semantic segmentation with point supervision **#eccv2016**
* Deep networks with stochastic depth **#eccv2016**
* Particular object retrieval with integral max-pooling of cnn activations **#iclr2016**
* Bayesian representation learning with oracle constraints. **#iclr2016**
* The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies -- **#acm2016**
* Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks **#tpami2016** -- `Anchors` and `RPN` origins
* Learning to compare image patches via convolutional neural networks **#cvpr2015**
* Matchnet: Unifying feature and metric learning for patch-based matching **#cvpr2015** **#industry** 
* FaceNet: A unified embedding for face recognition and clustering **#cvpr2015** **#industry** 
* Efficient object localization using convolutional networks **#cvpr2015** 
* Spatial Transformer Network **#nips2015**
* Deep visual analogy-making **#nips2015**
* Wide-area image geolocalization with aerial reference imagery **#iccv2015**
* Fast R-CNN **#iccv2015**
* Delving deep into rectifiers: Surpassing human-level performance on imagenet classification **#iccv2015**
* Discriminative unsupervised feature learning with exemplar convolutional neural networks **#pami2015**
* Learning both Weights and Connections for Efficient Neural Networks **#iclr2015**
* Fitnets: Hints for thin deep nets **#iclr2015**
* Flattened convolutional neural networks for feedforward acceleration **#iclr2015** Workshop
* NICE: Non-Linear Independent Components Estimation **#iclr2015**
* Understanding Locally Competitive Networks **#iclr2015**
* Highway networks  **#arxiv2015**
* Multi-path Convolutional Neural Networks for Complex Image Classification **#arxiv2015**
	> very primitive but interesting
* Learning fine-grained image similarity with deep ranking **#cvpr2014**
* DeepPose: Human Pose Estimation via Deep Neural Networks **#cvpr2014**
* Rich Feature Hierarchies For Accurate Object Detection And Semantic Segmentation **#cvpr2014** `R-CNN`
* LSDA: Large Scale Detection Through Adaptation **#nips2014**
* Deep convolutional network cascade for facial point detection **#cvpr2013**
* Write a Classifier: Zero-Shot Learning Using Purely Textual Description **#iccv2013**
	> Heavy numerical optimization
* Fast dropout training **#icml2013**
* Adaptive dropout for training deep neural networks **#nips2013**
* Maxout networks **#arvix2013** -- When Goodfellow was young
* Measuring the objectness of image windows **#pami2012**
* INbreast: Toward a Full-field Digital Mammographic Database **#nlm2012**
* Unbiased Look at Dataset Bias **#cvpr2011**
* The Power of Comparative Reasoning -- `PI_Reading_Grp` **#iccv2011**
* On random weights and unsupervised feature learning **#icml2011**
* Understanding the difficulty of training deep feedforward neural networks **#ai & stats 2010**
* Why does unsupervised pre-training help deep learning **#jmlr2010**
* What is the best multi-stage architecture for object recognition? **#iccv2009**
* Measuring Invariances in Deep Networks **#nips2009**
* Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning **#nips2008**
* Greedy Layer-Wise Training of Deep Networks **#nips2006**
* The development of embodied cognition: Six lessons from babies. **Artificiallife2005**
* Semi-supervised learning by entropy minimization **#nips2004** -- A simple and good idea but the notation! 
* The Dynamic Representation of Scenes **#VisCog2000**
* Separating Style and Content **#nips1997**
* Training feedforward neural networks using genetic algorithms **#IJCAI1989**
* Simplifying neural networks by soft weight-sharing **#Neural computation 1992** -- I admire Hinton's writing style
* Distributed Representation Geoffrey Hinton -- heavy reading but worth it **CMU 1984**
* K-Lines: A Theory of Memory **Cognitive Science 1980**
	> I like it. Yet, it felt more like a brain teaser than an academic paper.
	
	